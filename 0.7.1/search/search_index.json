{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nodestream","text":"<p><code>nodestream</code> is a modern and simple framework for performing ETL into Graph Databases.</p> <p>With <code>nodestream</code> you can enjoy:</p> <ol> <li>Building Fast: with a user friendly and extensible DSL.</li> <li>Fewer Bugs: because <code>less code</code> == <code>less bugs</code>.</li> <li>Robustness and Resilience: with battlehardend and optimized components.</li> </ol> <p></p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Get started with our Tutorial.</p>"},{"location":"#shell-completion","title":"Shell Completion","text":""},{"location":"#bash-ubuntu-debian","title":"BASH - Ubuntu / Debian","text":"Bash<pre><code>nodestream completions bash | sudo tee /etc/bash_completion.d/nodestream.bash-completion\n</code></pre>"},{"location":"#bash-mac-osx-with-homebrew-bash-completion","title":"BASH - Mac OSX (with Homebrew \"bash-completion\")","text":"Bash<pre><code>nodestream completions bash &gt; $(brew --prefix)/etc/bash_completion.d/nodestream.bash-completion\n</code></pre>"},{"location":"#zsh","title":"ZSH","text":"Bash<pre><code>mkdir ~/.zfunc\necho \"fpath+=~/.zfunc\" &gt;&gt; ~/.zshrc\nnodestream completions zsh &gt; ~/.zfunc/_test\n</code></pre>"},{"location":"#fish","title":"FISH","text":"Bash<pre><code>nodestream completions fish &gt; ~/.config/fish/completions/nodestream.fish\n</code></pre>"},{"location":"contributing/","title":"Contribution Guidelines","text":"<p>We fully embrace the open source model, and if you have something to add, we would love to review it and get it merged in!</p>"},{"location":"contributing/#before-you-start","title":"Before You Start","text":"<p>Clone the project</p> Bash<pre><code>  git clone git@github.com:nodestream-proj/nodestream.git\n</code></pre> <p>Go to the project directory</p> Bash<pre><code>  cd nodestream\n</code></pre> <p>Install dependencies</p> <p>(You'll need to have Python 3.11 and poetry installed)</p> Bash<pre><code>  poetry install\n</code></pre> <p>Run the Tests</p> Bash<pre><code>  poetry run pytest\n</code></pre>"},{"location":"contributing/#contribution-process","title":"Contribution Process","text":"<p>Please note we have a Code of Conduct, please follow it in all your interactions with the project.</p>"},{"location":"contributing/#communicating-with-the-project","title":"Communicating with the Project","text":"<p>Before starting work, please check the issues to see what's being discussed and worked on. If there's an open, unresolved issue for what you want to work on, please comment on it stating that you would like to tackle the changes. If there's not an issue, please add one and also state that you'll work on it.</p>"},{"location":"contributing/#contributon-strategy","title":"Contributon Strategy","text":"<ol> <li>Ensure all code matches the \"Code Expectations\" discussed below</li> <li>Commit and push your code to your fork</li> <li>Open a pull request</li> </ol>"},{"location":"contributing/#code-expectations","title":"Code Expectations","text":""},{"location":"contributing/#code-coverage","title":"Code Coverage","text":"<p>All new code will be unit tested to 90% coverage.</p>"},{"location":"contributing/#object-orientation","title":"Object Orientation","text":"<p><code>nodestream</code> relies on a heavily object oriented model. Most natural extensions of the framework should use that design philosophy.</p>"},{"location":"contributing/#contact-information","title":"Contact Information","text":"<p>The best way to contact the project Maintainers is through the issues; we'll get back to you within a few business days.</p>"},{"location":"technology/","title":"Technical Details","text":""},{"location":"technology/#asyncio","title":"<code>asyncio</code>","text":"<p>Nodestream is built on top of the more modern aspects of python. Primarily asyncio to improve performance.  To understand how it does so, it\u2019s important to understand what exactly asnycio is and how it works. </p> <p><code>asyncio</code> is a way of modeling concurrency in an application.  Note that concurrency is different than parallelism.  Parallelism deals with multiple operations executing simultaneously while concurrency deals with tasks that can be executed independently of each other and start, run, and complete in overlapping time periods</p> <p></p> <p>Note</p> <p>Images from https://realpython.com/async-io-python/</p> <p><code>asyncio</code> operates on the observation that in some systems, the cpu is spending a lot of time blocking or waiting for IO operations such as network calls, file reads, and the like to finish before it can continue operating.  To make better use of the CPU, <code>asyncio</code> introduces a series of language features that facilitate defining tasks and signaling when they need to wait. The language runtime can the \u201cswap in\u201d a different task that it can work on that\u2019s not (currently) IO bound.  In workloads where there is an abundant amout of IO, the overhead of managing these tasks is worth it because  we are able to make better use CPU time that would otherwise be wasted.</p> <p></p> <p>Note</p> <p>Images from https://eng.paxos.com/python-3s-killer-feature-asyncio</p> <p>So how does nodestream leverage it? All the performance sensitive apis in nodestream allow for asynchronous operations. Most importantly, each step in a pipeline is executed asyncronously. To do so, each step is given two components at exectuion time:</p> <ul> <li>A reference to the step before it</li> <li>An outbox which is used to store completed output</li> </ul> <p>Each step runs in its own asynchronous loop awaiting results from the upstream step, processing them, and putting results in the outbox. This design allows for each step to execute independently and not be constrained by IO bottlenecks down stream.  Take for example a standard pipeline of three components:</p> <ol> <li>An file extractor</li> <li>An interpreter</li> <li>A Graph database writer</li> </ol> <p>The slowest step in the pipeline is the graph database writer.  It takes large chunks of data, converts it all to queries and submits generally long running queries it has to await. While this is happening, python can suspend the task waiting on the database allowing the other steps to continue processing. This allows for the extractor and interpreter to continue processing data and not be constrained by the database writer.</p>"},{"location":"docs/tutorial/","title":"Tutorial","text":"<p>Welcome! And thank you for checking out nodestream. This guide will get you up and running with the basics of nodestream. To demonstrate it, we'll be building an ingestion of an org chart.</p>"},{"location":"docs/tutorial/#generating-a-new-project","title":"Generating a New Project","text":"<p>To generate a nodestream project, its simple. Make sure you have installed nodestream with:</p> Bash<pre><code>pip install nodestream\n</code></pre> <p>After doing so, lets generate a simple project. To do so, run</p> Text Only<pre><code>nodestream new --database neo4j my_first_graph\n</code></pre> <p>In this example, we are signaling that we want to use <code>neo4j</code> as the graph database and naming our project <code>my_first_graph</code>. <code>cd</code> into that new directory.</p>"},{"location":"docs/tutorial/#touring-the-scaffolded-project","title":"Touring the Scaffolded Project","text":"<p>In your newly created <code>my_first_graph</code> directory, you should see a folder structure that looks like so:</p> Text Only<pre><code>.\n\u251c\u2500\u2500 my_first_graph\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 argument_resolvers.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 normalizers.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 value_providers.py\n\u251c\u2500\u2500 nodestream.yaml\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 pipelines\n    \u2514\u2500\u2500 sample.yaml\n</code></pre> <p>Essentially it produces three things.</p> <ol> <li>A basically empty python package named the same thing as your project (<code>my_first_graph</code>).</li> <li>A pipelines folder which stores the definitions of your etl pipelines.</li> <li>A <code>nodestream.yaml</code> file which acts as your project configuration.</li> </ol> <p>Expanding on the <code>nodestream.yaml</code> file, it looks a little like this:</p> YAML<pre><code>scopes:\ndefault:\npipelines:\n- pipelines/sample.yaml\n</code></pre> <p>This file is comprised of a <code>scopes</code> section where pipelines are defined. A <code>scope</code> represents a logical grouping of pipelines that make sense for your application. Think of them like a folder.</p> <p>You can see the project status by running <code>nodestream show</code>. That should produce an output like this:</p> Text Only<pre><code>+---------+--------+-----------------------+-------------+\n| scope   | name   | file                  | annotations |\n+---------+--------+-----------------------+-------------+\n| default | sample | pipelines/sample.yaml |             |\n+---------+--------+-----------------------+-------------+\n</code></pre>"},{"location":"docs/tutorial/#managing-pipelines","title":"Managing Pipelines","text":"<p>In order to demonstrate how one can manage pipelines in nodestream, lets remove the default pipeline and add it back.</p>"},{"location":"docs/tutorial/#remove-a-pipeline","title":"Remove A Pipeline","text":"<p>Start by running:</p> Bash<pre><code>nodestream remove default sample\n</code></pre> <p>This removes the pipeline in the <code>default</code> scope with the name <code>sample</code>. Now if we run <code>nodestream show</code> again you will see the pipeline is gone:</p> Text Only<pre><code>+-------+------+------+-------------+\n| scope | name | file | annotations |\n+-------+------+------+-------------+\n</code></pre> <p>Further more you can also see that the pipeline file is removed from the project directory:</p> Text Only<pre><code>.\n\u251c\u2500\u2500 my_first_graph\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 argument_resolvers.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 normalizers.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 value_providers.py\n\u251c\u2500\u2500 nodestream.yaml\n\u2514\u2500\u2500 pipelines\n</code></pre>"},{"location":"docs/tutorial/#generate-a-new-pipeline","title":"Generate a New Pipeline","text":"<p>Now lets generate a new pipeline running:</p> Bash<pre><code>nodestream scaffold org-chart\n</code></pre> <p>That will produce a new pipeline file called <code>org-chart.yaml</code></p> Text Only<pre><code>.\n\u251c\u2500\u2500 my_first_graph\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 argument_resolvers.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 normalizers.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 value_providers.py\n\u251c\u2500\u2500 nodestream.yaml\n\u2514\u2500\u2500 pipelines\n    \u2514\u2500\u2500 org-chart.yaml\n</code></pre> <p>The output of <code>nodestream show</code> should now look like this:</p> Text Only<pre><code>+---------+-----------+--------------------------+-------------+\n| scope   | name      | file                     | annotations |\n+---------+-----------+--------------------------+-------------+\n| default | org-chart | pipelines/org-chart.yaml |             |\n+---------+-----------+--------------------------+-------------+\n</code></pre>"},{"location":"docs/tutorial/#introducing-our-data-set","title":"Introducing our Data Set","text":"<p>Before anything, lets create a directory to store our data:</p> Bash<pre><code>mkdir data\n</code></pre> <p>Now, lets examine our data. In this example, we are building an org chart. Let's take a look at a couple records:</p> data/jdoe.json<pre><code>{\n\"employee_id\": \"jdoe\",\n\"first_name\": \"Jane\",\n\"last_name\": \"Doe\",\n\"title\": \"CEO\",\n\"reports_to\": null\n}\n</code></pre> data/bsmith.json<pre><code>{\n\"employee_id\": \"bsmith\",\n\"first_name\": \"Bob\",\n\"last_name\": \"Smith\",\n\"title\": \"CTO\",\n\"reports_to\": \"jdoe\"\n}\n</code></pre> <p>For the purposes of this demonstration, create TWO <code>.json</code> files in the newly created <code>data</code> directory with these contents and create a couple more records that fit the same schema if you choose. So your project structure should look approximately like this:</p> Text Only<pre><code>.\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bsmith.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 jdoe.json\n\u251c\u2500\u2500 my_first_graph\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 argument_resolvers.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 normalizers.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 value_providers.py\n\u251c\u2500\u2500 nodestream.yaml\n\u2514\u2500\u2500 pipelines\n    \u2514\u2500\u2500 org-chart.yaml\n</code></pre>"},{"location":"docs/tutorial/#implement-the-pipeline","title":"Implement the Pipeline","text":"<p>Cracks Fingers...</p> <p>Alright, now lets get down to building out the pipeline. If you open the <code>pipelines/org-chart.yaml</code> file it should look like this:</p> YAML<pre><code>- arguments:\nstop: 100000\nfactory: range\nimplementation: nodestream.pipeline:IterableExtractor\n- arguments:\ninterpretations:\n- key:\nnumber: !jmespath 'index'\nnode_type: Number\ntype: source_node\nimplementation: nodestream.interpreting:Interpreter\n- arguments:\nbatch_size: 1000\ndatabase: neo4j\npassword: neo4j123\nuri: bolt://localhost:7687\nusername: neo4j\nimplementation: nodestream.databases:GraphDatabaseWriter\n</code></pre> <p>Each pipeline file is laid out as a series of <code>Step</code>s which are chained together and executed in order for each record. The first item in the pipeline is referred to generally as an <code>Extractor</code>.</p>"},{"location":"docs/tutorial/#loading-our-data-files","title":"Loading our Data Files","text":"<p>In the scaffold, we have an <code>IterableExtractor</code> configured to return records for each number in the range [0,100000). Not exactly the most interesting. Lets get to work wiring this up to use our newly created data instead. Replace the first block with the following:</p> YAML<pre><code>- implementation: nodestream.pipeline.extractors:FileExtractor\narguments:\nglobs:\n- data/*.json\n# remainder of the pipeline unchanged.\n</code></pre> <p>This block tells nodestream to initialize the <code>FileExtractor</code> class in the <code>nodestream.pipeline.extractors</code> module to handle the first step of the pipeline. To initialize it, it passes the <code>arguments</code> provided. In this case, the <code>FileExtractor</code> expects a list of glob strings. Every file that matches these glob strings is loaded and passed as a record in the pipeline.</p> <p>For more information on the <code>FileExtractor</code> or <code>Extractors</code> in general, refer to the Extractors Reference. For more information on the file formats and how they are supported, view the File Format Reference.</p>"},{"location":"docs/tutorial/#interpreting-the-data-into-nodes-and-relationships","title":"Interpreting the Data Into Nodes and Relationships","text":"<p>The <code>Step</code> that you will spend the most time becoming familiar with is the <code>Interpreter</code>. The interpreter takes instructions on how to convert a document or record like our JSON data and map it into nodes and relationships that we want to be in our graph.</p> <p>To do so, it relies on <code>Interpretation</code>s. An <code>Interpretation</code> is a single one of the aforementioned instructions. While there is tremendous depth to the <code>Interpreter</code> and <code>Interpretation</code>s, we are going to keep it simple for this tutorial.</p> <p>Replace the second step (the existing interpreter block) with the following (we'll go over it bit by bit):</p> YAML<pre><code>- implementation: nodestream.interpreting:Interpreter\narguments:\ninterpretations:\n- type: source_node\nkey:\nemployee_id: !jmespath 'employee_id'\nproperties:\nfirst_name: !jmespath 'first_name'\nlast_name: !jmespath 'last_name'\ntitle: !jmespath 'CEO'\nnode_type: Employee\n- type: relationship\nnode_type: Employee\nrelationship_type: REPORTS_TO\nnode_key:\nemployee_id: !jmespath 'reports_to'\n</code></pre> <p>At the top of the block, its follows the same pattern we saw with the <code>FileExtractor</code> from before with the <code>implementation</code> and <code>arguments</code> sections. However, the <code>arguments</code> are obviously very different. Lets investigate each of the two interpretations:</p>"},{"location":"docs/tutorial/#source-node","title":"Source Node","text":"YAML<pre><code>- type: source_node\nkey:\nemployee_id: !jmespath 'employee_id'\nproperties:\nfirst_name: !jmespath 'first_name'\nlast_name: !jmespath 'last_name'\ntitle: !jmespath 'CEO'\nnode_type: Employee\n</code></pre> <p>A source node represents the node at the conceptual \"center\" of the ingest. Typically this represents the central entity that you are modeling with the Ingest. In our case, its the employee for whom the record represents. We've decided to call this type of node an <code>Employee</code>.</p> <p>The <code>key</code> block tells nodestream what set of properties represents a unique node of the <code>node_type</code> and how to get the values. In our case, we use the <code>employee_id</code> field the record and extract that using the <code>!jmespath</code> Value Provider.</p> <p>We take a similar approach with the properties field. We extract the properties <code>first_name</code>, <code>last_name</code>, and <code>title</code> from their corresponding locations in the record, again with <code>!jmespath</code>. Note that we have kept the field names on the node the same as the json document, but this does not need to be the case.</p>"},{"location":"docs/tutorial/#relationship","title":"Relationship","text":"<p>A graph database would be a lot less useful if we did not create relationships to other data. In our case, we want to model the org chart, so we need to draw the relationship to the employee's boss.</p> YAML<pre><code>- type: relationship\nnode_type: Employee\nrelationship_type: REPORTS_TO\nnode_key:\nemployee_id: !jmespath 'reports_to'\n</code></pre> <p>Here we tell the interpreter that we want to relate to an <code>Employee</code> node with a relationship labels <code>REPORTS_TO</code>. For nodestream to know which <code>Employee</code> node to relate to, we need to specify the key of the related node. In our case, we can do that by extracting the value of <code>reports_to</code>  and mapping it to the <code>employee_id</code> key.</p>"},{"location":"docs/tutorial/#testing-it-out","title":"Testing it Out","text":"<p>Alright! We've done a lot of work. Lets see if our results can pay off. But before we get started, we need to have a database to connect to. In the beginning we selected neo4j. The easiest way to get a local neo4j database is to run it via docker. Below is a command to load a docker neo4j database:</p> Bash<pre><code>docker run \\\n--restart always \\\n--publish=7474:7474 --publish=7687:7687 \\\n--env NEO4J_AUTH=neo4j/neo4j123 \\\n--env NEO4J_PLUGINS='[\"apoc\",\"bloom\"]' \\\nneo4j:5\n</code></pre> <p>After that, we are finally ready! Drum roll please...</p> Bash<pre><code>nodestream run org-chart -v\n</code></pre> <p>Should give you output like this:</p> Text Only<pre><code>Running: Initialize Logger\nRunning: Initialize Project\nRunning: Run Pipeline\n - Finished running pipeline: 'org-chart' (1 sec)\n</code></pre>"},{"location":"docs/guides/creating-your-own-argument-resolver/","title":"Creating An ArgumentResolver","text":"<p>A <code>ArgumentResolver</code> allows you to inline a value into the Pipeline file before the pipeline is initialized. This can be useful for passing configuration from files, environment, secret stores, and the like.</p> <p>For example, assume that have a database password that you would like to retrieve from a secret store, in this case, AWS secrets manager.</p>"},{"location":"docs/guides/creating-your-own-argument-resolver/#defining-your-argumentresolver-class","title":"Defining your ArgumentResolver Class","text":"Python<pre><code>from typing import Any\n\nimport boto3\n\nfrom nodestream.pipeline.argument_resolvers import ArgumentResolver\n\n\nclass EnvironmentResolver(ArgumentResolver):\n    @classmethod\n    def install_yaml_tag(cls, loader: Type[SafeLoader]):\n        loader.add_constructor(\n            \"!secrets_manager\",\n            lambda loader, node: cls.get_from_secrets_manager(loader.construct_scalar(node)),\n        )\n\n    @staticmethod\n    def get_from_secrets_manager(variable_name):\n        client = boto3.client(\"secretsmanager\")\n        return client.get_secret_value(SecretId=variable_name)[\"SecretString\"]\n</code></pre> <p>Note that this implementation is pretty naive. But it's the simplest we need to demonstrate the point.</p> <p>In this example, we register with a yaml loader that can load a tag in yaml to utilize our new <code>ArgumentResolver</code>. Nodestream uses <code>pyyaml</code> to load our pipelines.</p>"},{"location":"docs/guides/creating-your-own-argument-resolver/#registering-your-argumentresolver","title":"Registering your ArgumentResolver","text":"<p>ArgumentResolvers are registered via the entry_points API of a Python Package. Specifically, the <code>entry_point</code> named <code>argument_resolvers</code> inside of the <code>nodestream.plugins</code> group is loaded. It is expected to be a subclass of <code>nodestream.pipeline.argument_resolvers:ArgumentResolver</code> as directed above.</p> <p>The <code>entry_point</code> should be a module that contains at least one argument resolver class. At runtime, the module will be loaded and all classes that inherit from <code>nodestream.pipeline.argument_resolvers:ArgumentResolver</code> will be registered.</p> <p>Depending on how you are building your package, you can register your Argument Resolver plugin in one of the following ways:</p> pyproject.tomlpyproject.toml (poetry)setup.cfgsetup.py TOML<pre><code>[project.entry-points.\"nodestream.plugins\"]\nargument_resolvers = \"nodestream_plugin_cool.argument_resolvers\"\n</code></pre> TOML<pre><code>[tool.poetry.plugins.\"nodestream.plugins\"]\nargument_resolvers = \"nodestream_plugin_cool.argument_resolvers\"\n</code></pre> INI<pre><code>[options.entry_points]\nnodestream.plugins =\nargument_resolvers = nodestream_plugin_cool.argument_resolvers\n</code></pre> Python<pre><code>from setuptools import setup\n\nsetup(\n    # ...,\n    entry_points = {\n        'nodestream.plugins': [\n            'argument_resolvers = nodestream_plugin_cool.argument_resolvers',\n        ]\n    }\n)\n</code></pre>"},{"location":"docs/guides/creating-your-own-argument-resolver/#using-your-argumentresolver","title":"Using your ArgumentResolver","text":"<p>You can now use your <code>ArgumentResolver</code> anywhere in the pipeline document. For instance, lets configure our database with the password:</p> YAML<pre><code>- implementation: nodestream.databases:GraphDatabaseWriter\narguments:\nbatch_size: 1000\ndatabase: neo4j\nuri: bolt://localhost:7687\nusername: neo4j\npassword: !secrets_manager my_neo_db_password\n</code></pre>"},{"location":"docs/guides/creating-your-own-audits/","title":"Creating Your Own Audits","text":"<p>Audits are used to validate the state of a project. They are run via the <code>nodestream audit {{audit name}}</code> command. Audits are intended to be used to validate the state of a project, but can be used to enforce any arbitrary rules about the project including things like naming conventions, pipeline structure, or other project-specific rules.</p>"},{"location":"docs/guides/creating-your-own-audits/#creating-an-audit","title":"Creating an Audit","text":"<p>To create an audit, you must create a class that inherits from <code>nodestream.project.audits:Audit</code>. The class must implement the <code>run</code> method, which takes a single argument, <code>project</code>, and returns nothing. During the execution of the audit, you can call <code>self.failure</code> to indicate that the audit has failed or <code>self.warning</code> to indicate that there is a non-fatal issue with the project. Both methods take a single argument, <code>message</code>, which is a string describing the failure or warning.</p> Python<pre><code>from nodestream.project import Project\nfrom nodestream.project.audits import Audit\n\nclass MyAudit(Audit):\n    name = \"my-audit\"\n\n    def run(self, project: Project) -&gt; None:\n        pass\n</code></pre>"},{"location":"docs/guides/creating-your-own-audits/#use-case-enforcing-pipeline-naming-conventions","title":"Use Case: Enforcing Pipeline Naming Conventions","text":"<p>Audits can be used to enforce pipeline naming conventions. As an example, lets say we want to enforce that all pipelines in our project are named in snake case. We can do this by iterating over all of the pipelines in the project and checking that the name is in snake case. If it is not, we can call <code>self.failure</code> to indicate that the audit has failed.</p> Python<pre><code>from nodestream.project import Project\nfrom nodestream.project.audits import Audit\n\nclass MyAudit(Audit):\n    name = \"my-audit\"\n\n    def is_snake_case(self, name: str) -&gt; bool:\n        return name.replace(\"_\", \"\").islower()\n\n    def run(self, project: Project) -&gt; None:\n        for scope in project.scopes_by_name.values():\n            for pipeline in scope.pipelines_by_name.values():\n                if not self.is_snake_case(pipeline.name):\n                    self.failure(f\"Pipeline {pipeline.name} is not in snake case\")\n</code></pre>"},{"location":"docs/guides/creating-your-own-audits/#registering-the-audit","title":"Registering the Audit","text":"<p>Audits are registered via the entry_points API of a Python Package. Specifically, the <code>entry_point</code> named <code>audits</code> inside of the <code>nodestream.plugins</code> group is loaded. It is expected to be a subclass of <code>nodestream.project.audits:Audit</code> as directed above. An instance of the class is created and the <code>run</code> method is called with an instance of <code>nodestream.project:Project</code>.</p> <p>The <code>entry_point</code> should be a module that contains at least one audit class. At runtime, the module will be loaded and all classes that inherit from <code>nodestream.project.audits:Audit</code> will be registered. The <code>name</code> attribute of the class will be used as the name of the audit.</p> <p>Depending on how you are building your package, you can register your audit plugin in one of the following ways:</p> pyproject.tomlpyproject.toml (poetry)setup.cfgsetup.py TOML<pre><code>[project.entry-points.\"nodestream.plugins\"]\naudits = \"nodestream_plugin_cool.audits\"\n</code></pre> TOML<pre><code>[tool.poetry.plugins.\"nodestream.plugins\"]\naudits = \"nodestream_plugin_cool.audits\"\n</code></pre> INI<pre><code>[options.entry_points]\nnodestream.plugins =\naudits = nodestream_plugin_cool.audits\n</code></pre> Python<pre><code>from setuptools import setup\n\nsetup(\n    # ...,\n    entry_points = {\n        'nodestream.plugins': [\n            'audits = nodestream_plugin_cool.audits',\n        ]\n    }\n)\n</code></pre>"},{"location":"docs/guides/creating-your-own-audits/#running-audits","title":"Running Audits","text":"<p>Audits are run via the <code>nodestream audit</code> command. The command takes a single argument, the name of the audit to run. The audit name is the <code>name</code> attribute of the audit class. For example, if we wanted to run the audit we created above, we would run <code>nodestream audit my-audit</code>.</p> <p>The <code>nodestream audit</code> command takes the following options:</p> <ul> <li><code>--project</code>: The path to the project file. Defaults to the current working directory's <code>nodestream.yaml</code> file.</li> </ul>"},{"location":"docs/guides/creating-your-own-commands/","title":"Creating your own Commands","text":"<p>Commands are used to interact with nodestream from the command line.</p>"},{"location":"docs/guides/creating-your-own-commands/#creating-your-own-command","title":"Creating your own command","text":"<p>Commands are implemented as cleo commands via a subclass of <code>nodestream.cli.commands:NodestreamCommand</code>.</p> Python<pre><code>from nodestream.cli.commands import NodestreamCommand\n\nclass MyCommand(NodestreamCommand):\n    name = \"my-command\"\n    description = \"My command description\"\n\n    async def handle_async(self) -&gt; None:\n        self.line(\"Hello World!\")\n</code></pre>"},{"location":"docs/guides/creating-your-own-commands/#operations","title":"Operations","text":"<p>Commands are typically executed as a series of operations. Operations are defined as seperate classes that from <code>nodestream.cli.operations:Operation</code>.</p> Python<pre><code>from nodestream.cli.command import NodestreamCommand\nfrom nodestream.cli.operations import Operation\n\nclass MyOperation(Operation):\n    async def perform(self, command: NodestreamCommand):\n        command.line(\"Hello World!\")\n</code></pre> <p>And then used as part of a command like so:</p> Python<pre><code>from nodestream.cli.commands import NodestreamCommand\n\nclass MyCommand(NodestreamCommand):\n    name = \"my-command\"\n    description = \"My command description\"\n\n    async def handle_async(self) -&gt; None:\n        operation = MyOperation()\n        await self.run_operation(operation)\n</code></pre>"},{"location":"docs/guides/creating-your-own-commands/#command-arguments","title":"Command Arguments","text":"<p>Commands can take arguments and options from the command line. These are defined as class attributes on the command class.</p> Python<pre><code>from cleo.helpers import argument, option\n\nfrom nodestream.cli.commands import NodestreamCommand\n\nclass MyCommand(NodestreamCommand):\n    name = \"my-command\"\n    description = \"My command description\"\n    arguments = [\n        argument(\"name\", \"the name of the project to create\"),\n    ]\n\n    options = [\n        option(\"shout\", \"s\", \"Yell the message\"),\n    ]\n\n    async def handle_async(self) -&gt; None:\n        name = self.argument(\"name\")\n        message = f\"Hello {name}!\"\n        if self.option(\"shout\"):\n            message = message.upper()\n        self.line(message)\n</code></pre>"},{"location":"docs/guides/creating-your-own-commands/#registering-the-command","title":"Registering the Command","text":"<p>Commands are registered via the entry_points API of a Python Package. Specifically, the <code>entry_point</code> named <code>commands</code> inside of the <code>nodestream.plugins</code> group is loaded. All commands are expected to be a subclass of <code>nodestream.cli.commands:NodestreamCommand</code> as directed above.</p> <p>The <code>entry_point</code> should be a module that contains at least one command class. At runtime, the module will be loaded and all classes that inherit from <code>nodestream.cli.commands:NodestreamCommand</code> will be registered. The <code>name</code> and <code>description</code> attributes of the class will be used as as the came and description of the command used in the command line and its help output.</p> <p>Depending on how you are building your package, you can register your command plugin in one of the following ways:</p> pyproject.tomlpyproject.toml (poetry)setup.cfgsetup.py TOML<pre><code>[project.entry-points.\"nodestream.plugins\"]\ncommands = \"nodestream_plugin_cool.commands\"\n</code></pre> TOML<pre><code>[tool.poetry.plugins.\"nodestream.plugins\"]\ncommands = \"nodestream_plugin_cool.commands\"\n</code></pre> INI<pre><code>[options.entry_points]\nnodestream.plugins =\ncommands = nodestream_plugin_cool.audits\n</code></pre> Python<pre><code>from setuptools import setup\n\nsetup(\n    # ...,\n    entry_points = {\n        'nodestream.plugins': [\n            'commands = nodestream_plugin_cool.audits',\n        ]\n    }\n)\n</code></pre>"},{"location":"docs/guides/creating-your-own-commands/#running-a-command","title":"Running a command","text":"<p>Commands are then run via the <code>nodestream</code> command line tool.</p> Bash<pre><code>$ nodestream my-command\nHello World!\n</code></pre>"},{"location":"docs/guides/creating-your-own-extractor/","title":"Creating an Extractor","text":"<p>An extractor is a component that extracts data from a source and yields it to the pipeline. Extractors are used to extract data from a variety of sources, including files, databases, and streams. Extractors are defined as classes that inherit from <code>nodestream.pipeline:Extractor</code>. The class must implement the <code>extract_records</code> method, which takes no arguments and is an async generator for the extracted data.</p> <p>For example, lets implement a crude file extractor that reads a file and yields each line as a record:</p> <p>NOTE: If you want to implement a file extractor, you should use the <code>FileExtractor</code> class instead of implementing your own.</p> Python<pre><code>from nodestream.pipeline import Extractor\n\nclass FileExtractor(Extractor):\n    def __init__(self, file_path):\n        self.file_path = file_path\n\n    async def extract_records(self):\n        with open(self.file_path, \"r\") as f:\n            for line in f:\n                yield line\n</code></pre>"},{"location":"docs/guides/creating-your-own-extractor/#using-an-extractor","title":"Using an Extractor","text":"<p>To use an extractor, you must add it to your pipeline configuration. For example, the following pipeline configuration uses the <code>FileExtractor</code> defined above:</p> YAML<pre><code>- implementation: my_module:FileExtractor\narguments:\nfile_path: /path/to/file\n</code></pre>"},{"location":"docs/guides/creating-your-own-file-format-handler/","title":"Handling a File Format","text":"<p>There are instances where you might need to support custom file formats for reading data into a nodestream pipeline's <code>FileExtractor</code>. This guide will walk you through the process of creating your own file format implementation for your project.</p>"},{"location":"docs/guides/creating-your-own-file-format-handler/#define-your-file-format-class","title":"Define Your File Format Class","text":"<p>To create your own file format, you need to define a class that implements the required behavior. Let's say we want to create a file format for reading JSON files. Here's an example implementation:</p> Python<pre><code>import yaml\nfrom io import StringIO\nfrom typing import Iterable\n\nfrom nodestream.extractors.files import SupportedFileFormat\n\nclass YamlFileFormat(SupportedFileFormat, alias=\".yaml\"):\n    def read_file_from_handle(self, fp: StringIO) -&gt; Iterable[dict]:\n        return [yaml.safe_load(fp)]\n</code></pre> <p>In this example, <code>YamlFileFormat</code> is a class that reads a YAML file from a file handle (<code>fp</code>) and returns an iterable of dictionaries representing the YAML document. In order to support your file format, you need to implement the method <code>read_file_from_handle</code>. This method reads the file and returns the data in a desired format.</p> <p>The method signature should match the expected behavior of the project or the specific module you're working with. In our example, the <code>read_file_from_handle</code> method takes a <code>StringIO</code> file handle and returns an iterable of dictionaries.</p>"},{"location":"docs/guides/creating-your-own-file-format-handler/#register-your-file-format","title":"Register Your File Format","text":"<p>File Formats are registered via the <code>entry_points</code> API of a Python Package. Specifically, the <code>entry_point</code> named <code>file_formats</code> inside of the <code>nodestream.plugins</code> group is loaded. It is expected to be a subclass of <code>nodestream.extractors.files:SupportedFileFormat</code> as directed above.</p> <p>The <code>entry_point</code> should be a module that contains at least one file format class. At runtime, the module will be loaded and all classes that inherit from <code>nodestream.extractors.files:SupportedFileFormat</code> will be registered. The <code>alias</code> attribute of the class will be used as the name of the file format. For example, the <code>YamlFileFormat</code> class above has an <code>alias</code> of <code>.yaml</code>. This means that the file format can be referenced in the <code>format</code> configuration option of the <code>FileExtractor</code> as <code>.yaml</code>. Additionally, files that end with <code>.yaml</code> will automatically be detected as files that should be parsed with the <code>YamlFileFormat</code>.</p> <p>Depending on how you are building your package, you can register your file format plugin in one of the following ways:</p> pyproject.tomlpyproject.toml (poetry)setup.cfgsetup.py TOML<pre><code>[project.entry-points.\"nodestream.plugins\"]\nfile_formats = \"nodestream_plugin_cool.file_formats\"\n</code></pre> TOML<pre><code>[tool.poetry.plugins.\"nodestream.plugins\"]\nfile_formats = \"nodestream_plugin_cool.file_formats\"\n</code></pre> INI<pre><code>[options.entry_points]\nnodestream.plugins =\nfile_formats = nodestream_plugin_cool.file_formats\n</code></pre> Python<pre><code>setup(\n    ...\n    entry_points={\n        \"nodestream.plugins\": [\n            \"file_formats = nodestream_plugin_cool.file_formats\"\n        ]\n    },\n    ...\n)\n</code></pre>"},{"location":"docs/guides/creating-your-own-interpretation/","title":"Creating A New Interpretation","text":"<p>Sometimes parsing data is extremely complex meaning its impossible to rely on the <code>Interpretation</code> DSL to handle every possible permutation of different data. To handle this, the <code>Interpretation</code> system is plug-able.</p> <p>Here's an example. Let's say you want to store a boolean property but also want to store the negative property. For instance, you want to store both an <code>enabled</code> and <code>disabled</code> property where the one is the opposite value of the other.</p> <p>For more information on interpretations see the Tutorial and the Interpreter Reference.</p>"},{"location":"docs/guides/creating-your-own-interpretation/#define-your-interpretation-class","title":"Define Your Interpretation Class","text":"<p>You can create a python file in your project, for example <code>interpretations.py</code>. If you generated your project from <code>nodestream new project</code>, you will already have this file. Now let's write some code:</p> Python<pre><code>from nodestream.interpreting import Interpretation\n\nclass MemoizeNegativeProperty(Interpretation, alias=\"memoize_negative\"):\n    pass\n</code></pre> <p>As you might imagine, this isn't particularly interesting. But, the <code>name=\"memoize_negative\"</code> might have caught your eye. <code>Interpretation</code>s are part of a unique registry. The <code>alias</code> property corresponds with the <code>type</code> property that is covered in the Interpreter Reference section. Functionally, all other keys in the object are forwarded to this classes constructor.</p> <p>Given that, let's consider our <code>MemoizeNegativeProperty</code> class. That implies that we could write down a constructor like this:</p> Python<pre><code>from nodestream.interpreting import Interpretation\nfrom nodestream.pipeline.value_providers import ValueProvider\n\nclass MemoizeNegativeProperty(Interpretation, alias=\"memoize_negative\"):\n    def __init__(self, positive_name: str, negative_name, value: ValueProvider):\n        # set properties\n</code></pre> <p>This code is type annotated. As you can see, <code>ValueProvider</code> is a new concept. A <code>ValueProvider</code> is something like <code>!jmespath</code> or <code>!variable</code>. <code>nodestream</code> has a well documented model layer and its worth understanding the API that the model layer provides if you start digging around more with extending nodestream.</p> <p>Let's complete our implementation. Perhaps unsurprisingly, <code>Interpretation</code> subclasses need to implement an <code>interpret</code> method. A working implementation of <code>MemoizeNegativeProperty</code> could look like this:</p> Python<pre><code>from nodestream.interpreting import Interpretation\nfrom nodestream.pipeline.value_providers import ValueProvider\n\nclass MemoizeNegativeProperty(Interpretation, alias=\"memoize_negative\"):\n    # __init__ omitted\n\n    def interpret(self, context: InterpretationContext):\n        source_node_properties = context.desired_ingest.source.properties\n        actual_value = self.value.provide_single_value(context)\n        source_node_properties.set_property(self.positive_name, actual_value)\n        source_node_properties.set_property(self.negative_name, not actual_value)\n</code></pre> <p>Again, it will be valuable to read the API details on nodestream's model. The above code leverages the aforementioned <code>InterpretationContext</code> as well as <code>DesiredIngest</code> and <code>PropertySet</code>.</p>"},{"location":"docs/guides/creating-your-own-interpretation/#register-your-interpretation","title":"Register Your Interpretation","text":"<p>Interpretations are registered via the entry_points API of a Python Package. Specifically, the <code>entry_point</code> named <code>interpretations</code> inside of the <code>nodestream.plugins</code> group is loaded. Every <code>Interpretation</code>  is expected to be a subclass of <code>nodestream.interpreting:Interpretation</code> as directed above.</p> <p>The <code>entry_point</code> should be a module that contains at least one <code>Interpretation</code> class. At runtime, the module will be loaded and all classes that inherit from <code>nodestream.interpreting:Interpretation</code> will be registered. The <code>alias</code> attribute of the class will be used as as the name of the tag used in the yaml pipeline.</p> <p>Depending on how you are building your package, you can register your <code>Interpretation</code> plugin in one of the following ways:</p> pyproject.tomlpyproject.toml (poetry)setup.cfgsetup.py TOML<pre><code>[project.entry-points.\"nodestream.plugins\"]\ninterpretations = \"nodestream_plugin_cool.interpretations\"\n</code></pre> TOML<pre><code>[tool.poetry.plugins.\"nodestream.plugins\"]\ninterpretations = \"nodestream_plugin_cool.interpretations\"\n</code></pre> INI<pre><code>[options.entry_points]\nnodestream.plugins =\ninterpretations = nodestream_plugin_cool.interpretations\n</code></pre> Python<pre><code>setup(\n    ...\n    entry_points={\n        \"nodestream.plugins\": [\n            \"interpretations = nodestream_plugin_cool.interpretations\"\n        ]\n    },\n    ...\n)\n</code></pre>"},{"location":"docs/guides/creating-your-own-interpretation/#use-your-interpretation","title":"Use Your Interpretation","text":"<p>Now that you've defined your interpretation, you can use it in your pipeline. For example:</p> YAML<pre><code># ... other pipeline steps\n- implementation: nodestream.interpreting:Interpreter\narguments:\ninterpretations:\n# ... other interpretations\n- type: memoize_negative\npositive_name: enabled\nnegative_name: disabled\nvalue: !jmespath \"enabled\"\n</code></pre>"},{"location":"docs/guides/creating-your-own-normalizer/","title":"Creating A Normalizer","text":"<p>A <code>Normalizer</code> allows you to clean data extracted by a <code>ValueProvider</code>. They are intended to provided stateless, simple transformations of data. Nodestream has some built in ones that you can view here.</p> <p>For example, assume that you have numeric numbers that should be rounded to a whole number before being used. Let's build a normalizer that does this for us.</p>"},{"location":"docs/guides/creating-your-own-normalizer/#defining-your-normalizer-class","title":"Defining your Normalizer Class","text":"Python<pre><code>from typing import Any\n\nfrom nodestream.pipeline.normalizers import Normalizer\n\nclass RoundToWholeNumber(Normalizer, alias=\"round_numbers\"):\n    def normalize_value(self, value: Any) -&gt; Any:\n        return int(value) if isinstance(value, float) else value\n</code></pre>"},{"location":"docs/guides/creating-your-own-normalizer/#registering-your-normalizer","title":"Registering your Normalizer","text":"<p>Normalizers are registered via the entry_points API of a Python Package. Specifically, the <code>entry_point</code> named <code>normalizers</code> inside of the <code>nodestream.plugins</code> group is loaded. Every Value Provider is expected to be a subclass of <code>nodestream.pipeline.normalizers:Normalizer</code> as directed above.</p> <p>The <code>entry_point</code> should be a module that contains at least one <code>Normalizer</code> class. At runtime, the module will be loaded and all classes that inherit from <code>nodestream.pipeline.normalizers:Normalizer</code> will be registered.</p> <p>Depending on how you are building your package, you can register your <code>Normalizer</code> plugin in one of the following ways:</p> pyproject.tomlpyproject.toml (poetry)setup.cfgsetup.py TOML<pre><code>[project.entry-points.\"nodestream.plugins\"]\nnormalizers = \"nodestream_plugin_cool.normalizers\"\n</code></pre> TOML<pre><code>[tool.poetry.plugins.\"nodestream.plugins\"]\nnormalizers = \"nodestream_plugin_cool.normalizers\"\n</code></pre> INI<pre><code>[options.entry_points]\nnodestream.plugins =\nnormalizers = nodestream_plugin_cool.normalizers\n</code></pre> Python<pre><code>setup(\n    ...\n    entry_points={\n        \"nodestream.plugins\": [\n            \"normalizers = nodestream_plugin_cool.normalizers\"\n        ]\n    },\n    ...\n)\n</code></pre>"},{"location":"docs/guides/creating-your-own-normalizer/#using-your-normalizer","title":"Using your Normalizer","text":"<p>You can now use your normalizer in sections that handle normalization flags. For more information, see the Interpretation's reference. For example, if you are using this in a source node interpretation, you could use it as follows:</p> YAML<pre><code>interpretations:\n- type: source_node\nnode_type: Metric\nkey:\nvalue: !jmespath value\nnormalization:\ndo_round_numbers: true\n</code></pre> <p>Note that the key for the flag is simply the concatenation of <code>round_numbers</code> from the <code>Normalizer</code> name and <code>do_</code></p>"},{"location":"docs/guides/creating-your-own-transformer/","title":"Creating A Transformer","text":"<p>A transformer is a class that takes a record as input and yields one or more output records. They are intended for pre-interpretation data transformations.</p>"},{"location":"docs/guides/creating-your-own-transformer/#defining-a-transformer","title":"Defining A Transformer","text":"<p>To define a transformer, you must create a class that inherits from <code>nodestream.pipeline:Transformer</code>. The class must implement the <code>transform_record</code> method, which takes a single argument, <code>record</code>, and yields one or more output records.</p> <p>For example, the following transformer takes a record with the following shape:</p> JSON<pre><code>{\n\"team\": \"Data Science\",\n\"people\": [\n{\"name\": \"Joe\", \"age\": 25},\n{\"name\": \"John\", \"age\": 34},\n]\n}\n</code></pre> <p>and yields the following records:</p> JSON<pre><code>{\n\"name\": \"Joe\",\n\"age\": 25,\n\"team\": \"Data Science\"\n}\n</code></pre> JSON<pre><code>{\n\"name\": \"John\",\n\"age\": 34,\n\"team\": \"Data Science\"\n}\n</code></pre> <p>The transformer is defined as follows:</p> Python<pre><code>from nodestream.pipeline import Transformer\n\nclass FlattenPeopleTransformer(Transformer):\n    def __init__(self, people_field):\n        self.people_field = people_field\n\n    def transform_record(self, record):\n        team = record[\"team\"]\n        for person in record[self.people_field]:\n            person[\"team\"] = team\n            yield person\n</code></pre>"},{"location":"docs/guides/creating-your-own-transformer/#using-a-transformer","title":"Using A Transformer","text":"<p>To use a transformer, you must add it to your pipeline configuration. For example, the following pipeline configuration uses the <code>FlattenPeopleTransformer</code> defined above:</p> YAML<pre><code>- implementation: my_module:FlattenPeopleTransformer\narguments:\npeople_field: people\n\n# pipeline to be continued...\n</code></pre>"},{"location":"docs/guides/creating-your-own-value-provider/","title":"Creating A Value Provider","text":"<p>There are many methods of extracting and providing data to the ETl pipeline as it operates. The various yaml tags such as <code>!jq</code> or <code>!variable</code> refer to an underlying <code>ValueProvider</code>. </p>"},{"location":"docs/guides/creating-your-own-value-provider/#creating-a-value-provider_1","title":"Creating a Value Provider","text":"<p>In order to introduce your own mechanism for providing values you can create your own subclass of <code>ValueProvider</code>.</p> Python<pre><code>from nodestream.pipeline.value_providers import ValueProvider\n\nclass HashValueProvider(ValueProvider):\n    pass\n</code></pre> Python<pre><code>from nodestream.pipeline.value_providers import ValueProvider, ProviderContext\n\nclass HashValueProvider(ValueProvider):\n    def single_value(self, context: ProviderContext) -&gt; Any:\n        ...\n\n    def many_values(self, context: ProviderContext) -&gt; Iterable[Any]:\n        ....\n</code></pre> Python<pre><code>from typing import Any, Iterable\n\nfrom nodestream.pipeline.value_providers import ValueProvider, ProviderContext\nfrom some_hashing_library import hash_value\n\n\nclass HashValueProvider(ValueProvider):\n    def __init__(self, value_provider_to_hash: ValueProvider):\n        self.value_provider_to_hash = value_provider_to_hash\n\n    def single_value(self, context: ProviderContext) -&gt; Any:\n        return hash_value(self.value_provider_to_hash.single_value(context))\n\n    def many_values(self, context: ProviderContext) -&gt; Iterable[Any]:\n        for value in self.value_provider_to_hash.many_values(context):\n            yield hash_value(value)\n</code></pre> <p>Now that we have implemented the hashing behavior, we'd like to use it. However, currently our <code>HashValueProvider</code> is not constructable in our pipelines. To accomplish this, we need to register a yaml loader that can register a tag in yaml that can instantiate our new value provider. Nodestream uses <code>pyyaml</code> to load our pipelines. For our purposes, our loader can be created by doing the following:</p> Python<pre><code># other imports omitted\n\nfrom typing import Type\n\nfrom yaml import SafeLoader\n\n\nclass HashValueProvider(ValueProvider):\n    # remainder of implementation omitted.\n\n    @classmethod\n    def install_yaml_tag(cls, loader: Type[SafeLoader]):\n        loader.add_constructor(\n            \"!hash\", lambda loader, node: cls(value_provider_to_hash=loader.construct_mapping(node)[\"hash_value\"])\n        )\n</code></pre>"},{"location":"docs/guides/creating-your-own-value-provider/#registering-the-value-provider","title":"Registering the Value Provider","text":"<p>ValueProviders are registered via the entry_points API of a Python Package. Specifically, the <code>entry_point</code> named <code>value_providers</code> inside of the <code>nodestream.plugins</code> group is loaded. Every Value Provider is expected to be a subclass of <code>nodestream.pipeline.value_providers:ValueProvider</code> as directed above. </p> <p>The <code>entry_point</code> should be a module that contains at least one Value Provider class. At runtime, the module will be loaded and all classes that inherit from <code>nodestream.pipeline.value_providers:ValueProvider</code> will be registered. </p> <p>Depending on how you are building your package, you can register your Value Provider plugin in one of the following ways:</p> pyproject.tomlpyproject.toml (poetry)setup.cfgsetup.py TOML<pre><code>[project.entry-points.\"nodestream.plugins\"]\nvalue_providers = \"nodestream_plugin_cool.value_providers\"\n</code></pre> TOML<pre><code>[tool.poetry.plugins.\"nodestream.plugins\"]\nvalue_providers = \"nodestream_plugin_cool.value_providers\"\n</code></pre> INI<pre><code>[options.entry_points]\nnodestream.plugins =\nvalue_providers = nodestream_plugin_cool.value_providers\n</code></pre> Python<pre><code>setup(\n    ...\n    entry_points={\n        \"nodestream.plugins\": [\n            \"value_providers = nodestream_plugin_cool.value_providers\"\n        ]\n    },\n    ...\n)\n</code></pre>"},{"location":"docs/guides/customizing-the-stream-extractor/","title":"Customizing the Stream Extractor","text":"<p>The <code>StreamExtractor</code> is responsible for extracting data from a stream source and converting it into a stream of records. The <code>StreamExtractor</code> is a subclass of <code>nodestream.pipeline.extractors:Extractor</code> and is responsible for the following: </p> <ul> <li>Polling data from the stream source</li> <li>Converting the raw data into a stream of records</li> <li>Yielding the records to the <code>Pipeline</code></li> </ul> <p>The <code>StreamExtractor</code> is configured via a series of configurations options. To learn more about the <code>StreamExtractor</code> and its configuration options, see the Stream Extractor reference. The stream extractor and the format of the records on that stream are abstracted from the <code>StreamExtractor</code> itself via the <code>StreamConnector</code> and <code>RecordFormat</code> classes. This allows you to create a <code>StreamExtractor</code> that can be used with any stream source and any record format.</p>"},{"location":"docs/guides/customizing-the-stream-extractor/#creating-a-new-stream-connector","title":"Creating a new Stream Connector","text":"<p>If you want to connect to a different stream source, you can create a new <code>StreamConnector</code> subclass. The <code>StreamConnector</code> class is responsible for polling data from the stream source and yielding it to the <code>StreamExtractor</code>. The <code>StreamConnector</code> class is an abstract class, so you must implement the <code>poll</code> method, which takes no arguments and yields the raw data from the stream source.</p> <p>For example, the following <code>StreamConnector</code> subclass polls data from a file (as a trivial example):</p> Python<pre><code>from nodestream.pipeline.extractors.streams import StreamConnector\n\nclass FileConnector(StreamConnector, alias=\"file\"):\n    def __init__(self, file_path):\n        self.file_path = file_path\n\n    def poll(self):\n        with open(self.file_path, \"r\") as f:\n            yield f.read()\n</code></pre> <p>The <code>alias</code> class attribute is used to specify the name of the stream connector. This name is used in the <code>stream</code> configuration option of the <code>StreamExtractor</code> to specify which stream connector to use. For more information see the Stream Extractor reference.</p>"},{"location":"docs/guides/customizing-the-stream-extractor/#registering-the-stream-connector","title":"Registering the Stream Connector","text":"<p>Stream connectors are registered via the entry_points API of a Python Package. Specifically, the <code>entry_point</code> named <code>stream_connectors</code> inside of the <code>nodestream.plugins</code> group is loaded. It is expected to be a subclass of <code>nodestream.pipeline.extractors.streams:StreamConnector</code> as directed above.</p> <p>The <code>entry_point</code> should be a module that contains at least one stream connector class. At runtime, the module will be loaded and all classes that inherit from <code>nodestream.pipeline.extractors.streams:StreamConnector</code> will be registered. The <code>alias</code> attribute of the class will be used as the name of the stream connector.</p> <p>Depending on how you are building your package, you can register your stream connector plugin in one of the following ways:</p> pyproject.tomlpyproject.toml (poetry)setup.cfgsetup.py TOML<pre><code>[project.entry-points.\"nodestream.plugins\"]\nstream_connectors = \"nodestream_plugin_cool.stream_connectors\"\n</code></pre> TOML<pre><code>[tool.poetry.plugins.\"nodestream.plugins\"]\nstream_connectors = \"nodestream_plugin_cool.stream_connectors\"\n</code></pre> INI<pre><code>[options.entry_points]\nnodestream.plugins =\nstream_connectors = nodestream_plugin_cool.stream_connectors\n</code></pre> Python<pre><code>setup(\n    ...\n    entry_points={\n        \"nodestream.plugins\": [\n            \"stream_connectors = nodestream_plugin_cool.stream_connectors\"\n        ]\n    },\n    ...\n)\n</code></pre>"},{"location":"docs/guides/customizing-the-stream-extractor/#creating-a-new-record-format","title":"Creating a new Record Format","text":"<p>If your stream data comes in a different format, you can create a new <code>RecordFormat</code> subclass. The <code>RecordFormat</code> class is responsible for parsing the raw data from the stream connector and yielding it to the <code>StreamExtractor</code>. The <code>RecordFormat</code> class is an abstract class, so you must implement the <code>parse</code> method, which takes a single argument, <code>data</code>, and yields the parsed records.</p> <p>For example, the following <code>RecordFormat</code> subclass parses data as yaml:</p> Python<pre><code>from nodestream.pipeline.extractors.streams import RecordFormat\n\nclass YamlFormat(RecordFormat, alias=\"yaml\"):\n    def parse(self, data):\n        import yaml\n        yield from yaml.load_all(data)\n</code></pre> <p>The <code>alias</code> class attribute is used to specify the name of the record format. This name is used in the <code>format</code> configuration option of the <code>StreamExtractor</code> to specify which record format to use. For more information see the Stream Extractor reference.</p>"},{"location":"docs/guides/customizing-the-stream-extractor/#registering-the-record-format","title":"Registering the Record Format","text":"<p>Record formats are registered via the entry_points API of a Python Package. Specifically, the <code>entry_point</code> named <code>record_formats</code> inside of the <code>nodestream.plugins</code> group is loaded. It is expected to be a subclass of <code>nodestream.pipeline.extractors.streams:RecordFormat</code> as directed above. An instance of the class is created and the <code>parse</code> method is called with the raw data from the stream connector.</p> <p>The <code>entry_point</code> should be a module that contains at least one record format class. At runtime, the module will be loaded and all classes that inherit from <code>nodestream.pipeline.extractors.streams:RecordFormat</code> will be registered. The <code>alias</code> attribute of the class will be used as the name of the record format.</p> <p>Depending on how you are building your package, you can register your record format plugin in one of the following ways:</p> pyproject.tomlsetup.pysetup.cfg TOML<pre><code>[project.entry-points.\"nodestream.plugins\"]\nrecord_formats = \"nodestream_plugin_cool.record_formats\"\n</code></pre> Python<pre><code>setup(\n    ...\n    entry_points={\n        \"nodestream.plugins\": [\n            \"record_formats = nodestream_plugin_cool.record_formats\",\n        ],\n    },\n    ...\n)\n</code></pre> INI<pre><code>[options.entry_points]\nnodestream.plugins =\nrecord_formats = nodestream_plugin_cool.record_formats\n</code></pre>"},{"location":"docs/guides/implementing-pipeline-testing/","title":"Implemeting Pipeline Testing","text":""},{"location":"docs/guides/implementing-pipeline-testing/#overview","title":"Overview","text":"<p>Pipelines are the core of NodeStream.  They are the building blocks that allow you to define your data processing logic. Since so many parts of the application depend on pipelines.  It is important to test them thoroughly to ensure that the integration between the different components is working as expected. </p> <p>Nodestream has some built-in tools to help you test your pipelines. See the Project#get_snapshot_for method for more details on running pipelines.</p>"},{"location":"docs/guides/implementing-pipeline-testing/#examples","title":"Examples","text":""},{"location":"docs/guides/implementing-pipeline-testing/#pytest","title":"<code>pytest</code>","text":"Python<pre><code># tests/test_pipelines.py\n\nimport dataclasses\nimport json\n\nfrom nodestream.project import Project\n\nfrom freezegun import freeze_time\n\n# Step 1: Define a custom JSON encoder that will convert dataclasses to dicts.\nclass EnhancedJSONEncoder(json.JSONEncoder):\n    def default(self, o):\n        if dataclasses.is_dataclass(o):\n            return dataclasses.asdict(o)\n        return super().default(o)\n\n# Step 2: Freeze time so that the snapshot is deterministic.\n@freeze_time(\"2020-01-01\")\ndef test_pipeline_snapshot(snapshot):\n    # Step 3: Load your application project\n    project = Project.read_from_file(\"nodestream.yaml\")\n\n    # Step 4: Run the pipeline and get the results gathered as a list.\n    snapshot = project.get_snapshot_for(\"pipeline\")\n    snapshot_str = json.dumps(snapshot, cls=EnhancedJSONEncoder)\n\n    # Step 5: Assert that the results match the snapshot\n    #   Note: This will fail the first time you run it.\n    snapshot.snapshot_dir = \"snapshots\"\n    snapshot.assert_match(snapshot_str, \"test_name_snapshot.json\")\n</code></pre>"},{"location":"docs/guides/implementing-time-to-live/","title":"Implemeting Time to Live","text":"<p>This guide will walk you through the process of implementing a time to live (TTL) feature in your application. This feature will allow you to set a time limit on the amount of time a document is available in your database. After the time limit has expired, the document will be deleted from the database.</p> <p>This guide assumes that you have already created a NodeStream application. If you have not, please see the Getting Started guide.</p>"},{"location":"docs/guides/implementing-time-to-live/#overview","title":"Overview","text":"<p>The TTL feature is implemented by building two data pipelines. One for evicting nodes and one for evicting relationships. The evictor pipelines are intended to run at a specified interval. The evictor pipelines will delete the nodes and relationships that have expired.</p>"},{"location":"docs/guides/implementing-time-to-live/#implementing-the-node-evictor-pipeline","title":"Implementing the Node Evictor Pipeline","text":"<p>The node evictor pipeline will delete nodes that have expired. The pipeline will be implemented by creating a new extractor and a new writer. The extractor will read the node TTL configurations from the database and the writer will delete the nodes that have expired.</p>"},{"location":"docs/guides/implementing-time-to-live/#creating-the-node-ttl-evictor","title":"Creating the Node TTL Evictor","text":"<p>The node TTL configuration extractor will read the node TTL configurations from the pipeline. The extractor will be implemented by using the <code>TimeToLiveConfigurationExtractor</code> class. The extractor will be configured to read the node TTL configurations from the database.</p> pipelines/node-ttl-evictor.yaml<pre><code>- implementation: nodestream.pipeline.extractors:TimeToLiveConfigurationExtractor\narguments:\ngraph_object_type: NODE\nconfigurations:\n- object_type: Person\nexpiry_in_hours: 96\n- object_type: Occupation\nexpiry_in_hours: 48\n- implementation: nodestream.writers:GraphDatabaseWriter\narguments:\nbatch_size: 100\n# TODO: Fill in the database configuration for your application\n</code></pre> <p>You can view all the configuration options in the <code>TimeToLiveConfigurationExtractor</code> documentation here.</p>"},{"location":"docs/guides/implementing-time-to-live/#creating-the-relationship-ttl-evictor","title":"Creating the Relationship TTL Evictor","text":"<p>The relationship TTL Evictor is implemented in the same way as the node TTL Evictor. The only difference is that the <code>graph_object_type</code> argument is set to <code>RELATIONSHIP</code>.</p> pipelines/relationship-ttl-evictor.yaml<pre><code>- implementation: nodestream.pipeline.extractors:TimeToLiveConfigurationExtractor\narguments:\ngraph_object_type: RELATIONSHIP\nconfigurations:\n- object_type: REPORTS_TO\nexpiry_in_hours: 96\n- object_type: PERFORMS\nexpiry_in_hours: 48\n- implementation: nodestream.writers:GraphDatabaseWriter\narguments:\nbatch_size: 100\n# TODO: Fill in the database configuration for your application\n</code></pre> <p>Again, You can view all the configuration options in the <code>TimeToLiveConfigurationExtractor</code> documentation here.</p>"},{"location":"docs/guides/implementing-time-to-live/#running-the-evictor-pipelines","title":"Running the Evictor Pipelines","text":"<p>The evictor pipelines can be run at whatever interval is correct for your application. Note that the data will only be eviceted when the piplines are run and not the moment the TTL expires. The evictor pipelines can be run by using the <code>nodestream</code> command line tool.</p> Bash<pre><code>nodestream run node-ttl-evictor\nnodestream run relationship-ttl-evictor\n</code></pre>"},{"location":"docs/guides/project-plugins/","title":"Project Plugins","text":"<p>Project plugins are used to modify, extend, or in otherwise interact with a project. They are loaded when a project is created or opened.  They are intended for things like adding custom metadata to project pipelines, distributing additional piplines, or other project-oriented tasks.</p>"},{"location":"docs/guides/project-plugins/#creating-a-project-plugin","title":"Creating a Project Plugin","text":"<p>To create a project plugin, you must create a class that inherits from <code>nodestream.project:ProjectPlugin</code>. The class must implement the <code>activate</code> method, which takes a single argument, <code>project</code>, and returns nothing. The <code>project</code> argument is an instance of <code>nodestream.project:Project</code>.</p> Python<pre><code>from nodestream.project import Project, ProjectPlugin\n\nclass MyProjectPlugin(ProjectPlugin):\n    def activate(self, project: Project) -&gt; None:\n        pass\n</code></pre>"},{"location":"docs/guides/project-plugins/#use-case-adding-custom-metadata-to-project-pipelines","title":"Use Case: Adding Custom Metadata to Project Pipelines","text":"<p>Project plugins can be used to add custom metadata to project pipelines. This metadata can be used for a variety of purposes specific to your  application. As an example, lets say we want to set cron-like schedules for our project pipelines. We can do this by adding a <code>schedule</code> field to the pipeline metadata. We can then use this metadata to schedule our pipelines to run at the appropriate times. As an example, we could search  for any pipeline whose <code>name</code> contains <code>daily</code> and add a <code>schedule</code> annotation to it.</p> Python<pre><code>from nodestream.project import Project, ProjectPlugin\n\nclass MyProjectPlugin(ProjectPlugin):\n    def activate(self, project: Project) -&gt; None:\n        for scope in project.scopes_by_name.values():\n            for pipeline in scope.pipelines_by_name.values():\n                if 'daily' in pipeline.name:\n                    pipeline.annotations['schedule'] = '0 0 * * *'\n</code></pre>"},{"location":"docs/guides/project-plugins/#use-case-distributing-additional-pipelines","title":"Use Case: Distributing Additional Pipelines","text":"<p>Project plugins can be used to distribute additional pipelines with your application. As an example, lets say we want to distribute a set of  pipelines that interact with a specific API. These pipelines can be packaged as a plugin and distributed with <code>pypi</code> as a seperate package. The plugin can then be registered as a project plugin, and the pipelines will be added to any project that is opened.</p> <p>To support this, <code>PipelineScope</code> has a factory method that can be used to create an instance using the  <code>importlib.resources</code> api. This allows you to distribute pipelines as part of a python package, and load them into a project at runtime.</p> Python<pre><code>from nodestream.project import Project, ProjectPlugin, PipelineScope, PipelineDefinition\n\nclass MyProjectPlugin(ProjectPlugin):\n    def activate(self, project: Project) -&gt; None:\n        scope = PipelineScope.from_resources(name=\"my_plugin\", package=\"my_plugin.pipelines\")\n        project.add_scope(scope)\n</code></pre>"},{"location":"docs/guides/project-plugins/#registering-the-project-plugin","title":"Registering the Project Plugin","text":"<p>Project plugins are registered via  the entry_points API of a Python Package. Specifically, the <code>entry_point</code> named <code>project</code> inside of the <code>nodestream.plugins</code> group is loaded. All project plugins are expected to be a subclass of <code>nodestream.project:ProjectPlugin</code> as directed above. An instance of the class is created and the <code>activate</code> method is called with an instance of <code>nodestream.project:Project</code>.</p> <p>The <code>entry_point</code> should be a module that contains at least one <code>ProjectPlugin</code> class. At runtime, the module will be loaded and all classes that inherit from <code>nodestream.project:ProjectPlugin</code> will be registered.</p> <p>Depending on how you are building your package, you can register your project plugin in one of the following ways:</p> pyproject.tomlpyproject.toml (poetry)setup.cfgsetup.py TOML<pre><code>[project.entry-points.\"nodestream.plugins\"]\nprojects = \"nodestream_plugin_cool.plugin\"\n</code></pre> TOML<pre><code>[tool.poetry.plugins.\"nodestream.plugins\"]\nprojects = \"nodestream_plugin_cool.plugin\"\n</code></pre> INI<pre><code>[options.entry_points]\nnodestream.plugins =\nprojects = nodestream_plugin_cool.plugin\n</code></pre> Python<pre><code>from setuptools import setup\n\nsetup(\n    # ...,\n    entry_points = {\n        'nodestream.plugins': [\n            'projects = nodestream_plugin_cool.plugin',\n        ]\n    }\n)\n</code></pre>"},{"location":"docs/reference/argument-resovlers/","title":"Argument Resolvers","text":"<p>An <code>ArgumentResolver</code> allows you to inline a value into the Pipeline file before the pipeline is initialized. This can be useful for passing configuration from files, environment, secret stores, and the like. By default, nodestream ships with a few built in argument resolvers.</p>"},{"location":"docs/reference/argument-resovlers/#include","title":"<code>!include</code>","text":"<p>Loads another yaml file and inlined the contents of that file into the location its supplied. For example, imagine you want to store the list of file globs separate from the <code>FileExtractor</code> itself. The file path can be referenced relatively or absolutely.</p>"},{"location":"docs/reference/argument-resovlers/#example","title":"Example","text":"pipelines/file_targets.yaml<pre><code>- people/*.json\n- other_people/*.json\n</code></pre> pipelines/ingest_files.yaml<pre><code>- implementation: nodestream.pipeline.extractors:FileExtractor\narguments:\nglobs: !include file_targets.yaml\n</code></pre>"},{"location":"docs/reference/argument-resovlers/#env","title":"<code>!env</code>","text":"<p>Inline the value of an environment variable at the current location.</p>"},{"location":"docs/reference/argument-resovlers/#example_1","title":"Example","text":"Bash<pre><code>export MY_DATABASE_PASSWORD=$(cat database_password.txt)\nnodestream run pipeline_hitting_database\n</code></pre> pipelines/pipeline_hitting_database.yaml<pre><code>- implementation: nodestream.databases:GraphDatabaseWriter\narguments:\nbatch_size: 1000\ndatabase: neo4j\nuri: bolt://localhost:7687\nusername: neo4j\npassword: !env MY_DATABASE_PASSWORD\n</code></pre>"},{"location":"docs/reference/extractors/","title":"Extractors","text":""},{"location":"docs/reference/extractors/#streamextractor","title":"<code>StreamExtractor</code>","text":"<p>The <code>StreamExtractor</code> provides a convenient abstraction for extracting records from different types of streams by allowing customization of the underlying stream system and the record format. By implementing the <code>StreamConnector</code> and <code>StreamRecordFormat</code> subclasses, one can easily adapt the extraction process to various stream sources and record formats.</p> <p>The documentation below contains information on the supported <code>StreamConnector</code> and <code>StreamRecordFormat</code> options and how to configure them. See the Customizing The Stream Extractor guide to learn how to add your own implementations of these classes.</p>"},{"location":"docs/reference/extractors/#top-level-arguments","title":"Top Level Arguments","text":"YAML<pre><code>- implementation: nodestream.pipeline.extractors.streams:StreamExtractor\narguments:\n# rest of the stream extractor format arguments\ntimeout: 10 # default 60. Number of seconds to await records.\nmax_records: 1 # default 100. Max number of records to get at one time.\n</code></pre>"},{"location":"docs/reference/extractors/#streamconnector","title":"<code>StreamConnector</code>","text":"<p>The <code>StreamConnector</code> describes how to poll data from the underlying streaming mechanism.</p>"},{"location":"docs/reference/extractors/#kafka","title":"<code>Kafka</code>","text":"YAML<pre><code>- implementation: nodestream.pipeline.extractors.streams:StreamExtractor\narguments:\n# rest of the stream extractor format arguments\nconnector: kafka\ntopic: my-topic-with-data\ngroup_id: my_group_id\nbootstrap_servers:\n- localhost:9092\n- localhost:9093\n</code></pre>"},{"location":"docs/reference/extractors/#streamrecordformat","title":"<code>StreamRecordFormat</code>","text":"<p>The <code>StreamRecordFormat</code> parses the raw data from the <code>StreamConnector</code>.</p>"},{"location":"docs/reference/extractors/#json","title":"<code>json</code>","text":"<p>The <code>json</code> format simply calls <code>json.loads</code> on the data provided from the <code>StreamConnector</code>. To use it, you can set the <code>record_format</code> to be <code>json</code> in the <code>StreamExtractor</code> configuration. For example:</p> YAML<pre><code>- implementation: nodestream.pipeline.extractors.streams:StreamExtractor\narguments:\n# rest of the stream extractor format\nrecord_format: json\n</code></pre>"},{"location":"docs/reference/extractors/#athenaextractor","title":"<code>AthenaExtractor</code>","text":"<p>The <code>AthenaExtractor</code> issues a query to Amazon Athena, and returns yields each row as a record to the pipeline. For example, the following <code>AthenaExtractor</code> configuration:</p> YAML<pre><code>- implementation: nodestream.pipeline.extractors.stores.aws:AthenaExtractor\narguments:\nquery: SELECT name, version FROM python_package_versions;\nworkgroup: MY_WORKGROUP_NAME\ndatabase: package_registry_metadata;\noutput_location: s3://my_bucket/some_path\n</code></pre> <p>produces records with the following shape:</p> JSON<pre><code>{\"name\": \"nodestream\", \"version\": \"0.2.0\"}\n</code></pre>"},{"location":"docs/reference/extractors/#arguments","title":"Arguments","text":"Parameter Name Type Description query String The actual query to run. The results yielded by the extractor will reflect the shape of the data returned from the query. workgroup String The workgroup name to use to execute the query under. See the AWS Docs for more information. output_location String The output location string to store results for Athena. See the AWS Docs for more information. database String The name of the athena logical database to execute the query in. See the AWS Docs for more information. assume_role_arn String The ARN of a role to assume before interacting with the bucket. Of course the appropriate configuration is needed on both the current credentials as well as the target role. assume_role_external_id String The external id that is required to assume role. Only used when <code>assume_role_arn</code> is set and only needed when the role is configured to require an external id. **session_args Any Any other argument that you want sent to the boto3.Session that will be used to interact with AWS."},{"location":"docs/reference/extractors/#s3extractor","title":"<code>S3Extractor</code>","text":"<p>The <code>S3Extractor</code> pulls files down from S3 and yields the records read from each file using the appropriate file format parser. A simple example would look like this:</p> YAML<pre><code>- implementation: nodestream.pipeline.extractors.stores.aws:S3Extractor\narguments:\nbucket: my-awesome-bucket\n</code></pre>"},{"location":"docs/reference/extractors/#additional-arguments","title":"Additional Arguments","text":"<p>With the previous minimal configuration, it will use your currently active aws credentials to read all objects from <code>my-awesome-bucket</code>. However, there are many options you can add to this:</p> Parameter Name Type Description prefix String Filter the objects pulled from S3 to only the ones that have this prefix in the name. object_format String Regardless of the file's extension, use the format provided from the list of file format supported. assume_role_arn String The ARN of a role to assume before interacting with the bucket. Of course the appropriate configuration is needed on both the current credentials as well as the target role. assume_role_external_id String The external id that is required to assume role. Only used when <code>assume_role_arn</code> is set and only needed when the role is configured to require an external id. **session_args Any Any other argument that you want sent to the boto3.Session that will be used to interact with AWS."},{"location":"docs/reference/extractors/#fileextractor","title":"<code>FileExtractor</code>","text":"<p>The <code>FileExtractor</code> class represents an extractor that reads records from files specified by glob patterns. It takes a collection of file paths as input and yields the records read from each file using the appropriate file format parser.</p> YAML<pre><code>- implementation: nodestream.pipeline.extractors:FileExtractor\narguments:\nglobs:\n- people/*.json\n- other_people/*.json\n</code></pre>"},{"location":"docs/reference/extractors/#remotefileextractor","title":"<code>RemoteFileExtractor</code>","text":"<p>The <code>RemoteFileExtractor</code> class represents an extractor that reads records from files specified by URLs. It takes a collection of URLs as input and yields the records read from each file using the appropriate file format parser.</p> YAML<pre><code>- implementation: nodestream.pipeline.extractors:RemoteFileExtractor\narguments:\nmemory_spooling_max_size_in_mb: 10 # Optional\nurls:\n- https://example.com/people.json\n- https://example.com/other_people.json\n</code></pre> <p>The <code>RemoteFileExtractor</code> will use the value of <code>memory_spooling_max_size_in_mb</code> to determine how much memory to use when spooling the file contents.  If the file is larger than the specified amount, it will be downloaded to a temporary file on disk and read from there.  If the file is smaller than the specified amount, it will be downloaded to memory and read from there.  The default value is 5 MB.</p>"},{"location":"docs/reference/extractors/#simpleapiextractor","title":"<code>SimpleApiExtractor</code>","text":"<p>The <code>SimpleApiExtractor</code> class represents an extractor that reads records from a simple API. It takes a single URL as input and yields the records read from the API. The API must return a JSON array of objects either directly or as the value of specified key.</p> <p>For example, if the API returns the following JSON:</p> JSON<pre><code>{\n\"people\": [\n{\n\"name\": \"John Doe\",\n\"age\": 42\n},\n{\n\"name\": \"Jane Doe\",\n\"age\": 42\n}\n]\n}\n</code></pre> <p>Then the extractor can be configured as follows:</p> YAML<pre><code>- implementation: nodestream.pipeline.extractors:SimpleApiExtractor\narguments:\nurl: https://example.com/people\nyield_from: people\n</code></pre> <p>If the API returns a JSON array directly, then the <code>yield_from</code> argument can be omitted.</p> <p>The <code>SimpleApiExtractor</code> will automatically paginate through the API until it reaches the end if the API supports limit-offset style pagination through a query parameter.  By default, no pagination is performed. The query parameter name can be configured using the <code>offset_query_param</code> argument.</p> <p>For example, if the API supports pagination through a <code>page</code> query parameter, then the extractor can be configured as follows:</p> YAML<pre><code>- implementation: nodestream.pipeline.extractors:SimpleApiExtractor\narguments:\nurl: https://example.com/people\noffset_query_param: page\n</code></pre> <p>You can also specify headers to be sent with the request using the <code>headers</code> argument.</p> YAML<pre><code>- implementation: nodestream.pipeline.extractors:SimpleApiExtractor\narguments:\nurl: https://example.com/people\nheaders:\nx-api-key: !env MY_API_KEY\n</code></pre>"},{"location":"docs/reference/extractors/#timetoliveconfigurationextractor","title":"<code>TimeToLiveConfigurationExtractor</code>","text":"<p>\"Extracts\" time to live configurations from the file and yields them one at a time to the graph database writer.</p> <p>One can configure a Node TTL like this: YAML<pre><code>- implementation: nodestream.pipeline.extractors.ttl:TimeToLiveConfigurationExtractor\narguments:\ngraph_object_type: NODE\nconfigurations:\n- object_type: Person\nexpiry_in_hours: 96\n- object_type: Occupation\nexpiry_in_hours: 48\n</code></pre></p> <p>and one can configure a Relationship TTL like this:</p> YAML<pre><code>- implementation: nodestream.pipeline.extractors.ttl:TimeToLiveConfigurationExtractor\narguments:\ngraph_object_type: RELATIONSHIP\nconfigurations:\n- object_type: REPORTS_TO\nexpiry_in_hours: 96\n- object_type: PERFORMS\nexpiry_in_hours: 48\n</code></pre>"},{"location":"docs/reference/extractors/#arguments_1","title":"Arguments","text":"<p>Each configuration can include the following arguments:</p> Parameter Name Type Description object_type String The object type to apply the TTL to. expiry_in_hours Integer The number of hours after which the object should be deleted. enabled Boolean Whether or not the TTL is enabled. Defaults to <code>True</code>. batch_size Integer The number of objects to delete in a single batch. Defaults to <code>100</code>. custom_query String A custom query to use to delete the objects. If not provided, the default query will be used. The custom query is database implmentation specific."},{"location":"docs/reference/file-formats/","title":"File Formats","text":""},{"location":"docs/reference/file-formats/#json","title":"<code>.json</code>","text":"<p><code>.json</code> files are loaded using <code>json.load</code> and the one record is returned per file. The record is the entire parsed contents of the <code>.json</code> file.</p>"},{"location":"docs/reference/file-formats/#txt","title":"<code>.txt</code>","text":"<p><code>.txt</code> files are read line by line and one record is produced per line. The record is in the shape:</p> JSON<pre><code>{\"line\": \"{{the line}}\"}\n</code></pre>"},{"location":"docs/reference/file-formats/#csv","title":"<code>.csv</code>","text":"<p>A <code>.csv</code> file is loading using <code>csv.DictReader</code>. For each row of the file, one record will be produced. Assuming you have a csv file with the format:</p> Text Only<pre><code>name,age\nbob,29\n</code></pre> <p>You will get a record in the following shape:</p> JSON<pre><code>{\"name\": \"bob\", \"age\": 29}\n</code></pre>"},{"location":"docs/reference/graph-database-writer/","title":"Graph Database Writer","text":"<p>The graph database writer is a writer that writes the output of a ingestion pipeline to a graph database. Currently, only Neo4j is supported.</p>"},{"location":"docs/reference/graph-database-writer/#database-independent-options","title":"Database Independent Options","text":""},{"location":"docs/reference/graph-database-writer/#database","title":"<code>database</code>","text":"<p>The name of the database technology to use. Currently only <code>neo4j</code> is supported.</p>"},{"location":"docs/reference/graph-database-writer/#batch_size","title":"<code>batch_size</code>","text":"<p>The number of ingests to write to the database in a single batch. Defaults to 10000.</p>"},{"location":"docs/reference/graph-database-writer/#database-specific-options","title":"Database Specific Options","text":""},{"location":"docs/reference/graph-database-writer/#neo4j","title":"<code>neo4j</code>","text":""},{"location":"docs/reference/graph-database-writer/#uri","title":"<code>uri</code>","text":"<p>The URI of the Neo4j database to connect to.</p>"},{"location":"docs/reference/graph-database-writer/#username","title":"<code>username</code>","text":"<p>The username to use when connecting to the Neo4j database.</p>"},{"location":"docs/reference/graph-database-writer/#password","title":"<code>password</code>","text":"<p>The password to use when connecting to the Neo4j database.</p>"},{"location":"docs/reference/graph-database-writer/#database_name","title":"<code>database_name</code>","text":"<p>The name of the logical database to use. If not specified, the default database (neo4j) will be used.</p>"},{"location":"docs/reference/graph-database-writer/#use_enterprise_features","title":"<code>use_enterprise_features</code>","text":"<p>If set to <code>true</code>, the Neo4j Enterprise features will be used. Defaults to <code>false</code>.</p>"},{"location":"docs/reference/interpretations/","title":"Interpretations","text":""},{"location":"docs/reference/interpretations/#source-node-interpretation","title":"Source Node Interpretation","text":"Parameter Name Required? Type Description node_type Yes String or ValueProvider Specifies the type of the source node. It is a required field. When a  ValueProvider is used dynamic index creation and schema introspection are not supported. key Yes Dictionary Contains key-value pairs that define the key of the source node.  The keys represent field names, and the values can be either static values or value providers.  It is a required field. properties No Dictionary Stores additional properties of the source node. It is a dictionary  where the keys represent property names, and the values can be either  static values or value providers. This field is optional. additional_indexes No List[String] Specifies additional indexes for desired on the source node. It is a list of field names. This field is optional. additional_types No List[String] Defines additional types for the source node. It is a list of strings representing the additional types.  These types are not considered by ingestion system as part of the identity of the node and rather considered as  extra labels applied after the ingestion of the node is completed.  This field is optional. normalization No Dictionary Contains normalization flags that should be adopted by value providers when getting values. This field is optional. See the normalization reference.  By default <code>do_lowercase_strings</code> is enabled."},{"location":"docs/reference/interpretations/#relationship-interpretation","title":"Relationship Interpretation","text":"Parameter Name Required? Type Description node_type Yes String or ValueProvider Specifies the type of the node to relationship connects to. It is a required field. When a ValueProvider is used dynamic index creation and schema introspection are not supported. relationship_type Yes String or ValueProvider Specifies the type of the relationship. It is a required field. When a ValueProvider is used dynamic index creation and schema introspection are not supported. node_key Yes Dictionary Contains key-value pairs that define the key of the related node. The keys represent field names, and the values can be either static values or value providers. It is a required field. node_properties No Dictionary Stores additional properties of the related node. It is a dictionary where the keys represent property names, and the values can be either static values or value providers. This field is optional. relationship_key No Dictionary Contains key-value pairs that define the key of the relationship itself. The keys represent field names, and the values can be either static values or value providers. It is a required field. The uniqueness of the relationship is defined in terms of the nodes it is relating and the key of the relationship. relationship_properties No Dictionary Stores additional properties of the relationship It is a dictionary where the keys represent property names, and the values can be either static values or value providers. This field is optional. outbound No Boolean Represents whether or not the relationship direction is outbound from the source node. By default, this is true. find_many No Boolean Represents whether or not the searches provided to node_key can return multiple values, and thus should create multiple relationships to multiple related nodes. iterate_on No ValueProvider Iterates over the values provided by the supplied value provider, and creates an relationship for each one. match_strategy No <code>EAGER</code> | <code>MATCH_ONLY</code> | <code>FUZZY</code> Defaults to <code>EAGER</code>. When <code>EAGER</code>, related nodes will be created when not present based on the supplied node type and key. <code>MATCH_ONLY</code> will not create a relationship when the related node does not already exists. <code>FUZZY</code> behaves like <code>MATCH ONLY</code>, but treats the key values as regular expressions to match on. key_normalization No Dictionary Contains normalization flags that should be adopted by value providers when getting values for node and relationship keys. This field is optional. See the normalization reference.  By default  <code>do_lowercase_strings</code> is enabled. property_normalization No Dictionary Contains normalization flags that should be adopted by value providers when getting values for node and relationship properties. This field is optional. See the normalization reference. By default no flags are enabled."},{"location":"docs/reference/interpretations/#properties-interpretation","title":"Properties Interpretation","text":"Parameter Name Required? Type Description properties Yes Dictionary Stores additional properties of the source node. It is a dictionary where the keys represent property names, and the values can be either static values or value providers. This field is optional. normalization No Dictionary Contains normalization flags that should be adopted by value providers when getting values. This field is optional. See the normalization reference.  By default no flags are enabled."},{"location":"docs/reference/interpretations/#variables-interpretation","title":"Variables Interpretation","text":"Parameter Name Required? Type Description variables Yes Dictionary Stores values as variables that can be referenced later with the <code>!variable</code> value provider. It is a dictionary where the keys represent property names, and the values can be either static values or value providers. This field is optional. normalization No Dictionary Contains normalization flags that should be adopted by value providers when getting values. This field is optional. See the normalization reference.  By default no flags are enabled."},{"location":"docs/reference/interpretations/#switch-interpretation","title":"Switch Interpretation","text":"<p>The switch interpretation allows you to define multiple interpretations that will be applied. The interpretations that will be applied is determined by the value of the <code>switch_on</code> parameter. The value of the <code>switch_on</code> parameter is a value provider that will be evaluated for each source node. The value of the value provider will be used to determine which interpretation to apply. The interpretation that will be applied is the one that has the same value as the value of the <code>switch_on</code> parameter. If no interpretation has the same value as the value of the <code>switch_on</code> parameter, the default interpretation will be applied. If no default interpretation is defined, the source node will be ignored.</p> Parameter Name Required? Type Description switch_on Yes ValueProvider The value provider that will be evaluated for each source node. The value of the value provider will be used to determine which interpretation to apply. interpretations Yes Dictionary Contains the interpretations that will be applied. The keys represent the values of the <code>switch_on</code> parameter. The values represent the interpretations that will be applied. Each value may also be a list of interpretations. default No Dictionary Contains the default interpretation that will be applied if no interpretation has the same value as the value of the <code>switch_on</code> parameter."},{"location":"docs/reference/interpreter/","title":"Interpreter","text":"<p>The interpreter takes instructions on how to convert a document or record like our JSON data and map it into nodes and relationships  that we want to be in our graph. In order to do this, we provided the interpreter with a set of instructions that tell it how to interpret the data. These instructions are provided in a YAML file that is passed to the interpreter. Each of these intstructions are  referred to as an interpretation which you can view the reference of here</p>"},{"location":"docs/reference/interpreter/#basics","title":"Basics","text":"<p>Imagine records that look like this:</p> <p>JSON<pre><code>{\"name\": \"John\", \"address\": \"123 Test St\"},\n{\"name\": \"Jane\", \"address\": \"456 Test St\"}\n</code></pre> And we want to map the records into nodes and relationships that look like this:</p> <pre><code>graph LR\n  A[John] --&gt;|LIVES_AT| B[123 Test St];</code></pre> <p>then we can use the following YAML definition to instruct the interpreter on how to do this:</p> YAML<pre><code>- implementation: nodestream.interpreting:Interpreter\narguments:\ninterpretations:\n- type: source_node\nname: !jmespath name\nnode_type: AwesomePerson\n- type: relationship\nrelated_node_type: Address\nrelated_field_name: address\nrelationship_type: LIVES_AT\nsearch: !jmespath address\n</code></pre>"},{"location":"docs/reference/interpreter/#using-iterate_on-and-before_iteration","title":"Using <code>iterate_on</code> and <code>before_iteration</code>","text":"<p>Sometimes single records come in a format were multiple ingestions are required for that single record. In cases where the record contains a single list, you can use a combination of the <code>iterate_on</code> and <code>before_iteration</code> keywords. <code>iterate_on</code> is given a <code>ValueProvider</code> for extrcting the subrecords of the source recod. While <code>before_iteration</code> is a separate section of interpretations applied to all objects matched by iterate_on. For example, take the following object:</p> JSON<pre><code>{\n\"team_name\": \"Nodestream\",\n\"people\": [\n{\"name\": \"John\", \"address\": \"123 Test St\"},\n{\"name\": \"Jane\", \"address\": \"456 Test St\"}\n]\n}\n</code></pre> <p>We can use <code>iterate_on</code> to iterate over the <code>people</code> list and <code>before_iteration</code> to set the <code>team_name</code> for each person.</p> YAML<pre><code>- implementation: nodestream.interpreting:Interpreter\narguments:\nbefore_iteration:\n- type: relationship\nrelated_node_type: AwesomeTeam\nrelated_field_name: name\nrelationship_type: PART_OF\nsearch: !jmespath team_name\niterate_on: !jmespath people[*]\ninterpretations:\n- type: source_node\nname: !jmespath name\nnode_type: AwesomePerson'\n- type: relationship\nrelated_node_type: Address\nrelated_field_name: address\nrelationship_type: LIVES_AT\nsearch: !jmespath address\n</code></pre> <p>Note that in the <code>interpretations</code> section we are using <code>!jmespath</code> that are relative to the current object in the iteration. In the <code>before_iteration</code> section we are using <code>!jmespath</code> that are relative to the source object.</p>"},{"location":"docs/reference/interpreter/#performing-multiple-passes","title":"Performing Multiple Passes","text":"<p>One limitation is that the interpreter can only relate nodes one level deep. For example, if we want to relate a person to a team, and a team to an address,  and an address to a city, we can't do that in a single pass. To accomplish it, we can do this by performing multiple passes over the data. In the following example, we will perform two passes. The first pass will relate a person to a team, and the second pass will relate a team to an address.</p> <p>Imaging we have data like this:</p> <p>JSON<pre><code>{\"name\": \"John\", \"address\": \"123 Test St\", \"city\": \"Test City\"},\n{\"name\": \"Jane\", \"address\": \"456 Test St\", \"city\": \"Test City\"}\n</code></pre> We can use the following YAML definition to instruct the interpreter on how to parse this data:</p> YAML<pre><code>- implementation: nodestream.interpreting:Interpreter\narguments:\ninterpretations:\n- - type: source_node\nname: !jmespath name\nnode_type: AwesomePerson\n- type: relationship\nrelated_node_type: Address\nrelated_field_name: address\nrelationship_type: LIVES_AT\nsearch: !jmespath address\n- - type: source_node\nname: !jmespath address\nnode_type: Address\n- type: relationship\nrelated_node_type: City\nrelated_field_name: city\nrelationship_type: IN_CITY\nsearch: !jmespath city\n</code></pre> <p>Mutliple interpretation passes are also allowed for the <code>before_iteration</code> block.  In this case, the <code>iterate_on</code> and <code>iterpretation</code> blocks are applied on the result of each pass pass of the <code>before_iteration</code> block.</p> <p>For example, imagine we have data like this:</p> JSON<pre><code>{\n\"site\": \"github.com\",\n\"other_site\": \"another-git-host.com\",\n\"people\": [\n{\"name\": \"John\", \"address\": \"123 Test St\"},\n{\"name\": \"Jane\", \"address\": \"456 Test St\"}\n]\n}\n</code></pre> <p>We can use the following YAML definition to instruct the interpreter on how to parse this data:</p> YAML<pre><code>- implementation: nodestream.interpreting:Interpreter\narguments:\nbefore_iteration:\n- - type: variables\nsite: !jmespath site\n- - type: variables\nsite: !jmespath other_site\niterate_on: !jmespath people[*]\ninterpretations:\n- type: source_node\nname: !jmespath name\nnode_type: AwesomePerson\n- type: relationship\nnode_type: Website\nrelationship_type: VISITS\nnode_key:\nsite: !variable site\n</code></pre> <p>This would result in the following graph:</p> <pre><code>graph LR\n  A[John] --&gt;|VISITS| B[github.com];\n  C[Jane] --&gt;|VISITS| B[github.com];\n  D[John] --&gt;|VISITS| E[another-git-host.com];\n  F[Jane] --&gt;|VISITS| E[another-git-host.com];</code></pre>"},{"location":"docs/reference/normalizers/","title":"Normalizers","text":"<p>A <code>Normalizer</code> allows you to clean data extracted by a <code>ValueProvider</code>. They are intended to provided stateless, simple transformations of data. Many different interpretations allow you to enable <code>Normalizers</code> to apply these transformations. See the <code>Interpretation</code> reference for where they can be applied.</p>"},{"location":"docs/reference/normalizers/#normalizer-flags","title":"Normalizer Flags","text":"Normalizer Flag Name Example Input Example Output <code>do_lowercase_strings</code> \"dO_LoWER_cASe_strings\" \"do_lowercase_strings\" <code>do_remove_trailing_dots</code> \"my.website.com.\" \"my.website.com\" <code>do_trim_whitespace</code> \"  some value \" \"some value\""},{"location":"docs/reference/transfomers/","title":"Transformers","text":""},{"location":"docs/reference/transfomers/#valueprojection","title":"<code>ValueProjection</code>","text":"<p>Uses a <code>ValueProvider</code> to project value(s) from the input record. For example:</p> YAML<pre><code>- implementation: nodestream.transformers:ValueProjection\narguments:\nprojection: !jmespath \"items[*]\"\n</code></pre> <p>Given this input:</p> JSON<pre><code>{\"items\": [{\"index\": 1}, {\"index\": 2}, {\"index\": 3}]}\n</code></pre> <p>Produces these output records:</p> JSON<pre><code>{\"index\": 1}\n{\"index\": 2}\n{\"index\": 3}\n</code></pre>"},{"location":"docs/reference/value-providers/","title":"Value Providers","text":"<p>There are many methods of extracting and providing data to the ETl pipeline as it operates. The various yaml tags such as <code>!jq</code> or <code>!variable</code> refer to an underlying ValueProvider.</p>"},{"location":"docs/reference/value-providers/#jmespath","title":"<code>!jmespath</code>","text":"<p>Represents a jmespath query language expression that should be executed against the input record.</p> <p>For example, if you want to get extract all of the <code>name</code> fields from the list of people provided in a document like this:</p> JSON<pre><code>{\n\"people\": [{\"name\": \"Joe\", \"age\": 25}, {\"name\": \"john\", \"age\": 45}]\n}\n</code></pre> <p>A valid <code>!jmespath</code> value provider would look like this: <code>!jmespath people[*].name</code> Essentially, any <code>jmespath</code> expression provided after the <code>!jmespath</code> tag will be parsed and loaded as one. Another guide on <code>jmespath</code> can be found here.</p>"},{"location":"docs/reference/value-providers/#jq","title":"<code>!jq</code>","text":"<p>Represents a jq query language expression that should be executed against the input record.</p> <p>For example, if you want to get extract all of the <code>name</code> fields from the list of people provided in a document like this:</p> JSON<pre><code>{\n\"people\": [{\"name\": \"Joe\", \"age\": 25}, {\"name\": \"john\", \"age\": 45}]\n}\n</code></pre> <p>A valid <code>!jq</code> value provider would look like this: <code>!jq .people[].name</code> Essentially, any <code>jq</code> expression provided after the <code>!jq</code> tag will be parsed and loaded as one. More information on <code>jq</code> can be found here.</p>"},{"location":"docs/reference/value-providers/#variable","title":"<code>!variable</code>","text":"<p>Provides the value of an extracted variable from the Variables Interpretation. For instance, if you provided an variables interpretation block like so:</p> YAML<pre><code>interpretations:\n- type: variables\nvariables:\nname: !jmespath person.name\n</code></pre> <p>You are then able to use the <code>!variable</code> provided in a later interpretation. For example,</p> YAML<pre><code>interpretations:\n# other interpretations are omitted.\n- type: source_node\nnode_type: Person\nname: !variable name\n</code></pre> <p>This is particularly helpful when using the <code>before_iteration</code> and <code>iterate_on</code> clause in an <code>Interpreter</code>. For example, assume that you have a record that looks like this:</p> JSON<pre><code>{\n\"team_name\": \"My Awesome Team\",\n\"people\": [\n{\"name\": \"Joe\", \"age\": 25},\n{\"name\": \"John\", \"age\": 34},\n]\n}\n</code></pre> <p>On way to ingest this data would be to do the following:</p> YAML<pre><code>- implementation: nodestream.interpreting:Interpreter\narguments:\nbefore_iteration:\n- type: variables\nvariables:\nteam: !jmespath team\niterate_on: !jmespath people[]\ninterpretations:\n- type: source_node\nnode_type: Person\nkey:\nname: !jmespath name\nproperties:\nage: !jmespath age\n- type: relationship\nnode_type: Team\nrelationship_type: PART_OF\nnode_key:\nname: !variable team\n</code></pre>"},{"location":"docs/reference/value-providers/#format","title":"<code>!format</code>","text":"<p>The <code>!format</code> value provider allows you to format a string using the <code>format</code> method. For example, if you wanted to create a hello world node based on a name field in the record, you could do the following:</p> JSON<pre><code>{\n\"name\": \"Joe\",\n\"age\": 25\n}\n</code></pre> <p>The following interpretation would create a node with the key <code>Hello, Joe!</code>:</p> YAML<pre><code>- implementation: nodestream.interpreting:Interpreter\narguments:\ninterpretations:\n- type: source_node\nnode_type: HelloNode\nkey:\nname: !format fmt: \"Hello, {name}!\"\nname: !jmespath name\nproperties:\nage: !jmespath age\n</code></pre>"},{"location":"docs/reference/value-providers/#regex","title":"<code>!regex</code>","text":"<p>The <code>!regex</code> value provider allows you to extract a value from a string using a regular expression. For example, if you wanted to extract the first name from a string given a record like this:</p> JSON<pre><code>{\n\"name\": \"Joe Smith\",\n\"age\": 25\n}\n</code></pre> <p>The following interpretation would create a node with the key <code>Joe</code>:</p> YAML<pre><code>- implementation: nodestream.interpreting:Interpreter\narguments:\ninterpretations:\n- type: source_node\nnode_type: HelloNode\nkey:\nfirst_name: !regex regex: \"^(?P&lt;first_name&gt;[a-zA-Z]+)\\s(?P&lt;last_name&gt;[a-zA-Z]+)$\"\ndata: !jmespath name\ngroup: first_name\nproperties:\nage: !jmespath age\n</code></pre> <p>You can either use named groups or numbered groups. If you use named groups, you can specify the group name in the <code>group</code> argument.  If you use numbered groups, you can specify the group number in the <code>group</code> argument.  If you do not specify a group, the first group will be used - which is the entire match.</p>"},{"location":"python_reference/file_io/","title":"File io","text":""},{"location":"python_reference/file_io/#nodestream.file_io.DescribesYamlSchema","title":"<code>DescribesYamlSchema</code>","text":"<p>             Bases: <code>ABC</code></p> <p>A mixin for classes that can be described by a YAML schema.</p> Source code in <code>nodestream/file_io.py</code> Python<pre><code>class DescribesYamlSchema(ABC):\n\"\"\"A mixin for classes that can be described by a YAML schema.\"\"\"\n\n    @abstractclassmethod\n    def describe_yaml_schema(self) -&gt; Schema:\n        raise NotImplementedError\n</code></pre>"},{"location":"python_reference/file_io/#nodestream.file_io.LoadsFromYaml","title":"<code>LoadsFromYaml</code>","text":"<p>             Bases: <code>DescribesYamlSchema</code></p> <p>A mixin for classes that can be read from YAML.</p> Source code in <code>nodestream/file_io.py</code> Python<pre><code>class LoadsFromYaml(DescribesYamlSchema):\n\"\"\"A mixin for classes that can be read from YAML.\"\"\"\n\n    @abstractclassmethod\n    def from_file_data(cls, data):\n\"\"\"Create an instance of this class from the given file data.\n\n        Args:\n            data: The data read from the file.\n\n        Returns:\n            An instance of this class.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def validate_and_load(cls, data):\n\"\"\"Validate the given data against the YAML schema and load it into an instance of this class.\n\n        Args:\n            data: The data to validate and load.\n\n        Raises:\n            SchemaError: If the data does not match the YAML schema.\n\n        Returns:\n            An instance of this class.\n        \"\"\"\n        validated_data = cls.describe_yaml_schema().validate(data)\n        return cls.from_file_data(validated_data)\n</code></pre>"},{"location":"python_reference/file_io/#nodestream.file_io.LoadsFromYaml.from_file_data","title":"<code>from_file_data(data)</code>","text":"<p>Create an instance of this class from the given file data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>The data read from the file.</p> required <p>Returns:</p> Type Description <p>An instance of this class.</p> Source code in <code>nodestream/file_io.py</code> Python<pre><code>@abstractclassmethod\ndef from_file_data(cls, data):\n\"\"\"Create an instance of this class from the given file data.\n\n    Args:\n        data: The data read from the file.\n\n    Returns:\n        An instance of this class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python_reference/file_io/#nodestream.file_io.LoadsFromYaml.validate_and_load","title":"<code>validate_and_load(data)</code>  <code>classmethod</code>","text":"<p>Validate the given data against the YAML schema and load it into an instance of this class.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>The data to validate and load.</p> required <p>Raises:</p> Type Description <code>SchemaError</code> <p>If the data does not match the YAML schema.</p> <p>Returns:</p> Type Description <p>An instance of this class.</p> Source code in <code>nodestream/file_io.py</code> Python<pre><code>@classmethod\ndef validate_and_load(cls, data):\n\"\"\"Validate the given data against the YAML schema and load it into an instance of this class.\n\n    Args:\n        data: The data to validate and load.\n\n    Raises:\n        SchemaError: If the data does not match the YAML schema.\n\n    Returns:\n        An instance of this class.\n    \"\"\"\n    validated_data = cls.describe_yaml_schema().validate(data)\n    return cls.from_file_data(validated_data)\n</code></pre>"},{"location":"python_reference/file_io/#nodestream.file_io.LoadsFromYamlFile","title":"<code>LoadsFromYamlFile</code>","text":"<p>             Bases: <code>LoadsFromYaml</code></p> <p>A mixin for classes that can be read from a YAML file.</p> Source code in <code>nodestream/file_io.py</code> Python<pre><code>class LoadsFromYamlFile(LoadsFromYaml):\n\"\"\"A mixin for classes that can be read from a YAML file.\"\"\"\n\n    @classmethod\n    def get_loader(cls) -&gt; Type[SafeLoader]:\n\"\"\"Get the YAML loader to use when reading this object from a file.\n\n        Returns:\n            The YAML loader to use when reading this object from a file.\n        \"\"\"\n        return SafeLoader\n\n    @classmethod\n    def read_from_file(cls, file_path: Path) -&gt; \"LoadsFromYaml\":\n\"\"\"Read this object from a YAML file.\n\n        Args:\n            file_path: The path to the file to read from.\n\n        Raises:\n            FileNotFoundError: If the file does not exist.\n\n        Returns:\n            An instance of this class.\n        \"\"\"\n        if not file_path.exists():\n            raise FileNotFoundError(f\"File '{file_path}' does not exist.\")\n\n        with file_path.open(\"r\") as f:\n            file_data = load(f, Loader=cls.get_loader())\n            return cls.validate_and_load(file_data)\n</code></pre>"},{"location":"python_reference/file_io/#nodestream.file_io.LoadsFromYamlFile.get_loader","title":"<code>get_loader()</code>  <code>classmethod</code>","text":"<p>Get the YAML loader to use when reading this object from a file.</p> <p>Returns:</p> Type Description <code>Type[SafeLoader]</code> <p>The YAML loader to use when reading this object from a file.</p> Source code in <code>nodestream/file_io.py</code> Python<pre><code>@classmethod\ndef get_loader(cls) -&gt; Type[SafeLoader]:\n\"\"\"Get the YAML loader to use when reading this object from a file.\n\n    Returns:\n        The YAML loader to use when reading this object from a file.\n    \"\"\"\n    return SafeLoader\n</code></pre>"},{"location":"python_reference/file_io/#nodestream.file_io.LoadsFromYamlFile.read_from_file","title":"<code>read_from_file(file_path)</code>  <code>classmethod</code>","text":"<p>Read this object from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The path to the file to read from.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> <p>Returns:</p> Type Description <code>LoadsFromYaml</code> <p>An instance of this class.</p> Source code in <code>nodestream/file_io.py</code> Python<pre><code>@classmethod\ndef read_from_file(cls, file_path: Path) -&gt; \"LoadsFromYaml\":\n\"\"\"Read this object from a YAML file.\n\n    Args:\n        file_path: The path to the file to read from.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n\n    Returns:\n        An instance of this class.\n    \"\"\"\n    if not file_path.exists():\n        raise FileNotFoundError(f\"File '{file_path}' does not exist.\")\n\n    with file_path.open(\"r\") as f:\n        file_data = load(f, Loader=cls.get_loader())\n        return cls.validate_and_load(file_data)\n</code></pre>"},{"location":"python_reference/file_io/#nodestream.file_io.SavesToYaml","title":"<code>SavesToYaml</code>","text":"<p>             Bases: <code>DescribesYamlSchema</code></p> <p>A mixin for classes that can be written to as YAML.</p> Source code in <code>nodestream/file_io.py</code> Python<pre><code>class SavesToYaml(DescribesYamlSchema):\n\"\"\"A mixin for classes that can be written to as YAML.\"\"\"\n\n    @abstractmethod\n    def to_file_data(self):\n\"\"\"Get the data to write to a YAML file.\n\n        Returns:\n            The data to write to a YAML file.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"python_reference/file_io/#nodestream.file_io.SavesToYaml.to_file_data","title":"<code>to_file_data()</code>  <code>abstractmethod</code>","text":"<p>Get the data to write to a YAML file.</p> <p>Returns:</p> Type Description <p>The data to write to a YAML file.</p> Source code in <code>nodestream/file_io.py</code> Python<pre><code>@abstractmethod\ndef to_file_data(self):\n\"\"\"Get the data to write to a YAML file.\n\n    Returns:\n        The data to write to a YAML file.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python_reference/file_io/#nodestream.file_io.SavesToYamlFile","title":"<code>SavesToYamlFile</code>","text":"<p>             Bases: <code>SavesToYaml</code></p> <p>A mixin for classes that can be written to a YAML file.</p> Source code in <code>nodestream/file_io.py</code> Python<pre><code>class SavesToYamlFile(SavesToYaml):\n\"\"\"A mixin for classes that can be written to a YAML file.\"\"\"\n\n    @classmethod\n    def get_dumper(cls) -&gt; Type[SafeDumper]:\n\"\"\"Get the YAML dumper to use when writing this object to a file.\n\n        Returns:\n            The YAML dumper to use when writing this object to a file.\n        \"\"\"\n        return SafeDumper\n\n    def write_to_file(self, file_path: Path):\n\"\"\"Write this object to a YAML file.\n\n        If the file already exists, it will be overwritten.\n\n        Args:\n            file_path: The path to the file to write to.\n\n        Returns:\n            None\n        \"\"\"\n        file_data = self.to_file_data()\n        validated_file_data = self.describe_yaml_schema().validate(file_data)\n        with file_path.open(\"w\") as f:\n            dump(\n                validated_file_data,\n                f,\n                Dumper=self.get_dumper(),\n                indent=2,\n                sort_keys=True,\n            )\n</code></pre>"},{"location":"python_reference/file_io/#nodestream.file_io.SavesToYamlFile.get_dumper","title":"<code>get_dumper()</code>  <code>classmethod</code>","text":"<p>Get the YAML dumper to use when writing this object to a file.</p> <p>Returns:</p> Type Description <code>Type[SafeDumper]</code> <p>The YAML dumper to use when writing this object to a file.</p> Source code in <code>nodestream/file_io.py</code> Python<pre><code>@classmethod\ndef get_dumper(cls) -&gt; Type[SafeDumper]:\n\"\"\"Get the YAML dumper to use when writing this object to a file.\n\n    Returns:\n        The YAML dumper to use when writing this object to a file.\n    \"\"\"\n    return SafeDumper\n</code></pre>"},{"location":"python_reference/file_io/#nodestream.file_io.SavesToYamlFile.write_to_file","title":"<code>write_to_file(file_path)</code>","text":"<p>Write this object to a YAML file.</p> <p>If the file already exists, it will be overwritten.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The path to the file to write to.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>nodestream/file_io.py</code> Python<pre><code>def write_to_file(self, file_path: Path):\n\"\"\"Write this object to a YAML file.\n\n    If the file already exists, it will be overwritten.\n\n    Args:\n        file_path: The path to the file to write to.\n\n    Returns:\n        None\n    \"\"\"\n    file_data = self.to_file_data()\n    validated_file_data = self.describe_yaml_schema().validate(file_data)\n    with file_path.open(\"w\") as f:\n        dump(\n            validated_file_data,\n            f,\n            Dumper=self.get_dumper(),\n            indent=2,\n            sort_keys=True,\n        )\n</code></pre>"},{"location":"python_reference/pluggable/","title":"Pluggable","text":""},{"location":"python_reference/subclass_registry/","title":"Subclass registry","text":""},{"location":"python_reference/subclass_registry/#nodestream.subclass_registry.AlreadyInRegistryError","title":"<code>AlreadyInRegistryError</code>","text":"<p>             Bases: <code>ValueError</code></p> <p>Raised when a subclass with the same name is already in the subclass registry.</p> Source code in <code>nodestream/subclass_registry.py</code> Python<pre><code>class AlreadyInRegistryError(ValueError):\n\"\"\"Raised when a subclass with the same name is already in the subclass registry.\"\"\"\n\n    def __init__(self, name, *args: object) -&gt; None:\n        super().__init__(f\"{name} is already registered\", *args)\n</code></pre>"},{"location":"python_reference/subclass_registry/#nodestream.subclass_registry.MissingFromRegistryError","title":"<code>MissingFromRegistryError</code>","text":"<p>             Bases: <code>ValueError</code></p> <p>Raised when a subclass is not in the subclass registry.</p> Source code in <code>nodestream/subclass_registry.py</code> Python<pre><code>class MissingFromRegistryError(ValueError):\n\"\"\"Raised when a subclass is not in the subclass registry.\"\"\"\n\n    def __init__(self, name, *args: object) -&gt; None:\n        super().__init__(f\"{name} is not in the subclass registry\", *args)\n</code></pre>"},{"location":"python_reference/subclass_registry/#nodestream.subclass_registry.SubclassRegistry","title":"<code>SubclassRegistry</code>","text":"<p>A registry for subclasses of a base class.</p> Source code in <code>nodestream/subclass_registry.py</code> Python<pre><code>class SubclassRegistry:\n\"\"\"A registry for subclasses of a base class.\"\"\"\n\n    __slots__ = (\"registry\", \"linked_base\")\n\n    def __init__(self) -&gt; None:\n        self.registry = {}\n        self.linked_base = None\n\n    def connect_baseclass(self, base_class):\n\"\"\"Connect a base class to this registry.\"\"\"\n\n        old_init_subclass = base_class.__init_subclass__\n        base_class.__subclass__registry__ = self\n        self.linked_base = base_class\n\n        @classmethod\n        @wraps(old_init_subclass)\n        def init_subclass(cls, alias=None, *args, **kwargs):\n            alias = alias or cls.__name__\n\n            if alias in self.registry:\n                raise AlreadyInRegistryError(alias)\n\n            self.registry[alias] = cls\n            return old_init_subclass(*args, **kwargs)\n\n        base_class.__init_subclass__ = init_subclass\n        return base_class\n\n    def name_for(self, cls):\n\"\"\"Get the name of a class in the registry.\"\"\"\n\n        for k, v in self.registry.items():\n            if v is cls:\n                return k\n\n        return None\n\n    def get(self, name):\n\"\"\"Get a subclass by name.\"\"\"\n        try:\n            return self.registry[name]\n        except KeyError:\n            raise MissingFromRegistryError(name)\n\n    @property\n    def all_subclasses(self):\n        return self.registry.values()\n</code></pre>"},{"location":"python_reference/subclass_registry/#nodestream.subclass_registry.SubclassRegistry.connect_baseclass","title":"<code>connect_baseclass(base_class)</code>","text":"<p>Connect a base class to this registry.</p> Source code in <code>nodestream/subclass_registry.py</code> Python<pre><code>def connect_baseclass(self, base_class):\n\"\"\"Connect a base class to this registry.\"\"\"\n\n    old_init_subclass = base_class.__init_subclass__\n    base_class.__subclass__registry__ = self\n    self.linked_base = base_class\n\n    @classmethod\n    @wraps(old_init_subclass)\n    def init_subclass(cls, alias=None, *args, **kwargs):\n        alias = alias or cls.__name__\n\n        if alias in self.registry:\n            raise AlreadyInRegistryError(alias)\n\n        self.registry[alias] = cls\n        return old_init_subclass(*args, **kwargs)\n\n    base_class.__init_subclass__ = init_subclass\n    return base_class\n</code></pre>"},{"location":"python_reference/subclass_registry/#nodestream.subclass_registry.SubclassRegistry.get","title":"<code>get(name)</code>","text":"<p>Get a subclass by name.</p> Source code in <code>nodestream/subclass_registry.py</code> Python<pre><code>def get(self, name):\n\"\"\"Get a subclass by name.\"\"\"\n    try:\n        return self.registry[name]\n    except KeyError:\n        raise MissingFromRegistryError(name)\n</code></pre>"},{"location":"python_reference/subclass_registry/#nodestream.subclass_registry.SubclassRegistry.name_for","title":"<code>name_for(cls)</code>","text":"<p>Get the name of a class in the registry.</p> Source code in <code>nodestream/subclass_registry.py</code> Python<pre><code>def name_for(self, cls):\n\"\"\"Get the name of a class in the registry.\"\"\"\n\n    for k, v in self.registry.items():\n        if v is cls:\n            return k\n\n    return None\n</code></pre>"},{"location":"python_reference/cli/application/","title":"Application","text":""},{"location":"python_reference/cli/commands/audit_command/","title":"Audit command","text":""},{"location":"python_reference/cli/commands/new/","title":"New","text":""},{"location":"python_reference/cli/commands/nodestream_command/","title":"Nodestream command","text":""},{"location":"python_reference/cli/commands/print_schema/","title":"Print schema","text":""},{"location":"python_reference/cli/commands/remove/","title":"Remove","text":""},{"location":"python_reference/cli/commands/run/","title":"Run","text":""},{"location":"python_reference/cli/commands/scaffold/","title":"Scaffold","text":""},{"location":"python_reference/cli/commands/shared_options/","title":"Shared options","text":""},{"location":"python_reference/cli/commands/show/","title":"Show","text":""},{"location":"python_reference/cli/operations/add_pipeline_to_project/","title":"Add pipeline to project","text":""},{"location":"python_reference/cli/operations/commit_project_to_disk/","title":"Commit project to disk","text":""},{"location":"python_reference/cli/operations/generate_pipeline_scaffold/","title":"Generate pipeline scaffold","text":""},{"location":"python_reference/cli/operations/initialize_logger/","title":"Initialize logger","text":""},{"location":"python_reference/cli/operations/initialize_project/","title":"Initialize project","text":""},{"location":"python_reference/cli/operations/operation/","title":"Operation","text":""},{"location":"python_reference/cli/operations/print_project_schema/","title":"Print project schema","text":""},{"location":"python_reference/cli/operations/remove_pipeline_from_project/","title":"Remove pipeline from project","text":""},{"location":"python_reference/cli/operations/run_audit/","title":"Run audit","text":""},{"location":"python_reference/cli/operations/run_pipeline/","title":"Run pipeline","text":""},{"location":"python_reference/cli/operations/run_project_cookiecutter/","title":"Run project cookiecutter","text":""},{"location":"python_reference/cli/operations/show_pipelines/","title":"Show pipelines","text":""},{"location":"python_reference/databases/database_connector/","title":"Database connector","text":""},{"location":"python_reference/databases/debounced_ingest_strategy/","title":"Debounced ingest strategy","text":""},{"location":"python_reference/databases/ingest_strategy/","title":"Ingest strategy","text":""},{"location":"python_reference/databases/ingest_strategy/#nodestream.databases.ingest_strategy.IngestionStrategy","title":"<code>IngestionStrategy</code>","text":"<p>             Bases: <code>ABC</code></p> <p>An IngestionStrategy represents the methods taken to commit data to a Graph Database.</p> <p>Within nodestream, many components gather and intend to commit changes to a Graph Database. How that data is committed and how it is stored internally is decoupled through the <code>IngestionStrategy</code> interface.</p> <p>Generally, your usage of nodestream is decoupled from <code>IngestionStrategy</code> unless you intend to provide an implementation of your own database writer. The writer API will give you a <code>DesiredIngestion</code> or other <code>Ingestible</code> object that needs a instance of an <code>IngestionStrategy</code> to apply operations to.</p> Source code in <code>nodestream/databases/ingest_strategy.py</code> Python<pre><code>@INGESTION_STRATEGY_REGISTRY.connect_baseclass\nclass IngestionStrategy(ABC):\n\"\"\"An IngestionStrategy represents the methods taken to commit data to a Graph Database.\n\n    Within nodestream, many components gather and intend to commit changes to a Graph Database.\n    How that data is committed and how it is stored internally is decoupled through the `IngestionStrategy` interface.\n\n    Generally, your usage of nodestream is decoupled from `IngestionStrategy` unless you intend to provide an implementation\n    of your own database writer. The writer API will give you a `DesiredIngestion` or other `Ingestible` object that needs a\n    instance of an `IngestionStrategy` to apply operations to.\n    \"\"\"\n\n    @abstractmethod\n    async def ingest_source_node(self, source: \"Node\"):\n\"\"\"Given a provided instance of `Node`, ensure that it is committed to the GraphDatabase.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def ingest_relationship(self, relationship: \"RelationshipWithNodes\"):\n\"\"\"Given a provided instance of `SourceNode`, ensure that the provided `Relationship` is committed to the database.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def run_hook(self, request: \"IngestionHookRunRequest\"):\n\"\"\"Runs the provided request for an IngestHook given the context.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def upsert_key_index(self, index: \"KeyIndex\"):\n\"\"\"Create a Key Index Immediately for a given Node Type.\n\n        See information on `KeyIndex` for more information.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def upsert_field_index(self, index: \"FieldIndex\"):\n\"\"\"Create a Key Index Immediately for a Object Type.\n\n        See information on `FieldIndex` for more information.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def perform_ttl_operation(self, config: \"TimeToLiveConfiguration\"):\n\"\"\"Perform a TTL Operation.\n\n        See Information on `TimeToLiveConfiguration` for more Information.\n        \"\"\"\n        raise NotImplementedError\n\n    async def flush(self):\n\"\"\"Flush any pending operations to the database.\"\"\"\n        pass\n</code></pre>"},{"location":"python_reference/databases/ingest_strategy/#nodestream.databases.ingest_strategy.IngestionStrategy.flush","title":"<code>flush()</code>  <code>async</code>","text":"<p>Flush any pending operations to the database.</p> Source code in <code>nodestream/databases/ingest_strategy.py</code> Python<pre><code>async def flush(self):\n\"\"\"Flush any pending operations to the database.\"\"\"\n    pass\n</code></pre>"},{"location":"python_reference/databases/ingest_strategy/#nodestream.databases.ingest_strategy.IngestionStrategy.ingest_relationship","title":"<code>ingest_relationship(relationship)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Given a provided instance of <code>SourceNode</code>, ensure that the provided <code>Relationship</code> is committed to the database.</p> Source code in <code>nodestream/databases/ingest_strategy.py</code> Python<pre><code>@abstractmethod\nasync def ingest_relationship(self, relationship: \"RelationshipWithNodes\"):\n\"\"\"Given a provided instance of `SourceNode`, ensure that the provided `Relationship` is committed to the database.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python_reference/databases/ingest_strategy/#nodestream.databases.ingest_strategy.IngestionStrategy.ingest_source_node","title":"<code>ingest_source_node(source)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Given a provided instance of <code>Node</code>, ensure that it is committed to the GraphDatabase.</p> Source code in <code>nodestream/databases/ingest_strategy.py</code> Python<pre><code>@abstractmethod\nasync def ingest_source_node(self, source: \"Node\"):\n\"\"\"Given a provided instance of `Node`, ensure that it is committed to the GraphDatabase.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python_reference/databases/ingest_strategy/#nodestream.databases.ingest_strategy.IngestionStrategy.perform_ttl_operation","title":"<code>perform_ttl_operation(config)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Perform a TTL Operation.</p> <p>See Information on <code>TimeToLiveConfiguration</code> for more Information.</p> Source code in <code>nodestream/databases/ingest_strategy.py</code> Python<pre><code>@abstractmethod\nasync def perform_ttl_operation(self, config: \"TimeToLiveConfiguration\"):\n\"\"\"Perform a TTL Operation.\n\n    See Information on `TimeToLiveConfiguration` for more Information.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python_reference/databases/ingest_strategy/#nodestream.databases.ingest_strategy.IngestionStrategy.run_hook","title":"<code>run_hook(request)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Runs the provided request for an IngestHook given the context.</p> Source code in <code>nodestream/databases/ingest_strategy.py</code> Python<pre><code>@abstractmethod\nasync def run_hook(self, request: \"IngestionHookRunRequest\"):\n\"\"\"Runs the provided request for an IngestHook given the context.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python_reference/databases/ingest_strategy/#nodestream.databases.ingest_strategy.IngestionStrategy.upsert_field_index","title":"<code>upsert_field_index(index)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Create a Key Index Immediately for a Object Type.</p> <p>See information on <code>FieldIndex</code> for more information.</p> Source code in <code>nodestream/databases/ingest_strategy.py</code> Python<pre><code>@abstractmethod\nasync def upsert_field_index(self, index: \"FieldIndex\"):\n\"\"\"Create a Key Index Immediately for a Object Type.\n\n    See information on `FieldIndex` for more information.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python_reference/databases/ingest_strategy/#nodestream.databases.ingest_strategy.IngestionStrategy.upsert_key_index","title":"<code>upsert_key_index(index)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Create a Key Index Immediately for a given Node Type.</p> <p>See information on <code>KeyIndex</code> for more information.</p> Source code in <code>nodestream/databases/ingest_strategy.py</code> Python<pre><code>@abstractmethod\nasync def upsert_key_index(self, index: \"KeyIndex\"):\n\"\"\"Create a Key Index Immediately for a given Node Type.\n\n    See information on `KeyIndex` for more information.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python_reference/databases/operation_debouncer/","title":"Operation debouncer","text":""},{"location":"python_reference/databases/query_executor/","title":"Query executor","text":""},{"location":"python_reference/databases/query_executor_with_statistics/","title":"Query executor with statistics","text":""},{"location":"python_reference/databases/writer/","title":"Writer","text":""},{"location":"python_reference/databases/neo4j/database_connector/","title":"Database connector","text":""},{"location":"python_reference/databases/neo4j/index_query_builder/","title":"Index query builder","text":""},{"location":"python_reference/databases/neo4j/index_query_builder/#nodestream.databases.neo4j.index_query_builder.Neo4jEnterpriseIndexQueryBuilder","title":"<code>Neo4jEnterpriseIndexQueryBuilder</code>","text":"<p>             Bases: <code>Neo4jIndexQueryBuilder</code></p> <p>Creates index creation query that will work only for Neo4j Enterprise</p> Source code in <code>nodestream/databases/neo4j/index_query_builder.py</code> Python<pre><code>class Neo4jEnterpriseIndexQueryBuilder(Neo4jIndexQueryBuilder):\n\"\"\"Creates index creation query that will work only for Neo4j Enterprise\"\"\"\n\n    def create_key_index_query(self, key_index: KeyIndex) -&gt; Query:\n\"\"\"Generates a key index using a `Node key constraint` which is an enterprise feature.\n\n        see: https://neo4j.com/docs/cypher-manual/current/constraints/\n        \"\"\"\n        return key_index_from_format(key_index, ENTERPRISE_KEY_INDEX_QUERY_FORMAT)\n</code></pre>"},{"location":"python_reference/databases/neo4j/index_query_builder/#nodestream.databases.neo4j.index_query_builder.Neo4jEnterpriseIndexQueryBuilder.create_key_index_query","title":"<code>create_key_index_query(key_index)</code>","text":"<p>Generates a key index using a <code>Node key constraint</code> which is an enterprise feature.</p> <p>see: https://neo4j.com/docs/cypher-manual/current/constraints/</p> Source code in <code>nodestream/databases/neo4j/index_query_builder.py</code> Python<pre><code>def create_key_index_query(self, key_index: KeyIndex) -&gt; Query:\n\"\"\"Generates a key index using a `Node key constraint` which is an enterprise feature.\n\n    see: https://neo4j.com/docs/cypher-manual/current/constraints/\n    \"\"\"\n    return key_index_from_format(key_index, ENTERPRISE_KEY_INDEX_QUERY_FORMAT)\n</code></pre>"},{"location":"python_reference/databases/neo4j/index_query_builder/#nodestream.databases.neo4j.index_query_builder.Neo4jIndexQueryBuilder","title":"<code>Neo4jIndexQueryBuilder</code>","text":"<p>Creates index creation queries that will work for any Neo4j Database Supported.</p> Source code in <code>nodestream/databases/neo4j/index_query_builder.py</code> Python<pre><code>class Neo4jIndexQueryBuilder:\n\"\"\"Creates index creation queries that will work for any Neo4j Database Supported.\"\"\"\n\n    def create_key_index_query(self, key_index: KeyIndex) -&gt; Query:\n\"\"\"Creates a key index using a \"Unique node property constraint\" constraint which is supported by community.\n\n        see: https://neo4j.com/docs/cypher-manual/current/constraints/\n        \"\"\"\n        return key_index_from_format(key_index, KEY_INDEX_QUERY_FORMAT)\n\n    def create_field_index_query(self, field_index: FieldIndex) -&gt; Query:\n\"\"\"Creates a filed index using a 'Range Index' or equivalent.\n\n        see: https://neo4j.com/docs/cypher-manual/current/indexes-for-search-performance/#indexes-create-indexes\n        \"\"\"\n        constraint_name = f\"{field_index.type}_{field_index.field}_additional_index\"\n        format = (\n            REL_FIELD_INDEX_QUERY_FORMAT\n            if field_index.object_type == GraphObjectType.RELATIONSHIP\n            else NODE_FIELD_INDEX_QUERY_FORMAT\n        )\n        statement = format.format(\n            constraint_name=constraint_name,\n            type=field_index.type,\n            field=field_index.field,\n        )\n        return Query.from_statement(statement)\n</code></pre>"},{"location":"python_reference/databases/neo4j/index_query_builder/#nodestream.databases.neo4j.index_query_builder.Neo4jIndexQueryBuilder.create_field_index_query","title":"<code>create_field_index_query(field_index)</code>","text":"<p>Creates a filed index using a 'Range Index' or equivalent.</p> <p>see: https://neo4j.com/docs/cypher-manual/current/indexes-for-search-performance/#indexes-create-indexes</p> Source code in <code>nodestream/databases/neo4j/index_query_builder.py</code> Python<pre><code>def create_field_index_query(self, field_index: FieldIndex) -&gt; Query:\n\"\"\"Creates a filed index using a 'Range Index' or equivalent.\n\n    see: https://neo4j.com/docs/cypher-manual/current/indexes-for-search-performance/#indexes-create-indexes\n    \"\"\"\n    constraint_name = f\"{field_index.type}_{field_index.field}_additional_index\"\n    format = (\n        REL_FIELD_INDEX_QUERY_FORMAT\n        if field_index.object_type == GraphObjectType.RELATIONSHIP\n        else NODE_FIELD_INDEX_QUERY_FORMAT\n    )\n    statement = format.format(\n        constraint_name=constraint_name,\n        type=field_index.type,\n        field=field_index.field,\n    )\n    return Query.from_statement(statement)\n</code></pre>"},{"location":"python_reference/databases/neo4j/index_query_builder/#nodestream.databases.neo4j.index_query_builder.Neo4jIndexQueryBuilder.create_key_index_query","title":"<code>create_key_index_query(key_index)</code>","text":"<p>Creates a key index using a \"Unique node property constraint\" constraint which is supported by community.</p> <p>see: https://neo4j.com/docs/cypher-manual/current/constraints/</p> Source code in <code>nodestream/databases/neo4j/index_query_builder.py</code> Python<pre><code>def create_key_index_query(self, key_index: KeyIndex) -&gt; Query:\n\"\"\"Creates a key index using a \"Unique node property constraint\" constraint which is supported by community.\n\n    see: https://neo4j.com/docs/cypher-manual/current/constraints/\n    \"\"\"\n    return key_index_from_format(key_index, KEY_INDEX_QUERY_FORMAT)\n</code></pre>"},{"location":"python_reference/databases/neo4j/ingest_query_builder/","title":"Ingest query builder","text":""},{"location":"python_reference/databases/neo4j/ingest_query_builder/#nodestream.databases.neo4j.ingest_query_builder.Neo4jIngestQueryBuilder","title":"<code>Neo4jIngestQueryBuilder</code>","text":"Source code in <code>nodestream/databases/neo4j/ingest_query_builder.py</code> Python<pre><code>class Neo4jIngestQueryBuilder:\n    @cache\n    @correct_parameters\n    def generate_update_node_operation_query_statement(\n        self,\n        operation: OperationOnNodeIdentity,\n    ) -&gt; str:\n\"\"\"Generate a query to update a node in the database given a node type and a match strategy.\"\"\"\n\n        if operation.match_strategy == MatchStrategy.EAGER:\n            query = str(_merge_node(operation))\n        else:\n            query = str(_match_node(operation))\n\n        result = f\"{query} SET {GENERIC_NODE_REF_NAME} += params.{generate_prefixed_param_name(PROPERTIES_PARAM_NAME, GENERIC_NODE_REF_NAME)}\"\n        if operation.node_identity.additional_types:\n            result += f\" WITH {GENERIC_NODE_REF_NAME}, params CALL apoc.create.addLabels({GENERIC_NODE_REF_NAME}, params.{generate_prefixed_param_name(ADDITIONAL_LABELS_PARAM_NAME, GENERIC_NODE_REF_NAME)}) yield node RETURN true\"\n        return result\n\n    def generate_update_node_operation_params(self, node: Node) -&gt; dict:\n\"\"\"Generate the parameters for a query to update a node in the database.\"\"\"\n\n        params = self.generate_node_key_params(node)\n        params[\n            generate_prefixed_param_name(PROPERTIES_PARAM_NAME, GENERIC_NODE_REF_NAME)\n        ] = node.properties\n        params[\n            generate_prefixed_param_name(\n                ADDITIONAL_LABELS_PARAM_NAME, GENERIC_NODE_REF_NAME\n            )\n        ] = node.additional_types\n\n        return params\n\n    def generate_node_key_params(self, node: Node, name=GENERIC_NODE_REF_NAME) -&gt; dict:\n\"\"\"Generate the parameters for a query to update a node in the database.\"\"\"\n\n        return {\n            generate_prefixed_param_name(k, name): v for k, v in node.key_values.items()\n        }\n\n    @cache\n    @correct_parameters\n    def generate_update_relationship_operation_query_statement(\n        self,\n        operation: OperationOnRelationshipIdentity,\n    ) -&gt; str:\n\"\"\"Generate a query to update a relationship in the database given a relationship operation.\"\"\"\n\n        match_from_node_segment = _match_node(operation.from_node, FROM_NODE_REF_NAME)\n        match_to_node_segment = _match_node(operation.to_node, TO_NODE_REF_NAME)\n        merge_rel_segment = _merge_relationship(operation.relationship_identity)\n        return f\"{match_from_node_segment} {match_to_node_segment} {merge_rel_segment}\"\n\n    def generate_update_rel_params(self, rel: Relationship) -&gt; dict:\n\"\"\"Generate the parameters for a query to update a relationship in the database.\"\"\"\n\n        params = {\n            generate_prefixed_param_name(k, RELATIONSHIP_REF_NAME): v\n            for k, v in rel.key_values.items()\n        }\n        params[\n            generate_prefixed_param_name(PROPERTIES_PARAM_NAME, RELATIONSHIP_REF_NAME)\n        ] = rel.properties\n\n        return params\n\n    def generate_update_rel_between_nodes_params(\n        self, rel: RelationshipWithNodes\n    ) -&gt; dict:\n\"\"\"Generate the parameters for a query to update a relationship in the database.\"\"\"\n\n        params = self.generate_update_rel_params(rel.relationship)\n        params.update(self.generate_node_key_params(rel.from_node, FROM_NODE_REF_NAME))\n        params.update(self.generate_node_key_params(rel.to_node, TO_NODE_REF_NAME))\n        return params\n\n    def generate_batch_update_node_operation_batch(\n        self,\n        operation: OperationOnNodeIdentity,\n        nodes: Iterable[Node],\n    ) -&gt; QueryBatch:\n\"\"\"Generate a batch of queries to update nodes in the database in the same way of the same type.\"\"\"\n\n        query_statement = self.generate_update_node_operation_query_statement(operation)\n        params = [self.generate_update_node_operation_params(node) for node in nodes]\n        return QueryBatch(query_statement, params)\n\n    def generate_batch_update_relationship_query_batch(\n        self,\n        operation: OperationOnRelationshipIdentity,\n        relationships: Iterable[RelationshipWithNodes],\n    ) -&gt; QueryBatch:\n\"\"\"Generate a batch of queries to update relationships in the database in the same way of the same type.\"\"\"\n\n        query = self.generate_update_relationship_operation_query_statement(operation)\n        params = [\n            self.generate_update_rel_between_nodes_params(rel) for rel in relationships\n        ]\n        return QueryBatch(query, params)\n\n    def generate_ttl_query_from_configuration(\n        self, config: TimeToLiveConfiguration\n    ) -&gt; Query:\n        earliest_allowed_time = datetime.utcnow() - timedelta(\n            hours=config.expiry_in_hours\n        )\n        params = {\"earliest_allowed_time\": earliest_allowed_time}\n        if config.custom_query is not None:\n            return Query(config.custom_query, params)\n\n        query_builder = QueryBuilder()\n        ref_name = \"x\"\n\n        if config.graph_object_type == GraphObjectType.NODE:\n            query_builder = query_builder.match().node(\n                labels=config.object_type, ref_name=ref_name\n            )\n        else:\n            query_builder = (\n                query_builder.match()\n                .node()\n                .related_to(label=config.object_type, ref_name=ref_name)\n                .node()\n            )\n\n        query_builder = query_builder.where_literal(\n            f\"{ref_name}.last_ingested_at &lt;= $earliest_allowed_time\"\n        ).return_literal(f\"id({ref_name}) as id\")\n\n        return Query(str(query_builder), params)\n</code></pre>"},{"location":"python_reference/databases/neo4j/ingest_query_builder/#nodestream.databases.neo4j.ingest_query_builder.Neo4jIngestQueryBuilder.generate_batch_update_node_operation_batch","title":"<code>generate_batch_update_node_operation_batch(operation, nodes)</code>","text":"<p>Generate a batch of queries to update nodes in the database in the same way of the same type.</p> Source code in <code>nodestream/databases/neo4j/ingest_query_builder.py</code> Python<pre><code>def generate_batch_update_node_operation_batch(\n    self,\n    operation: OperationOnNodeIdentity,\n    nodes: Iterable[Node],\n) -&gt; QueryBatch:\n\"\"\"Generate a batch of queries to update nodes in the database in the same way of the same type.\"\"\"\n\n    query_statement = self.generate_update_node_operation_query_statement(operation)\n    params = [self.generate_update_node_operation_params(node) for node in nodes]\n    return QueryBatch(query_statement, params)\n</code></pre>"},{"location":"python_reference/databases/neo4j/ingest_query_builder/#nodestream.databases.neo4j.ingest_query_builder.Neo4jIngestQueryBuilder.generate_batch_update_relationship_query_batch","title":"<code>generate_batch_update_relationship_query_batch(operation, relationships)</code>","text":"<p>Generate a batch of queries to update relationships in the database in the same way of the same type.</p> Source code in <code>nodestream/databases/neo4j/ingest_query_builder.py</code> Python<pre><code>def generate_batch_update_relationship_query_batch(\n    self,\n    operation: OperationOnRelationshipIdentity,\n    relationships: Iterable[RelationshipWithNodes],\n) -&gt; QueryBatch:\n\"\"\"Generate a batch of queries to update relationships in the database in the same way of the same type.\"\"\"\n\n    query = self.generate_update_relationship_operation_query_statement(operation)\n    params = [\n        self.generate_update_rel_between_nodes_params(rel) for rel in relationships\n    ]\n    return QueryBatch(query, params)\n</code></pre>"},{"location":"python_reference/databases/neo4j/ingest_query_builder/#nodestream.databases.neo4j.ingest_query_builder.Neo4jIngestQueryBuilder.generate_node_key_params","title":"<code>generate_node_key_params(node, name=GENERIC_NODE_REF_NAME)</code>","text":"<p>Generate the parameters for a query to update a node in the database.</p> Source code in <code>nodestream/databases/neo4j/ingest_query_builder.py</code> Python<pre><code>def generate_node_key_params(self, node: Node, name=GENERIC_NODE_REF_NAME) -&gt; dict:\n\"\"\"Generate the parameters for a query to update a node in the database.\"\"\"\n\n    return {\n        generate_prefixed_param_name(k, name): v for k, v in node.key_values.items()\n    }\n</code></pre>"},{"location":"python_reference/databases/neo4j/ingest_query_builder/#nodestream.databases.neo4j.ingest_query_builder.Neo4jIngestQueryBuilder.generate_update_node_operation_params","title":"<code>generate_update_node_operation_params(node)</code>","text":"<p>Generate the parameters for a query to update a node in the database.</p> Source code in <code>nodestream/databases/neo4j/ingest_query_builder.py</code> Python<pre><code>def generate_update_node_operation_params(self, node: Node) -&gt; dict:\n\"\"\"Generate the parameters for a query to update a node in the database.\"\"\"\n\n    params = self.generate_node_key_params(node)\n    params[\n        generate_prefixed_param_name(PROPERTIES_PARAM_NAME, GENERIC_NODE_REF_NAME)\n    ] = node.properties\n    params[\n        generate_prefixed_param_name(\n            ADDITIONAL_LABELS_PARAM_NAME, GENERIC_NODE_REF_NAME\n        )\n    ] = node.additional_types\n\n    return params\n</code></pre>"},{"location":"python_reference/databases/neo4j/ingest_query_builder/#nodestream.databases.neo4j.ingest_query_builder.Neo4jIngestQueryBuilder.generate_update_node_operation_query_statement","title":"<code>generate_update_node_operation_query_statement(operation)</code>  <code>cached</code>","text":"<p>Generate a query to update a node in the database given a node type and a match strategy.</p> Source code in <code>nodestream/databases/neo4j/ingest_query_builder.py</code> Python<pre><code>@cache\n@correct_parameters\ndef generate_update_node_operation_query_statement(\n    self,\n    operation: OperationOnNodeIdentity,\n) -&gt; str:\n\"\"\"Generate a query to update a node in the database given a node type and a match strategy.\"\"\"\n\n    if operation.match_strategy == MatchStrategy.EAGER:\n        query = str(_merge_node(operation))\n    else:\n        query = str(_match_node(operation))\n\n    result = f\"{query} SET {GENERIC_NODE_REF_NAME} += params.{generate_prefixed_param_name(PROPERTIES_PARAM_NAME, GENERIC_NODE_REF_NAME)}\"\n    if operation.node_identity.additional_types:\n        result += f\" WITH {GENERIC_NODE_REF_NAME}, params CALL apoc.create.addLabels({GENERIC_NODE_REF_NAME}, params.{generate_prefixed_param_name(ADDITIONAL_LABELS_PARAM_NAME, GENERIC_NODE_REF_NAME)}) yield node RETURN true\"\n    return result\n</code></pre>"},{"location":"python_reference/databases/neo4j/ingest_query_builder/#nodestream.databases.neo4j.ingest_query_builder.Neo4jIngestQueryBuilder.generate_update_rel_between_nodes_params","title":"<code>generate_update_rel_between_nodes_params(rel)</code>","text":"<p>Generate the parameters for a query to update a relationship in the database.</p> Source code in <code>nodestream/databases/neo4j/ingest_query_builder.py</code> Python<pre><code>def generate_update_rel_between_nodes_params(\n    self, rel: RelationshipWithNodes\n) -&gt; dict:\n\"\"\"Generate the parameters for a query to update a relationship in the database.\"\"\"\n\n    params = self.generate_update_rel_params(rel.relationship)\n    params.update(self.generate_node_key_params(rel.from_node, FROM_NODE_REF_NAME))\n    params.update(self.generate_node_key_params(rel.to_node, TO_NODE_REF_NAME))\n    return params\n</code></pre>"},{"location":"python_reference/databases/neo4j/ingest_query_builder/#nodestream.databases.neo4j.ingest_query_builder.Neo4jIngestQueryBuilder.generate_update_rel_params","title":"<code>generate_update_rel_params(rel)</code>","text":"<p>Generate the parameters for a query to update a relationship in the database.</p> Source code in <code>nodestream/databases/neo4j/ingest_query_builder.py</code> Python<pre><code>def generate_update_rel_params(self, rel: Relationship) -&gt; dict:\n\"\"\"Generate the parameters for a query to update a relationship in the database.\"\"\"\n\n    params = {\n        generate_prefixed_param_name(k, RELATIONSHIP_REF_NAME): v\n        for k, v in rel.key_values.items()\n    }\n    params[\n        generate_prefixed_param_name(PROPERTIES_PARAM_NAME, RELATIONSHIP_REF_NAME)\n    ] = rel.properties\n\n    return params\n</code></pre>"},{"location":"python_reference/databases/neo4j/ingest_query_builder/#nodestream.databases.neo4j.ingest_query_builder.Neo4jIngestQueryBuilder.generate_update_relationship_operation_query_statement","title":"<code>generate_update_relationship_operation_query_statement(operation)</code>  <code>cached</code>","text":"<p>Generate a query to update a relationship in the database given a relationship operation.</p> Source code in <code>nodestream/databases/neo4j/ingest_query_builder.py</code> Python<pre><code>@cache\n@correct_parameters\ndef generate_update_relationship_operation_query_statement(\n    self,\n    operation: OperationOnRelationshipIdentity,\n) -&gt; str:\n\"\"\"Generate a query to update a relationship in the database given a relationship operation.\"\"\"\n\n    match_from_node_segment = _match_node(operation.from_node, FROM_NODE_REF_NAME)\n    match_to_node_segment = _match_node(operation.to_node, TO_NODE_REF_NAME)\n    merge_rel_segment = _merge_relationship(operation.relationship_identity)\n    return f\"{match_from_node_segment} {match_to_node_segment} {merge_rel_segment}\"\n</code></pre>"},{"location":"python_reference/databases/neo4j/query/","title":"Query","text":""},{"location":"python_reference/databases/neo4j/query_executor/","title":"Query executor","text":""},{"location":"python_reference/interpreting/interpreter/","title":"Interpreter","text":""},{"location":"python_reference/interpreting/record_decomposers/","title":"Record decomposers","text":""},{"location":"python_reference/interpreting/record_decomposers/#nodestream.interpreting.record_decomposers.RecordDecomposer","title":"<code>RecordDecomposer</code>","text":"<p>             Bases: <code>ABC</code></p> <p>A RecordDecomposer is responsible for looking at a record to decomposing it to sub-records to look at</p> Source code in <code>nodestream/interpreting/record_decomposers.py</code> Python<pre><code>class RecordDecomposer(ABC):\n\"\"\"A RecordDecomposer is responsible for looking at a record to decomposing it to sub-records to look at\"\"\"\n\n    @abstractmethod\n    def decompose_record(self, context: ProviderContext) -&gt; Iterable[ProviderContext]:\n        raise NotImplementedError\n\n    @classmethod\n    def from_iteration_arguments(cls, iteration_arguments: Optional[ValueProvider]):\n        if iteration_arguments:\n            return ValueProviderDecomposer(iteration_arguments)\n\n        return WholeRecordDecomposer()\n</code></pre>"},{"location":"python_reference/interpreting/record_decomposers/#nodestream.interpreting.record_decomposers.ValueProviderDecomposer","title":"<code>ValueProviderDecomposer</code>","text":"<p>             Bases: <code>RecordDecomposer</code></p> <p>Iterates on the values provided from a value provider from the root context.</p> <p><code>decompose_record</code> will take the supplied <code>ProviderContext</code> and deep copy it for each provided value from the value provider when called on the supplied context.</p> Source code in <code>nodestream/interpreting/record_decomposers.py</code> Python<pre><code>class ValueProviderDecomposer(RecordDecomposer):\n\"\"\"Iterates on the values provided from a value provider from the root context.\n\n    `decompose_record` will take the supplied `ProviderContext` and deep copy it for each\n    provided value from the value provider when called on the supplied context.\n    \"\"\"\n\n    def __init__(self, value_provider: ValueProvider) -&gt; None:\n        self.value_provider = value_provider\n\n    def decompose_record(self, context: ProviderContext) -&gt; Iterable[ProviderContext]:\n        for sub_document in self.value_provider.many_values(context):\n            sub_context = deepcopy(context)\n            sub_context.document = sub_document\n            yield sub_context\n</code></pre>"},{"location":"python_reference/interpreting/record_decomposers/#nodestream.interpreting.record_decomposers.WholeRecordDecomposer","title":"<code>WholeRecordDecomposer</code>","text":"<p>             Bases: <code>RecordDecomposer</code></p> <p>Simply returns the original <code>ProviderContext</code>.</p> <p><code>decompose_record</code> will take the supplied <code>ProviderContext</code> and return it as the only decomposed record in the set.</p> Source code in <code>nodestream/interpreting/record_decomposers.py</code> Python<pre><code>class WholeRecordDecomposer(RecordDecomposer):\n\"\"\"Simply returns the original `ProviderContext`.\n\n    `decompose_record` will take the supplied `ProviderContext` and return it as the only\n    decomposed record in the set.\n    \"\"\"\n\n    def decompose_record(self, context: ProviderContext) -&gt; Iterable[ProviderContext]:\n        yield context\n</code></pre>"},{"location":"python_reference/interpreting/interpretations/extract_variables_interpretation/","title":"Extract variables interpretation","text":""},{"location":"python_reference/interpreting/interpretations/extract_variables_interpretation/#nodestream.interpreting.interpretations.extract_variables_interpretation.ExtractVariablesInterpretation","title":"<code>ExtractVariablesInterpretation</code>","text":"<p>             Bases: <code>Interpretation</code></p> <p>Stores variables that can be used later in the processing of a record.</p> <p>You may store an arbitrary set of properties as variables that come from any value provider or statically provided.</p> YAML<pre><code>interpretations:\n- type: variables\nvariables:\nfirst_name: !jmespath first_name\nlast_name: Smith\n</code></pre> <p>You may also apply normalization in the same way as any other interpretation.</p> YAML<pre><code>interpretations:\n- type: variables\nvariables:\nfirst_name: !jmespath first_name\nnormalization:\ndo_lowercase_strings: true\n</code></pre> Source code in <code>nodestream/interpreting/interpretations/extract_variables_interpretation.py</code> Python<pre><code>class ExtractVariablesInterpretation(Interpretation, alias=\"variables\"):\n\"\"\"Stores variables that can be used later in the processing of a record.\n\n    You may store an arbitrary set of properties as variables that come from any value provider or statically provided.\n\n    ```yaml\n    interpretations:\n      - type: variables\n        variables:\n          first_name: !jmespath first_name\n          last_name: Smith\n    ```\n\n    You may also apply normalization in the same way as any other interpretation.\n\n    ```yaml\n    interpretations:\n      - type: variables\n        variables:\n          first_name: !jmespath first_name\n        normalization:\n            do_lowercase_strings: true\n    ```\n    \"\"\"\n\n    __slots__ = (\"variables\", \"norm_args\")\n\n    def __init__(\n        self,\n        variables: Dict[str, StaticValueOrValueProvider],\n        normalization: Optional[Dict[str, Any]] = None,\n    ):\n        self.variables = ValueProvider.guarantee_provider_dictionary(variables)\n        self.norm_args = normalization or {}\n\n    def interpret(self, context: ProviderContext):\n        context.variables.apply_providers(context, self.variables, **self.norm_args)\n</code></pre>"},{"location":"python_reference/interpreting/interpretations/interpretation/","title":"Interpretation","text":""},{"location":"python_reference/interpreting/interpretations/properties_interpretation/","title":"Properties interpretation","text":""},{"location":"python_reference/interpreting/interpretations/properties_interpretation/#nodestream.interpreting.interpretations.properties_interpretation.PropertiesInterpretation","title":"<code>PropertiesInterpretation</code>","text":"<p>             Bases: <code>Interpretation</code></p> <p>Stores additional properties onto the source node.</p> Source code in <code>nodestream/interpreting/interpretations/properties_interpretation.py</code> Python<pre><code>class PropertiesInterpretation(Interpretation, alias=\"properties\"):\n\"\"\"Stores additional properties onto the source node.\"\"\"\n\n    __slots__ = (\"properties\", \"norm_args\")\n\n    def __init__(\n        self,\n        properties: StaticValueOrValueProvider,\n        normalization: Optional[Dict[str, Any]] = None,\n    ):\n        self.properties = ValueProvider.guarantee_provider_dictionary(properties)\n        self.norm_args = normalization or {}\n\n    def interpret(self, context: ProviderContext):\n        source = context.desired_ingest.source\n        source.properties.apply_providers(context, self.properties, **self.norm_args)\n\n    def gather_object_shapes(self) -&gt; Iterable[GraphObjectShape]:\n        yield GraphObjectShape(\n            graph_object_type=GraphObjectType.NODE,\n            object_type=UnknownTypeMarker.source_node(),\n            properties=PropertyMetadataSet.from_names(self.properties.keys()),\n        )\n</code></pre>"},{"location":"python_reference/interpreting/interpretations/relationship_interpretation/","title":"Relationship interpretation","text":""},{"location":"python_reference/interpreting/interpretations/relationship_interpretation/#nodestream.interpreting.interpretations.relationship_interpretation.InvalidKeyLengthError","title":"<code>InvalidKeyLengthError</code>","text":"<p>             Bases: <code>ValueError</code></p> <p>Raised when a related nodes have differing lengths of key parts returned from a value provider..</p> Source code in <code>nodestream/interpreting/interpretations/relationship_interpretation.py</code> Python<pre><code>class InvalidKeyLengthError(ValueError):\n\"\"\"Raised when a related nodes have differing lengths of key parts returned from a value provider..\"\"\"\n\n    def __init__(self, district_lengths, *args: object) -&gt; None:\n        lengths = f\"({','.join((str(length) for length in district_lengths))})\"\n        error = f\"Node Relationships do not have a consistent key length. Lengths are: ({lengths}) \"\n        super().__init__(error, *args)\n</code></pre>"},{"location":"python_reference/interpreting/interpretations/relationship_interpretation/#nodestream.interpreting.interpretations.relationship_interpretation.RelationshipInterpretation","title":"<code>RelationshipInterpretation</code>","text":"<p>             Bases: <code>Interpretation</code></p> <p>Provides a generic method by which to interpret a relationship between the source node and zero-to-many related nodes.</p> Source code in <code>nodestream/interpreting/interpretations/relationship_interpretation.py</code> Python<pre><code>class RelationshipInterpretation(Interpretation, alias=\"relationship\"):\n\"\"\"Provides a generic method by which to interpret a relationship between the source node and zero-to-many related nodes.\"\"\"\n\n    __slots__ = (\n        \"can_find_many\",\n        \"outbound\",\n        \"match_strategy\",\n        \"decomposer\",\n        \"node_type\",\n        \"relationship_type\",\n        \"node_key\",\n        \"node_properties\",\n        \"relationship_key\",\n        \"relationship_properties\",\n        \"key_normalization\",\n        \"properties_normalization\",\n        \"key_search_algorithm\",\n    )\n\n    def __init__(\n        self,\n        node_type: StaticValueOrValueProvider,\n        relationship_type: StaticValueOrValueProvider,\n        node_key: Dict[str, StaticValueOrValueProvider],\n        node_properties: Optional[Dict[str, StaticValueOrValueProvider]] = None,\n        relationship_key: Optional[Dict[str, StaticValueOrValueProvider]] = None,\n        relationship_properties: Optional[Dict[str, StaticValueOrValueProvider]] = None,\n        outbound: bool = True,\n        find_many: bool = False,\n        iterate_on: Optional[ValueProvider] = None,\n        match_strategy: Optional[str] = None,\n        key_normalization: Optional[Dict[str, Any]] = None,\n        properties_normalization: Optional[Dict[str, Any]] = None,\n    ):\n        self.can_find_many = find_many or iterate_on is not None\n        self.outbound = outbound\n        self.match_strategy = MatchStrategy(match_strategy or MatchStrategy.EAGER.value)\n        self.decomposer = RecordDecomposer.from_iteration_arguments(iterate_on)\n        self.node_type = ValueProvider.guarantee_value_provider(node_type)\n        self.relationship_type = ValueProvider.guarantee_value_provider(\n            relationship_type\n        )\n        self.node_key = ValueProvider.guarantee_provider_dictionary(node_key)\n        self.node_properties = ValueProvider.guarantee_provider_dictionary(\n            node_properties or {}\n        )\n        self.relationship_key = ValueProvider.guarantee_provider_dictionary(\n            relationship_key or {}\n        )\n        self.relationship_properties = ValueProvider.guarantee_provider_dictionary(\n            relationship_properties or {}\n        )\n        self.key_normalization = {\n            **DEFAULT_KEY_NORMALIZATION_ARGUMENTS,\n            **(key_normalization or {}),\n        }\n        self.properties_normalization = properties_normalization or {}\n\n        key_search_algorithm = (\n            MultiNodeKeySearchAlgorithm if find_many else SingleNodeKeySearchAlgorithm\n        )\n        self.key_search_algorithm = key_search_algorithm(\n            self.node_key, self.key_normalization\n        )\n\n    def interpret(self, context: ProviderContext):\n        for sub_context in self.decomposer.decompose_record(context):\n            for relationship, related_node in self.find_matches(sub_context):\n                context.desired_ingest.add_relationship(\n                    related_node, relationship, self.outbound, self.match_strategy\n                )\n\n    def find_relationship(self, context: ProviderContext) -&gt; Relationship:\n        rel = Relationship(type=self.relationship_type.single_value(context))\n        rel.key_values.apply_providers(\n            context, self.relationship_key, **self.key_normalization\n        )\n        rel.properties.apply_providers(\n            context, self.relationship_properties, **self.properties_normalization\n        )\n        return rel\n\n    def find_related_nodes(self, context: ProviderContext) -&gt; Iterable[Node]:\n        for key_set in self.key_search_algorithm.get_related_node_key_sets(context):\n            node = Node(\n                type=self.node_type.single_value(context),\n                key_values=PropertySet(key_set),\n            )\n            if node.has_valid_id:\n                node.properties.apply_providers(\n                    context, self.node_properties, **self.properties_normalization\n                )\n                yield node\n\n    def find_matches(\n        self, context: ProviderContext\n    ) -&gt; Iterable[Tuple[Relationship, Node]]:\n        relationship = self.find_relationship(context)\n        for related_node in self.find_related_nodes(context):\n            yield relationship, related_node\n\n    # NOTE: We cannot participate in introspection when we don't know the relationship\n    # or related node type until runtime. Sometimes we can partially participate if we know\n    # one or the other.\n\n    def gather_present_relationships(self):\n        if not all((self.relationship_type.is_static, self.node_type.is_static)):\n            return\n\n        relationship_type = KnownTypeMarker(self.relationship_type.value)\n        source_node = UnknownTypeMarker.source_node()\n        related_node = KnownTypeMarker(type=self.node_type.value)\n        from_type = source_node if self.outbound else related_node\n        to_type = related_node if self.outbound else source_node\n        foreign_cardinality = Cardinality.MANY\n        source_cardinality = (\n            Cardinality.MANY if self.can_find_many else Cardinality.SINGLE\n        )\n\n        if self.outbound:\n            to_side_cardinality, from_side_cardinality = (\n                foreign_cardinality,\n                source_cardinality,\n            )\n        else:\n            from_side_cardinality, to_side_cardinality = (\n                foreign_cardinality,\n                source_cardinality,\n            )\n\n        yield PresentRelationship(\n            from_object_type=from_type,\n            to_object_type=to_type,\n            relationship_type=relationship_type,\n            to_side_cardinality=to_side_cardinality,\n            from_side_cardinality=from_side_cardinality,\n        )\n\n    def gather_used_indexes(self):\n        if self.node_type.is_static:\n            related_node_type = self.node_type.value\n            yield FieldIndex.for_ttl_timestamp(related_node_type)\n\n            # If we are matching fuzzy or MATCH_ONLY, we cannot rely on the key index\n            # to find the related node.\n            # TODO: Perhaps in the future we can do a FieldIndex instead?\n            if self.match_strategy == MatchStrategy.EAGER:\n                yield KeyIndex(related_node_type, frozenset(self.node_key.keys()))\n\n        if self.relationship_type.is_static:\n            relationship_type = self.relationship_type.value\n            yield FieldIndex.for_ttl_timestamp(\n                relationship_type, object_type=GraphObjectType.RELATIONSHIP\n            )\n\n    def gather_object_shapes(self):\n        if self.node_type.is_static:\n            yield GraphObjectShape(\n                graph_object_type=GraphObjectType.NODE,\n                object_type=KnownTypeMarker(self.node_type.value),\n                properties=PropertyMetadataSet.from_names(\n                    self.node_properties.keys(), self.node_key.keys()\n                ),\n            )\n\n        if self.relationship_type.is_static:\n            yield GraphObjectShape(\n                graph_object_type=GraphObjectType.RELATIONSHIP,\n                object_type=KnownTypeMarker(self.relationship_type.value),\n                properties=PropertyMetadataSet.from_names(\n                    self.relationship_key.keys(), self.relationship_properties.keys()\n                ),\n            )\n</code></pre>"},{"location":"python_reference/interpreting/interpretations/source_node_interpretation/","title":"Source node interpretation","text":""},{"location":"python_reference/interpreting/interpretations/source_node_interpretation/#nodestream.interpreting.interpretations.source_node_interpretation.SourceNodeInterpretation","title":"<code>SourceNodeInterpretation</code>","text":"<p>             Bases: <code>Interpretation</code></p> <p>Stores information regarding the source node.</p> <p>Within a Pipeline File, a simple usage may look like this:</p> YAML<pre><code>interpretations:\n# Conventionally, this should be first.\n- type: source_node\nnode_type: TheTypeOfTheNodeYouAreIngesting\nkey:\nid: !!python/jmespath value_for_id_field\n</code></pre> <p>However, more complex usages may look like this to populate a rich node.</p> YAML<pre><code>interpretations:\n- type: source_node\nnode_type: Person\nkey:\nfirst_name: !jmespath first_name\nlast_name: !jmespath last_name\nproperties:\nbio: !jmespath biography\n</code></pre> <p>You may also apply additional labels and field level indexes:</p> YAML<pre><code>interpretations:\n- type: source_node\nnode_type: Person\nkey:\nfirst_name: !jmespath first_name\nlast_name: !jmespath last_name\nproperties:\nphone_number: !jmespath phone\nadditionally_index:\n- phone_number\nadditional_types:\n- Customer\n</code></pre> <p>You are also allowed by default to send normalization flags in:</p> YAML<pre><code>interpretations:\n- type: source_node\nnode_type: DomainName\nkey:\nname: !jmespath name\nnormalization:\ndrop_trailing_dots: true\n</code></pre> Source code in <code>nodestream/interpreting/interpretations/source_node_interpretation.py</code> Python<pre><code>class SourceNodeInterpretation(Interpretation, alias=\"source_node\"):\n\"\"\"Stores information regarding the source node.\n\n    Within a Pipeline File, a simple usage may look like this:\n\n    ```yaml\n    interpretations:\n      # Conventionally, this should be first.\n      - type: source_node\n        node_type: TheTypeOfTheNodeYouAreIngesting\n        key:\n          id: !!python/jmespath value_for_id_field\n    ```\n\n    However, more complex usages may look like this to populate a rich node.\n\n    ```yaml\n    interpretations:\n      - type: source_node\n        node_type: Person\n        key:\n          first_name: !jmespath first_name\n          last_name: !jmespath last_name\n        properties:\n           bio: !jmespath biography\n    ```\n\n    You may also apply additional labels and field level indexes:\n\n    ```yaml\n    interpretations:\n      - type: source_node\n        node_type: Person\n        key:\n          first_name: !jmespath first_name\n          last_name: !jmespath last_name\n        properties:\n           phone_number: !jmespath phone\n        additionally_index:\n            - phone_number\n        additional_types:\n            - Customer\n    ```\n\n    You are also allowed by default to send normalization flags in:\n\n    ```yaml\n    interpretations:\n      - type: source_node\n        node_type: DomainName\n        key:\n          name: !jmespath name\n        normalization:\n          drop_trailing_dots: true\n    ```\n    \"\"\"\n\n    __slots__ = (\n        \"node_type\",\n        \"key\",\n        \"properties\",\n        \"additional_indexes\",\n        \"additional_types\",\n        \"norm_args\",\n    )\n\n    def __init__(\n        self,\n        node_type: StaticValueOrValueProvider,\n        key: Dict[str, StaticValueOrValueProvider],\n        properties: Optional[Dict[str, StaticValueOrValueProvider]] = None,\n        additional_indexes: Optional[List[str]] = None,\n        additional_types: Optional[List[str]] = None,\n        normalization: Optional[Dict[str, Any]] = None,\n    ):\n        self.node_type = ValueProvider.guarantee_value_provider(node_type)\n        self.key = ValueProvider.guarantee_provider_dictionary(key)\n        self.properties = ValueProvider.guarantee_provider_dictionary(properties or {})\n        self.additional_indexes = additional_indexes or []\n        self.additional_types = tuple(additional_types or [])\n        self.norm_args = {**DEFAULT_NORMALIZATION_ARGUMENTS, **(normalization or {})}\n\n    def interpret(self, context: ProviderContext):\n        source = context.desired_ingest.source\n        source.type = self.node_type.single_value(context)\n        source.key_values.apply_providers(context, self.key, **self.norm_args)\n        source.properties.apply_providers(context, self.properties, **self.norm_args)\n        source.additional_types = self.additional_types\n\n    def gather_used_indexes(self) -&gt; Iterable[Union[KeyIndex, FieldIndex]]:\n        # NOTE: We cannot generate indexes when we do not know the type until runtime.\n        if not self.node_type.is_static:\n            return\n\n        node_type = self.node_type.value\n        yield KeyIndex(node_type, frozenset(self.key.keys()))\n        yield FieldIndex.for_ttl_timestamp(node_type)\n        for field in self.additional_indexes:\n            yield FieldIndex(node_type, field, object_type=GraphObjectType.NODE)\n\n    def gather_object_shapes(self) -&gt; Iterable[GraphObjectShape]:\n        # NOTE: We cannot generate schemas when we do not know the type until runtime.\n        if not self.node_type.is_static:\n            return\n\n        node_type = self.node_type.value\n        yield GraphObjectShape(\n            graph_object_type=GraphObjectType.NODE,\n            object_type=KnownTypeMarker.fulfilling_source_node(node_type),\n            properties=PropertyMetadataSet.from_names(\n                self.key.keys(), self.properties.keys()\n            ),\n        )\n</code></pre>"},{"location":"python_reference/interpreting/interpretations/switch_interpretation/","title":"Switch interpretation","text":""},{"location":"python_reference/interpreting/interpretations/switch_interpretation/#nodestream.interpreting.interpretations.switch_interpretation.UnhandledBranchError","title":"<code>UnhandledBranchError</code>","text":"<p>             Bases: <code>ValueError</code></p> <p>Raised when a branch is not handled in a switch case.</p> Source code in <code>nodestream/interpreting/interpretations/switch_interpretation.py</code> Python<pre><code>class UnhandledBranchError(ValueError):\n\"\"\"Raised when a branch is not handled in a switch case.\"\"\"\n\n    def __init__(self, missing_branch_value, *args: object) -&gt; None:\n        super().__init__(\n            f\"'{missing_branch_value}' was not matched in switch case\", *args\n        )\n</code></pre>"},{"location":"python_reference/model/desired_ingestion/","title":"Desired ingestion","text":""},{"location":"python_reference/model/graph_objects/","title":"Graph objects","text":""},{"location":"python_reference/model/graph_objects/#nodestream.model.graph_objects.Node","title":"<code>Node</code>  <code>dataclass</code>","text":"<p>             Bases: <code>DeduplicatableObject</code></p> <p>A <code>Node</code> is a entity that has a distinct identity.</p> <p>Each <code>Node</code> represents an entity (a person, place, thing, category or other piece of data) that has a distinct identity. Nodestream assumes the underlying graph database layer is a Labeled Property Graph. The identity of a node is defined by a root <code>type</code> (sometimes referred to as a label) as well as set of property name, value pairs representing the primary key of that node. In a relational database, this would be the combination of the table name as well as the primary key columns.</p> <p>The node class also can store additional property key value pairs that are not considered part of the identity of the node but rather additional data. In a relational database, these would be the non-primary key columns.</p> <p>A <code>Node</code> can also store additional types (labels) that can apply additional categorization in the database. This has no direct analogy in a relational database.</p> Source code in <code>nodestream/model/graph_objects.py</code> Python<pre><code>@dataclass(slots=True)\nclass Node(DeduplicatableObject):\n\"\"\"A `Node` is a entity that has a distinct identity.\n\n    Each `Node` represents an entity (a person, place, thing, category or other piece of data) that has a distinct\n    identity. Nodestream assumes the underlying graph database layer is a Labeled Property Graph. The identity\n    of a node is defined by a root `type` (sometimes referred to as a label) as well as set of property name, value pairs\n    representing the primary key of that node. In a relational database, this would be the combination of the table name\n    as well as the primary key columns.\n\n    The node class also can store additional property key value pairs that are not considered part of the identity of the\n    node but rather additional data. In a relational database, these would be the non-primary key columns.\n\n    A `Node` can also store additional types (labels) that can apply additional categorization in the database. This has\n    no direct analogy in a relational database.\n    \"\"\"\n\n    type: Optional[str] = None\n    key_values: PropertySet = field(default_factory=PropertySet.empty)\n    properties: PropertySet = field(default_factory=PropertySet.default_properties)\n    additional_types: Tuple[str] = field(default_factory=tuple)\n\n    @property\n    def has_valid_id(self) -&gt; bool:\n        # Return that some of the ID values are defined.\n        return not all(value is None for value in self.key_values.values())\n\n    @property\n    def is_valid(self) -&gt; bool:\n        return self.has_valid_id and self.type is not None\n\n    @property\n    def identity_shape(self) -&gt; \"NodeIdentityShape\":\n        return NodeIdentityShape(\n            type=self.type,\n            keys=tuple(self.key_values.keys()),\n            additional_types=self.additional_types,\n        )\n\n    def has_same_key(self, other: \"Node\") -&gt; bool:\n        return self.key_values == other.key_values\n\n    def update(self, other: \"Relationship\"):\n        self.properties.update(other.properties)\n\n    def get_dedup_key(self) -&gt; tuple:\n        return tuple(sorted(self.key_values.values()))\n</code></pre>"},{"location":"python_reference/model/graph_objects/#nodestream.model.graph_objects.PropertySet","title":"<code>PropertySet</code>","text":"<p>             Bases: <code>dict</code></p> Source code in <code>nodestream/model/graph_objects.py</code> Python<pre><code>class PropertySet(dict):\n    def set_property(self, property_key: str, property_value: Any):\n        self[property_key] = property_value\n\n    @classmethod\n    def default_properties(cls) -&gt; \"PropertySet\":\n        from pandas import Timestamp\n\n        from ..pipeline.meta import get_context\n\n\"\"\"Returns a default set of properties which set values.\n\n        These default values indicate when the current pipeline touched the object the properties are for.\n        \"\"\"\n        pipeline_name = get_context().name\n        now = Timestamp.utcnow()\n        return cls(\n            {\n                \"last_ingested_at\": now,\n                f\"last_ingested_by_{pipeline_name}_at\": now,\n                f\"was_ingested_by_{pipeline_name}\": True,\n            }\n        )\n\n    @classmethod\n    def empty(cls) -&gt; \"PropertySet\":\n\"\"\"Returns an empty property set.\"\"\"\n        return PropertySet()\n\n    def apply_providers(\n        self,\n        context: \"ProviderContext\",\n        provider_map: \"Dict[str, ValueProvider]\",\n        **norm_args,\n    ):\n\"\"\"For every `(key, provider)` pair provided, sets the property to the values provided.\n\n        This method can take arbitrary keyword arguments which are passed to `ValueProvider` as\n        arguments for value normalization.\n        \"\"\"\n        for key, provider in provider_map.items():\n            v = provider.normalize_single_value(context, **norm_args)\n            self.set_property(key, v)\n</code></pre>"},{"location":"python_reference/model/graph_objects/#nodestream.model.graph_objects.PropertySet.apply_providers","title":"<code>apply_providers(context, provider_map, **norm_args)</code>","text":"<p>For every <code>(key, provider)</code> pair provided, sets the property to the values provided.</p> <p>This method can take arbitrary keyword arguments which are passed to <code>ValueProvider</code> as arguments for value normalization.</p> Source code in <code>nodestream/model/graph_objects.py</code> Python<pre><code>def apply_providers(\n    self,\n    context: \"ProviderContext\",\n    provider_map: \"Dict[str, ValueProvider]\",\n    **norm_args,\n):\n\"\"\"For every `(key, provider)` pair provided, sets the property to the values provided.\n\n    This method can take arbitrary keyword arguments which are passed to `ValueProvider` as\n    arguments for value normalization.\n    \"\"\"\n    for key, provider in provider_map.items():\n        v = provider.normalize_single_value(context, **norm_args)\n        self.set_property(key, v)\n</code></pre>"},{"location":"python_reference/model/graph_objects/#nodestream.model.graph_objects.PropertySet.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Returns an empty property set.</p> Source code in <code>nodestream/model/graph_objects.py</code> Python<pre><code>@classmethod\ndef empty(cls) -&gt; \"PropertySet\":\n\"\"\"Returns an empty property set.\"\"\"\n    return PropertySet()\n</code></pre>"},{"location":"python_reference/model/graph_objects/#nodestream.model.graph_objects.Relationship","title":"<code>Relationship</code>  <code>dataclass</code>","text":"<p>             Bases: <code>DeduplicatableObject</code></p> <p>A <code>Relationship</code> represents an inherent connection between two <code>Node</code>s.</p> <p>Each <code>Relationship</code> follows a relatively similar model to a <code>Node</code>. There is a single type for the relationship. Relationships can store properties on the relationship itself (This would be similar to a jump table in a relational database).</p> <p>A key for a <code>Relationship</code> can also be provided. By default, <code>nodestream</code> will assume that there should be one relationship of the same type between two nodes. By providing keys, <code>nodestream</code> will create multiple relationships between two nodes and will discriminate based on the key values.</p> <p>This model represents the relationship itself and DOES NOT include a reference of the nodes that are stored.</p> Source code in <code>nodestream/model/graph_objects.py</code> Python<pre><code>@dataclass(slots=True, frozen=True)\nclass Relationship(DeduplicatableObject):\n\"\"\"A `Relationship` represents an inherent connection between two `Node`s.\n\n    Each `Relationship` follows a relatively similar model to a `Node`. There is a _single_ type for the relationship.\n    Relationships can store properties on the relationship itself (This would be similar to a jump table in a relational database).\n\n    A key for a `Relationship` can also be provided. By default, `nodestream` will assume that there should be one\n    relationship of the same type between two nodes. By providing keys, `nodestream` will create multiple relationships between\n    two nodes and will discriminate based on the key values.\n\n    This model represents the relationship itself and DOES NOT include a reference of the nodes that are stored.\n    \"\"\"\n\n    type: str\n    key_values: PropertySet = field(default_factory=PropertySet.empty)\n    properties: PropertySet = field(default_factory=PropertySet.default_properties)\n\n    @property\n    def identity_shape(self) -&gt; \"RelationshipIdentityShape\":\n        return RelationshipIdentityShape(\n            type=self.type, keys=tuple(self.key_values.keys())\n        )\n\n    def has_same_key(self, other: \"Node\") -&gt; bool:\n        return self.key_values == other.key_values\n\n    def update(self, other: \"Relationship\"):\n        self.properties.update(other.properties)\n\n    def get_dedup_key(self) -&gt; tuple:\n        return tuple(sorted(self.key_values.values()))\n</code></pre>"},{"location":"python_reference/model/graph_objects/#nodestream.model.graph_objects.RelationshipWithNodes","title":"<code>RelationshipWithNodes</code>  <code>dataclass</code>","text":"<p>             Bases: <code>DeduplicatableObject</code></p> <p>Stores information about the related node and the relationship itself.</p> Source code in <code>nodestream/model/graph_objects.py</code> Python<pre><code>@dataclass(slots=True)\nclass RelationshipWithNodes(DeduplicatableObject):\n\"\"\"Stores information about the related node and the relationship itself.\"\"\"\n\n    from_node: Node\n    to_node: Node\n    relationship: Relationship\n\n    to_side_match_strategy: MatchStrategy = MatchStrategy.EAGER\n    from_side_match_strategy: MatchStrategy = MatchStrategy.EAGER\n\n    def has_same_keys(self, other: \"RelationshipWithNodes\") -&gt; bool:\n        return (\n            self.to_node.has_same_key(other.to_node)\n            and self.from_node.has_same_key(other.from_node)\n            and self.relationship.has_same_key(other.relationship)\n        )\n\n    def update(self, other: \"RelationshipWithNodes\"):\n        self.to_node.properties.update(other.to_node.properties)\n        self.from_node.properties.update(other.from_node.properties)\n        self.relationship.properties.update(other.relationship.properties)\n\n    def get_dedup_key(self) -&gt; tuple:\n        return (\n            self.to_node.get_dedup_key(),\n            self.from_node.get_dedup_key(),\n            self.relationship.get_dedup_key(),\n        )\n</code></pre>"},{"location":"python_reference/model/ingestion_hooks/","title":"Ingestion hooks","text":""},{"location":"python_reference/model/ingestion_hooks/#nodestream.model.ingestion_hooks.IngestionHook","title":"<code>IngestionHook</code>","text":"<p>             Bases: <code>ABC</code></p> <p>An IngestionHook is a custom piece of logic that is bundled as a query.</p> <p>IngestionHooks provide a mechanism to enrich the graph model that is derived from data added to and currently existing in the graph. For example, drawing an edge between two nodes where following a complex path.</p> Source code in <code>nodestream/model/ingestion_hooks.py</code> Python<pre><code>class IngestionHook(ABC):\n\"\"\"An IngestionHook is a custom piece of logic that is bundled as a query.\n\n    IngestionHooks provide a mechanism to enrich the graph model that is derived from\n    data added to and currently existing in the graph. For example, drawing an edge\n    between two nodes where following a complex path.\n    \"\"\"\n\n    @abstractmethod\n    def as_cypher_query_and_parameters(self) -&gt; Tuple[str, Dict[str, Any]]:\n\"\"\"Returns a cypher query string and parameters to execute.\"\"\"\n\n        raise NotImplementedError\n</code></pre>"},{"location":"python_reference/model/ingestion_hooks/#nodestream.model.ingestion_hooks.IngestionHook.as_cypher_query_and_parameters","title":"<code>as_cypher_query_and_parameters()</code>  <code>abstractmethod</code>","text":"<p>Returns a cypher query string and parameters to execute.</p> Source code in <code>nodestream/model/ingestion_hooks.py</code> Python<pre><code>@abstractmethod\ndef as_cypher_query_and_parameters(self) -&gt; Tuple[str, Dict[str, Any]]:\n\"\"\"Returns a cypher query string and parameters to execute.\"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"python_reference/model/ingestion_hooks/#nodestream.model.ingestion_hooks.IngestionHookRunRequest","title":"<code>IngestionHookRunRequest</code>  <code>dataclass</code>","text":"<p>An <code>IngestionHookRunRequest</code> defines what hook is meant to be run and when the hook should run.</p> Source code in <code>nodestream/model/ingestion_hooks.py</code> Python<pre><code>@dataclass(slots=True, frozen=True)\nclass IngestionHookRunRequest:\n\"\"\"An `IngestionHookRunRequest` defines what hook is meant to be run and when the hook should run.\"\"\"\n\n    hook: IngestionHook\n    before_ingest: bool\n</code></pre>"},{"location":"python_reference/model/match_strategy/","title":"Match strategy","text":""},{"location":"python_reference/model/ttl/","title":"Ttl","text":""},{"location":"python_reference/pipeline/class_loader/","title":"Class loader","text":""},{"location":"python_reference/pipeline/class_loader/#nodestream.pipeline.class_loader.ClassLoader","title":"<code>ClassLoader</code>","text":"<p>Loads a class from a string path and instantiates it with the given arguments.</p> Source code in <code>nodestream/pipeline/class_loader.py</code> Python<pre><code>class ClassLoader:\n\"\"\"Loads a class from a string path and instantiates it with the given arguments.\"\"\"\n\n    def find_class_initializer(self, implementation, factory=None):\n        class_definition = find_class(implementation)\n        factory_method = factory or DECLARATIVE_INIT_METHOD_NAME\n        if hasattr(class_definition, factory_method):\n            return getattr(class_definition, factory_method)\n        else:\n            return class_definition\n\n    def load_class(self, implementation, arguments=None, factory=None):\n        arguments = arguments or {}\n        initializer = self.find_class_initializer(implementation, factory)\n        try:\n            return initializer(**arguments)\n        except TypeError as e:\n            raise PipelineComponentInitializationError(initializer, arguments) from e\n</code></pre>"},{"location":"python_reference/pipeline/class_loader/#nodestream.pipeline.class_loader.InvalidClassPathError","title":"<code>InvalidClassPathError</code>","text":"<p>             Bases: <code>ValueError</code></p> <p>Raised when a class path is invalid.</p> Source code in <code>nodestream/pipeline/class_loader.py</code> Python<pre><code>class InvalidClassPathError(ValueError):\n\"\"\"Raised when a class path is invalid.\"\"\"\n\n    pass\n</code></pre>"},{"location":"python_reference/pipeline/class_loader/#nodestream.pipeline.class_loader.PipelineComponentInitializationError","title":"<code>PipelineComponentInitializationError</code>","text":"<p>             Bases: <code>ValueError</code></p> <p>Raised when a component fails to initialize.</p> Source code in <code>nodestream/pipeline/class_loader.py</code> Python<pre><code>class PipelineComponentInitializationError(ValueError):\n\"\"\"Raised when a component fails to initialize.\"\"\"\n\n    def __init__(self, initializer, init_arguments, *args: object) -&gt; None:\n        super().__init__(\n            \"Failed to Initialize Component in Declarative Pipeline. Likely the arguments are incorrect.\",\n            *args,\n        )\n        self.initializer = initializer\n        self.init_arguments = init_arguments\n</code></pre>"},{"location":"python_reference/pipeline/filters/","title":"Filters","text":""},{"location":"python_reference/pipeline/filters/#nodestream.pipeline.filters.Filter","title":"<code>Filter</code>","text":"<p>             Bases: <code>Step</code></p> <p>A <code>Filter</code> takes a given record and evaluates whether or not it should continue downstream.</p> <p><code>Filter</code> steps generally make up the middle of an ETL pipeline and are responsible for ensuring only relevant records make it through.</p> Source code in <code>nodestream/pipeline/filters.py</code> Python<pre><code>class Filter(Step):\n\"\"\"A `Filter` takes a given record and evaluates whether or not it should continue downstream.\n\n    `Filter` steps generally make up the middle of an ETL pipeline and are responsible\n    for ensuring only relevant records make it through.\n    \"\"\"\n\n    async def handle_async_record_stream(\n        self, record_stream: AsyncGenerator[Any, Any]\n    ) -&gt; AsyncGenerator[Any, Any]:\n        async for record in record_stream:\n            if record is Flush or not self.should_filter(record):\n                yield record\n\n    @abstractmethod\n    async def filter_record(self, record: Any) -&gt; bool:\n        raise NotImplementedError\n</code></pre>"},{"location":"python_reference/pipeline/filters/#nodestream.pipeline.filters.ValuesMatchPossibilitiesFilter","title":"<code>ValuesMatchPossibilitiesFilter</code>","text":"<p>             Bases: <code>Filter</code></p> <p>A filter that checks if a given value matches any of a set of possibilities.</p> Source code in <code>nodestream/pipeline/filters.py</code> Python<pre><code>class ValuesMatchPossibilitiesFilter(Filter):\n\"\"\"A filter that checks if a given value matches any of a set of possibilities.\"\"\"\n\n    @classmethod\n    def from_file_data(cls, *, fields: Iterable[Dict[str, Any]]):\n        value_matchers = [ValueMatcher.from_file_data(**field) for field in fields]\n        return cls(value_matchers=value_matchers)\n\n    def __init__(self, value_matchers: Iterable[ValueMatcher]):\n        self.value_matchers = value_matchers\n\n    async def filter_record(self, item):\n        context_from_record = ProviderContext(item, None)\n        return not all(\n            matcher.does_match(context_from_record) for matcher in self.value_matchers\n        )\n</code></pre>"},{"location":"python_reference/pipeline/flush/","title":"Flush","text":""},{"location":"python_reference/pipeline/meta/","title":"Meta","text":""},{"location":"python_reference/pipeline/pipeline/","title":"Pipeline","text":""},{"location":"python_reference/pipeline/pipeline/#nodestream.pipeline.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>             Bases: <code>AggregatedIntrospectiveIngestionComponent</code></p> <p>A pipeline is a series of steps that are executed in order.</p> Source code in <code>nodestream/pipeline/pipeline.py</code> Python<pre><code>class Pipeline(AggregatedIntrospectiveIngestionComponent):\n\"\"\"A pipeline is a series of steps that are executed in order.\"\"\"\n\n    __slots__ = (\"steps\",)\n\n    def __init__(self, steps: List[Step], step_outbox_size: int) -&gt; None:\n        self.steps = steps\n        self.step_outbox_size = step_outbox_size\n\n    async def run(\n        self, progress_reporter: Optional[PipelineProgressReporter] = None\n    ) -&gt; AsyncGenerator[Any, Any]:\n        current_executor = None\n        tasks = []\n\n        for step in self.steps:\n            current_executor = StepExecutor(\n                upstream=current_executor, step=step, outbox_size=self.step_outbox_size\n            )\n            tasks.append(asyncio.create_task(current_executor.work_loop()))\n\n        current_executor.set_end_of_line(progress_reporter)\n\n        await asyncio.gather(*tasks)\n\n    def all_subordinate_components(self) -&gt; Iterable[IntrospectiveIngestionComponent]:\n        return (s for s in self.steps if isinstance(s, IntrospectiveIngestionComponent))\n</code></pre>"},{"location":"python_reference/pipeline/pipeline_file_loader/","title":"Pipeline file loader","text":""},{"location":"python_reference/pipeline/pipeline_file_loader/#nodestream.pipeline.pipeline_file_loader.InvalidPipelineDefinitionError","title":"<code>InvalidPipelineDefinitionError</code>","text":"<p>             Bases: <code>ValueError</code></p> <p>Raised when a pipeline definition is invalid.</p> Source code in <code>nodestream/pipeline/pipeline_file_loader.py</code> Python<pre><code>class InvalidPipelineDefinitionError(ValueError):\n\"\"\"Raised when a pipeline definition is invalid.\"\"\"\n\n    pass\n</code></pre>"},{"location":"python_reference/pipeline/pipeline_file_loader/#nodestream.pipeline.pipeline_file_loader.PipelineFileLoader","title":"<code>PipelineFileLoader</code>","text":"<p>Loads a pipeline from a YAML file.</p> Source code in <code>nodestream/pipeline/pipeline_file_loader.py</code> Python<pre><code>class PipelineFileLoader:\n\"\"\"Loads a pipeline from a YAML file.\"\"\"\n\n    def __init__(self, file_path: Path):\n        self.file_path = file_path\n\n    def load_pipeline(\n        self, init_args: Optional[PipelineInitializationArguments] = None\n    ) -&gt; Pipeline:\n        init_args = init_args or PipelineInitializationArguments()\n        return self.load_pipeline_from_file_data(\n            self.load_pipeline_file_data(), init_args\n        )\n\n    def load_pipeline_from_file_data(\n        self, file_data, init_args: PipelineInitializationArguments\n    ):\n        if not isinstance(file_data, list):\n            raise InvalidPipelineDefinitionError(\n                \"File should be a list of step class to load\"\n            )\n\n        return init_args.initialize_from_file_data(file_data)\n\n    def load_pipeline_file_data(self):\n        return PipelineFileSafeLoader.load_file_by_path(self.file_path)\n</code></pre>"},{"location":"python_reference/pipeline/pipeline_file_loader/#nodestream.pipeline.pipeline_file_loader.PipelineFileSafeLoader","title":"<code>PipelineFileSafeLoader</code>","text":"<p>             Bases: <code>SafeLoader</code></p> <p>A YAML loader that can load pipeline files.</p> Source code in <code>nodestream/pipeline/pipeline_file_loader.py</code> Python<pre><code>class PipelineFileSafeLoader(SafeLoader):\n\"\"\"A YAML loader that can load pipeline files.\"\"\" \"\"\n\n    was_configured = False\n\n    @classmethod\n    def configure(cls):\n        if cls.was_configured:\n            return\n\n        for normalizer in Normalizer.all():\n            normalizer.setup()\n        for value_provider in ValueProvider.all():\n            value_provider.install_yaml_tag(cls)\n        for argument_resolver in ArgumentResolver.all():\n            argument_resolver.install_yaml_tag(cls)\n\n        cls.was_configured = True\n\n    @classmethod\n    def load_file_by_path(cls, file_path: str):\n        PipelineFileSafeLoader.configure()\n        with open(file_path) as fp:\n            return load(fp, cls)\n</code></pre>"},{"location":"python_reference/pipeline/pipeline_file_loader/#nodestream.pipeline.pipeline_file_loader.PipelineInitializationArguments","title":"<code>PipelineInitializationArguments</code>  <code>dataclass</code>","text":"<p>Arguments used to initialize a pipeline from a file.</p> Source code in <code>nodestream/pipeline/pipeline_file_loader.py</code> Python<pre><code>@dataclass(slots=True)\nclass PipelineInitializationArguments:\n\"\"\"Arguments used to initialize a pipeline from a file.\"\"\"\n\n    step_outbox_size: int = 1000\n    annotations: Optional[List[str]] = None\n\n    @classmethod\n    def for_introspection(cls):\n        return cls(annotations=[\"introspection\"])\n\n    @classmethod\n    def for_testing(cls):\n        return cls(annotations=[\"test\"])\n\n    def initialize_from_file_data(self, file_data: List[dict]):\n        return Pipeline(\n            steps=self.load_steps(ClassLoader(), file_data),\n            step_outbox_size=self.step_outbox_size,\n        )\n\n    def load_steps(self, class_loader, file_data):\n        return [\n            class_loader.load_class(**step_data)\n            for step_data in file_data\n            if self.should_load_step(step_data)\n        ]\n\n    def should_load_step(self, step):\n        return self.step_is_tagged_properly(step)\n\n    def step_is_tagged_properly(self, step):\n        if \"annotations\" in step and self.annotations:\n            if not set(step.pop(\"annotations\")).intersection(self.annotations):\n                return False\n\n        return True\n</code></pre>"},{"location":"python_reference/pipeline/progress_reporter/","title":"Progress reporter","text":""},{"location":"python_reference/pipeline/progress_reporter/#nodestream.pipeline.progress_reporter.PipelineProgressReporter","title":"<code>PipelineProgressReporter</code>  <code>dataclass</code>","text":"<p>A <code>PipelineProgressReporter</code> is a utility that can be used to report on the progress of a pipeline.</p> Source code in <code>nodestream/pipeline/progress_reporter.py</code> Python<pre><code>@dataclass\nclass PipelineProgressReporter:\n\"\"\"A `PipelineProgressReporter` is a utility that can be used to report on the progress of a pipeline.\"\"\"\n\n    reporting_frequency: int = 1000\n    logger: Logger = field(default_factory=getLogger)\n    callback: Callable[[int, Any], None] = field(default=no_op)\n    on_start_callback: Callable[[], None] = field(default=no_op)\n    on_finish_callback: Callable[[PipelineContext], None] = field(default=no_op)\n\n    @classmethod\n    def for_testing(cls, results_list: list) -&gt; \"PipelineProgressReporter\":\n\"\"\"Create a `PipelineProgressReporter` for testing.\n\n        This method is intended to be used for testing purposes only. It will create a\n        `PipelineProgressReporter` with the default values for testing.\n\n        Args:\n            results_list: The list to append results to.\n\n        Returns:\n            PipelineProgressReporter: A `PipelineProgressReporter` for testing.\n        \"\"\"\n        return cls(\n            reporting_frequency=1,\n            logger=getLogger(\"test\"),\n            callback=lambda _, record: results_list.append(record),\n            on_start_callback=no_op,\n            on_finish_callback=no_op,\n        )\n\n    def report(self, index, record):\n        if index % self.reporting_frequency == 0:\n            self.logger.info(\n                \"Records Processed\",\n                extra={\"index\": index, \"max_memory\": get_max_mem_mb()},\n            )\n            self.callback(index, record)\n</code></pre>"},{"location":"python_reference/pipeline/progress_reporter/#nodestream.pipeline.progress_reporter.PipelineProgressReporter.for_testing","title":"<code>for_testing(results_list)</code>  <code>classmethod</code>","text":"<p>Create a <code>PipelineProgressReporter</code> for testing.</p> <p>This method is intended to be used for testing purposes only. It will create a <code>PipelineProgressReporter</code> with the default values for testing.</p> <p>Parameters:</p> Name Type Description Default <code>results_list</code> <code>list</code> <p>The list to append results to.</p> required <p>Returns:</p> Name Type Description <code>PipelineProgressReporter</code> <code>PipelineProgressReporter</code> <p>A <code>PipelineProgressReporter</code> for testing.</p> Source code in <code>nodestream/pipeline/progress_reporter.py</code> Python<pre><code>@classmethod\ndef for_testing(cls, results_list: list) -&gt; \"PipelineProgressReporter\":\n\"\"\"Create a `PipelineProgressReporter` for testing.\n\n    This method is intended to be used for testing purposes only. It will create a\n    `PipelineProgressReporter` with the default values for testing.\n\n    Args:\n        results_list: The list to append results to.\n\n    Returns:\n        PipelineProgressReporter: A `PipelineProgressReporter` for testing.\n    \"\"\"\n    return cls(\n        reporting_frequency=1,\n        logger=getLogger(\"test\"),\n        callback=lambda _, record: results_list.append(record),\n        on_start_callback=no_op,\n        on_finish_callback=no_op,\n    )\n</code></pre>"},{"location":"python_reference/pipeline/progress_reporter/#nodestream.pipeline.progress_reporter.get_max_mem_mb","title":"<code>get_max_mem_mb()</code>","text":"<p>Get the maximum memory used by the current process in MB.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The maximum memory used by the current process in MB.</p> Source code in <code>nodestream/pipeline/progress_reporter.py</code> Python<pre><code>def get_max_mem_mb():\n\"\"\"Get the maximum memory used by the current process in MB.\n\n    Returns:\n        int: The maximum memory used by the current process in MB.\n    \"\"\"\n    max_mem = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n    max_mem /= 1024\n    if platform.system() == \"Darwin\":\n        max_mem /= 1024\n    return int(max_mem)\n</code></pre>"},{"location":"python_reference/pipeline/progress_reporter/#nodestream.pipeline.progress_reporter.no_op","title":"<code>no_op(*_, **__)</code>","text":"<p>A no-op function that does nothing.</p> Source code in <code>nodestream/pipeline/progress_reporter.py</code> Python<pre><code>def no_op(*_, **__):\n\"\"\"A no-op function that does nothing.\"\"\"\n    pass\n</code></pre>"},{"location":"python_reference/pipeline/step/","title":"Step","text":""},{"location":"python_reference/pipeline/step/#nodestream.pipeline.step.PassStep","title":"<code>PassStep</code>","text":"<p>             Bases: <code>Step</code></p> <p>A <code>PassStep</code> is a step that does nothing.</p> Source code in <code>nodestream/pipeline/step.py</code> Python<pre><code>class PassStep(Step):\n\"\"\"A `PassStep` is a step that does nothing.\"\"\"\n\n    async def handle_async_record_stream(\n        self, record_stream: AsyncGenerator[Any, Any]\n    ) -&gt; AsyncGenerator[Any, Any]:\n        async for record in record_stream:\n            yield record\n</code></pre>"},{"location":"python_reference/pipeline/step/#nodestream.pipeline.step.Step","title":"<code>Step</code>","text":"<p>             Bases: <code>ABC</code></p> <p>A <code>Step</code> represents a phase of an ETl pipeline.</p> Source code in <code>nodestream/pipeline/step.py</code> Python<pre><code>class Step(ABC):\n\"\"\"A `Step` represents a phase of an ETl pipeline.\"\"\"\n\n    @classmethod\n    def from_file_data(cls, **kwargs):\n        return cls(**kwargs)\n\n    @abstractmethod\n    async def handle_async_record_stream(\n        self, record_stream: AsyncGenerator[Any, Any]\n    ) -&gt; AsyncGenerator[Any, Any]:\n        raise NotImplementedError\n\n    async def finish(self):\n        pass\n</code></pre>"},{"location":"python_reference/pipeline/writers/","title":"Writers","text":""},{"location":"python_reference/pipeline/writers/#nodestream.pipeline.writers.LoggerWriter","title":"<code>LoggerWriter</code>","text":"<p>             Bases: <code>Writer</code></p> <p>A <code>Writer</code> that logs the record to a logger.</p> Source code in <code>nodestream/pipeline/writers.py</code> Python<pre><code>class LoggerWriter(Writer):\n\"\"\"A `Writer` that logs the record to a logger.\"\"\"\n\n    def __init__(self, logger_name=None, level=INFO) -&gt; None:\n        logger_name = logger_name or self.__class__.__name__\n        self.logger = getLogger(logger_name)\n        # At first glance this line would appear wrong. `getLevelName` does the\n        # mapping bi-directionally. If you give it a number (level), you get the name\n        # and vice-versa. Since we are standardizing on the int representation of\n        # the level (thats what Logger#log requires), then we need to\n        # call `getLevelName` when the value is the string name for the log level.\n        #\n        # see: https://docs.python.org/3/library/logging.html#logging.getLevelName\n        self.level = getLevelName(level) if isinstance(level, str) else level\n\n    async def write_record(self, record):\n        self.logger.log(self.level, record)\n</code></pre>"},{"location":"python_reference/pipeline/writers/#nodestream.pipeline.writers.Writer","title":"<code>Writer</code>","text":"<p>             Bases: <code>Step</code></p> <p>A <code>Writer</code> takes a given record and commits it to a downstream data store.</p> <p><code>Writer</code> steps generally make up the end of an ETL pipeline and are responsible for ensuring that the newly transformed data is persisted. After writing the record, the record is passed downstream.</p> Source code in <code>nodestream/pipeline/writers.py</code> Python<pre><code>class Writer(Step):\n\"\"\"A `Writer` takes a given record and commits it to a downstream data store.\n\n    `Writer` steps generally make up the end of an ETL pipeline and are responsible\n    for ensuring that the newly transformed data is persisted. After writing the record,\n    the record is passed downstream.\n    \"\"\"\n\n    async def handle_async_record_stream(\n        self, record_stream: AsyncGenerator[Any, Any]\n    ) -&gt; AsyncGenerator[Any, Any]:\n        async for record in record_stream:\n            await self.write_record(record)\n            yield record\n\n    @abstractmethod\n    async def write_record(self, record: Any):\n        raise NotImplementedError\n</code></pre>"},{"location":"python_reference/pipeline/argument_resolvers/argument_resolver/","title":"Argument resolver","text":""},{"location":"python_reference/pipeline/argument_resolvers/argument_resolver/#nodestream.pipeline.argument_resolvers.argument_resolver.ArgumentResolver","title":"<code>ArgumentResolver</code>","text":"<p>             Bases: <code>Pluggable</code>, <code>ABC</code></p> <p>An <code>ArgumentResolver</code> is a class that can resolve a value by injecting into the yaml parser.</p> Source code in <code>nodestream/pipeline/argument_resolvers/argument_resolver.py</code> Python<pre><code>@ARGUMENT_RESOLVER_REGISTRY.connect_baseclass\nclass ArgumentResolver(Pluggable, ABC):\n\"\"\"An `ArgumentResolver` is a class that can resolve a value by injecting into the yaml parser.\"\"\"\n\n    entrypoint_name = \"argument_resolvers\"\n\n    @classmethod\n    def install_yaml_tag(cls, loader: Type[SafeLoader]):\n        pass\n</code></pre>"},{"location":"python_reference/pipeline/argument_resolvers/environment_variable_resolver/","title":"Environment variable resolver","text":""},{"location":"python_reference/pipeline/argument_resolvers/environment_variable_resolver/#nodestream.pipeline.argument_resolvers.environment_variable_resolver.EnvironmentResolver","title":"<code>EnvironmentResolver</code>","text":"<p>             Bases: <code>ArgumentResolver</code></p> <p>An <code>EnvironmentResolver</code> is an <code>ArgumentResolver</code> that can resolve an environment variable into its value.</p> Source code in <code>nodestream/pipeline/argument_resolvers/environment_variable_resolver.py</code> Python<pre><code>class EnvironmentResolver(ArgumentResolver):\n\"\"\"An `EnvironmentResolver` is an `ArgumentResolver` that can resolve an environment variable into its value.\"\"\"\n\n    @classmethod\n    def install_yaml_tag(cls, loader: Type[SafeLoader]):\n        loader.add_constructor(\n            \"!env\",\n            lambda loader, node: cls.resolve_argument(loader.construct_scalar(node)),\n        )\n\n    @staticmethod\n    def resolve_argument(variable_name):\n        return os.environ.get(variable_name)\n</code></pre>"},{"location":"python_reference/pipeline/argument_resolvers/include_file_resolver/","title":"Include file resolver","text":""},{"location":"python_reference/pipeline/argument_resolvers/include_file_resolver/#nodestream.pipeline.argument_resolvers.include_file_resolver.IncludeFileResolver","title":"<code>IncludeFileResolver</code>","text":"<p>             Bases: <code>ArgumentResolver</code></p> <p>An <code>IncludeFileResolver</code> is an <code>ArgumentResolver</code> that can resolve a file path into a file's contents.</p> Source code in <code>nodestream/pipeline/argument_resolvers/include_file_resolver.py</code> Python<pre><code>class IncludeFileResolver(ArgumentResolver):\n\"\"\"An `IncludeFileResolver` is an `ArgumentResolver` that can resolve a file path into a file's contents.\"\"\"\n\n    @classmethod\n    def install_yaml_tag(cls, loader: Type[SafeLoader]):\n        loader.add_constructor(\n            \"!include\",\n            lambda loader, node: cls.include_file(loader.construct_scalar(node)),\n        )\n\n    @staticmethod\n    def include_file(file_path: str):\n        from ..pipeline_file_loader import PipelineFileSafeLoader\n\n        return PipelineFileSafeLoader.load_file_by_path(file_path)\n</code></pre>"},{"location":"python_reference/pipeline/extractors/apis/","title":"Apis","text":""},{"location":"python_reference/pipeline/extractors/extractor/","title":"Extractor","text":""},{"location":"python_reference/pipeline/extractors/extractor/#nodestream.pipeline.extractors.extractor.Extractor","title":"<code>Extractor</code>","text":"<p>             Bases: <code>Step</code></p> <p>Extractors represent the source of a set of records.</p> <p>They are like any other step. However, they ignore the incoming record stream and instead produce their own stream of records. For this reason they generally should only be set at the beginning of a pipeline.</p> Source code in <code>nodestream/pipeline/extractors/extractor.py</code> Python<pre><code>class Extractor(Step):\n\"\"\"Extractors represent the source of a set of records.\n\n    They are like any other step. However, they ignore the incoming record stream and instead produce their own\n    stream of records. For this reason they generally should only be set at the beginning of a pipeline.\n    \"\"\"\n\n    def handle_async_record_stream(\n        self, _: AsyncGenerator[Any, Any]\n    ) -&gt; AsyncGenerator[Any, Any]:\n        return self.extract_records()\n\n    @abstractmethod\n    async def extract_records(self) -&gt; AsyncGenerator[Any, Any]:\n        raise NotImplementedError\n</code></pre>"},{"location":"python_reference/pipeline/extractors/files/","title":"Files","text":""},{"location":"python_reference/pipeline/extractors/iterable/","title":"Iterable","text":""},{"location":"python_reference/pipeline/extractors/iterable/#nodestream.pipeline.extractors.iterable.IterableExtractor","title":"<code>IterableExtractor</code>","text":"<p>             Bases: <code>Extractor</code></p> <p>An extractor that produces records from an iterable.</p> Source code in <code>nodestream/pipeline/extractors/iterable.py</code> Python<pre><code>class IterableExtractor(Extractor):\n\"\"\"An extractor that produces records from an iterable.\"\"\"\n\n    @classmethod\n    def range(cls, start=0, stop=100, step=1):\n        return cls(iterable=({\"index\": i} for i in range(start, stop, step)))\n\n    def __init__(self, iterable: Iterable[Any]) -&gt; None:\n        self.iterable = iterable\n\n    async def extract_records(self) -&gt; AsyncGenerator[Any, Any]:\n        for record in self.iterable:\n            yield record\n</code></pre>"},{"location":"python_reference/pipeline/extractors/ttls/","title":"Ttls","text":""},{"location":"python_reference/pipeline/extractors/stores/aws/athena_extractor/","title":"Athena extractor","text":""},{"location":"python_reference/pipeline/extractors/stores/aws/credential_utils/","title":"Credential utils","text":""},{"location":"python_reference/pipeline/extractors/stores/aws/s3_extractor/","title":"S3 extractor","text":""},{"location":"python_reference/pipeline/extractors/streams/extractor/","title":"Extractor","text":""},{"location":"python_reference/pipeline/extractors/streams/extractor/#nodestream.pipeline.extractors.streams.extractor.StreamExtractor","title":"<code>StreamExtractor</code>","text":"<p>             Bases: <code>Extractor</code></p> <p>A StreamExtractor implements the standard behavior of polling data from a stream.</p> <p>The StreamExtractor requires both a StreamConnector and a StreamRecordFormat to delegate to for the actual polling implementation and parsing of the data, respectively.</p> Source code in <code>nodestream/pipeline/extractors/streams/extractor.py</code> Python<pre><code>class StreamExtractor(Extractor):\n\"\"\"A StreamExtractor implements the standard behavior of polling data from a stream.\n\n    The StreamExtractor requires both a StreamConnector and a StreamRecordFormat to delegate\n    to for the actual polling implementation and parsing of the data, respectively.\n    \"\"\"\n\n    @classmethod\n    def from_file_data(\n        cls,\n        connector: str,\n        record_format: str,\n        timeout: int = DEFAULT_TIMEOUT,\n        max_records: int = DEFAULT_MAX_RECORDS,\n        **connector_args\n    ):\n        # Import all plugins so that they can register themselves\n        StreamRecordFormat.import_all()\n        StreamConnector.import_all()\n\n        object_format_cls = STREAM_OBJECT_FORMAT_SUBCLASS_REGISTRY.get(record_format)\n        connector_cls = STREAM_CONNECTOR_SUBCLASS_REGISTRY.get(connector)\n        return cls(\n            timeout=timeout,\n            max_records=max_records,\n            record_format=object_format_cls(),\n            connector=connector_cls(**connector_args),\n        )\n\n    def __init__(\n        self,\n        connector: StreamConnector,\n        record_format: StreamRecordFormat,\n        timeout: int,\n        max_records: int,\n    ):\n        self.connector = connector\n        self.record_format = record_format\n        self.timeout = timeout\n        self.max_records = max_records\n\n    async def poll(self):\n        return await self.connector.poll(\n            timeout=self.timeout, max_records=self.max_records\n        )\n\n    async def extract_records(self):\n        await self.connector.connect()\n        try:\n            results = tuple(await self.poll())\n            if len(results) == 0:\n                yield Flush\n            else:\n                for record in results:\n                    yield self.record_format.parse(record)\n        finally:\n            await self.connector.disconnect()\n</code></pre>"},{"location":"python_reference/pipeline/extractors/streams/kafka/","title":"Kafka","text":""},{"location":"python_reference/pipeline/extractors/streams/kafka/#nodestream.pipeline.extractors.streams.kafka.KafkaStreamConnector","title":"<code>KafkaStreamConnector</code>","text":"<p>             Bases: <code>StreamConnector</code></p> <p>A KafkaStreamConnector implements the StreamConnector interface for Kafka.</p> <p>The KafkaStreamConnector uses the aiokafka library to connect to a Kafka cluster and poll data from it.</p> Source code in <code>nodestream/pipeline/extractors/streams/kafka.py</code> Python<pre><code>class KafkaStreamConnector(StreamConnector, alias=\"kafka\"):\n\"\"\"A KafkaStreamConnector implements the StreamConnector interface for Kafka.\n\n    The KafkaStreamConnector uses the aiokafka library to connect to a\n    Kafka cluster and poll data from it.\n    \"\"\"\n\n    def __init__(\n        self, bootstrap_servers: List[str], topic: str, group_id: Optional[str] = None\n    ):\n        self.bootstrap_servers = \",\".join(bootstrap_servers)\n        self.topic = topic\n        self.group_id = group_id or DEFAULT_GROUP_ID\n        self.consumer = None\n        self.logger = getLogger(__name__)\n\n    async def connect(self):\n        self.consumer = AIOKafkaConsumer(\n            self.topic,\n            bootstrap_servers=self.bootstrap_servers,\n            group_id=self.group_id,\n        )\n        await self.consumer.start()\n\n    async def disconnect(self):\n        await self.consumer.stop()\n\n    async def poll(self, timeout: int, max_records: int) -&gt; Iterable[Any]:\n        result = await self.consumer.getmany(\n            max_records=max_records, timeout_ms=timeout * 1000\n        )\n        for tp, messages in result.items():\n            self.logger.debug(\n                \"Recived Kafka Messages\",\n                extra={\"topic\": tp.topic, \"partition\": tp.partition},\n            )\n            for message in messages:\n                yield message.value\n</code></pre>"},{"location":"python_reference/pipeline/normalizers/lowercase_strings/","title":"Lowercase strings","text":""},{"location":"python_reference/pipeline/normalizers/normalizer/","title":"Normalizer","text":""},{"location":"python_reference/pipeline/normalizers/normalizer/#nodestream.pipeline.normalizers.normalizer.InvalidFlagError","title":"<code>InvalidFlagError</code>","text":"<p>             Bases: <code>ValueError</code></p> <p>Raised when a normalization flag is not valid.</p> Source code in <code>nodestream/pipeline/normalizers/normalizer.py</code> Python<pre><code>class InvalidFlagError(ValueError):\n\"\"\"Raised when a normalization flag is not valid.\"\"\"\n\n    def __init__(self, flag_name, *args: object) -&gt; None:\n        super().__init__(\n            f\"Normalization flag with name '{flag_name}' is not valid.`\", *args\n        )\n</code></pre>"},{"location":"python_reference/pipeline/normalizers/normalizer/#nodestream.pipeline.normalizers.normalizer.Normalizer","title":"<code>Normalizer</code>","text":"<p>             Bases: <code>Pluggable</code>, <code>ABC</code></p> <p>A <code>Normalizer</code> is responsible for turning objects into a consistent form.</p> <p>When data is extracted from pipeline records from a value provider, the <code>Normalizer</code> is responsible for \"cleaning up\" the raw data such that is consistent. Often this comes in with regard to strings.</p> Source code in <code>nodestream/pipeline/normalizers/normalizer.py</code> Python<pre><code>@NORMALIZER_REGISTRY.connect_baseclass\nclass Normalizer(Pluggable, ABC):\n\"\"\"A `Normalizer` is responsible for turning objects into a consistent form.\n\n    When data is extracted from pipeline records from a value provider, the `Normalizer`\n    is responsible for \"cleaning up\" the raw data such that is consistent. Often this comes\n    in with regard to strings.\n    \"\"\"\n\n    entrypoint_name = \"normalizers\"\n\n    @classmethod\n    def setup(cls):\n        pass\n\n    @classmethod\n    def normalize_by_args(cls, value: Any, **normalizer_args) -&gt; Any:\n        for flag_name, enabled in normalizer_args.items():\n            if enabled:\n                value = cls.by_flag_name(flag_name).normalize_value(value)\n\n        return value\n\n    @classmethod\n    def argument_flag(cls):\n        return f\"do_{NORMALIZER_REGISTRY.name_for(cls)}\"\n\n    @classmethod\n    @cache\n    def by_flag_name(cls, flag_name: str) -&gt; \"Normalizer\":\n        if not flag_name.startswith(\"do_\"):\n            raise InvalidFlagError(flag_name)\n\n        try:\n            normalizer_class = NORMALIZER_REGISTRY.get(flag_name.strip(\"do_\"))\n        except MissingFromRegistryError:\n            raise InvalidFlagError(flag_name)\n\n        return normalizer_class()\n\n    @abstractmethod\n    def normalize_value(self, value: Any) -&gt; Any:\n        raise NotImplementedError\n</code></pre>"},{"location":"python_reference/pipeline/normalizers/remove_trailing_dots/","title":"Remove trailing dots","text":""},{"location":"python_reference/pipeline/normalizers/trim_whitespace/","title":"Trim whitespace","text":""},{"location":"python_reference/pipeline/transformers/expand_json_field/","title":"Expand json field","text":""},{"location":"python_reference/pipeline/transformers/transformer/","title":"Transformer","text":""},{"location":"python_reference/pipeline/transformers/transformer/#nodestream.pipeline.transformers.transformer.Transformer","title":"<code>Transformer</code>","text":"<p>             Bases: <code>Step</code></p> <p>A <code>Transformer</code> takes a given record and mutates into a new record.</p> <p><code>Transformer</code> steps generally make up the middle of an ETL pipeline and are responsible for reshaping an object so its more ingestible by the downstream sink.</p> Source code in <code>nodestream/pipeline/transformers/transformer.py</code> Python<pre><code>class Transformer(Step):\n\"\"\"A `Transformer` takes a given record and mutates into a new record.\n\n    `Transformer` steps generally make up the middle of an ETL pipeline and are responsible\n    for reshaping an object so its more ingestible by the downstream sink.\n    \"\"\"\n\n    async def handle_async_record_stream(\n        self, record_stream: AsyncGenerator[Any, Any]\n    ) -&gt; AsyncGenerator[Any, Any]:\n        async for record in record_stream:\n            if record is Flush:\n                yield record\n            else:\n                val_or_gen = self.transform_record(record)\n                if isinstance(val_or_gen, AsyncGenerator):\n                    async for result in val_or_gen:\n                        yield result\n                else:\n                    yield await val_or_gen\n\n    @abstractmethod\n    async def transform_record(self, record: Any) -&gt; Any:\n        raise NotImplementedError\n</code></pre>"},{"location":"python_reference/pipeline/transformers/value_projection/","title":"Value projection","text":""},{"location":"python_reference/pipeline/value_providers/context/","title":"Context","text":""},{"location":"python_reference/pipeline/value_providers/context/#nodestream.pipeline.value_providers.context.ProviderContext","title":"<code>ProviderContext</code>  <code>dataclass</code>","text":"<p>Defines the state of the Interpretation at a given point of time.</p> <p>As a record from the pipeline traverses the series of components responsible for extracting data from it and converting it into a SubGraph, data must be viewed within the context of that pipeline. Variables and data mappings are stored that are later referenced by disparate components, each of which, coordinate through an <code>ProviderContext</code>.</p> <p>This data does live longer then the record's time spent in the pipeline and is discarded excepted for the stored <code>DesiredIngestion</code>. The <code>DesiredIngestion</code> represents the outcome of the work spent on the record as it passes through the pipeline.</p> Source code in <code>nodestream/pipeline/value_providers/context.py</code> Python<pre><code>@dataclass(slots=True)\nclass ProviderContext:\n\"\"\"Defines the state of the Interpretation at a given point of time.\n\n    As a record from the pipeline traverses the series of components responsible for extracting data from it and\n    converting it into a SubGraph, data must be viewed within the context of that pipeline. Variables and data mappings\n    are stored that are later referenced by disparate components, each of which, coordinate through an `ProviderContext`.\n\n    This data does live longer then the record's time spent in the pipeline and is discarded excepted for the stored `DesiredIngestion`.\n    The `DesiredIngestion` represents the outcome of the work spent on the record as it passes through the pipeline.\n    \"\"\"\n\n    document: JsonLikeDocument\n    desired_ingest: DesiredIngestion\n    mappings: Dict[Any, Any] = field(default_factory=dict)\n    variables: PropertySet = field(default_factory=PropertySet.empty)\n\n    @classmethod\n    def fresh(cls, record):\n        return cls(record, DesiredIngestion())\n</code></pre>"},{"location":"python_reference/pipeline/value_providers/jmespath_value_provider/","title":"Jmespath value provider","text":""},{"location":"python_reference/pipeline/value_providers/jmespath_value_provider/#nodestream.pipeline.value_providers.jmespath_value_provider.JmespathValueProvider","title":"<code>JmespathValueProvider</code>","text":"<p>             Bases: <code>ValueProvider</code></p> <p>A <code>ValueProvider</code> that uses JMESPath to extract values from a document.</p> Source code in <code>nodestream/pipeline/value_providers/jmespath_value_provider.py</code> Python<pre><code>class JmespathValueProvider(ValueProvider):\n\"\"\"A `ValueProvider` that uses JMESPath to extract values from a document.\"\"\"\n\n    __slots__ = (\"compiled_query\",)\n\n    @classmethod\n    def install_yaml_tag(cls, loader: Type[SafeLoader]):\n        loader.add_constructor(\n            \"!jmespath\",\n            lambda loader, node: cls.from_string_expression(\n                loader.construct_scalar(node)\n            ),\n        )\n\n    @classmethod\n    def from_string_expression(cls, expression: str):\n        return cls(jmespath.compile(expression))\n\n    def __init__(self, compiled_query: ParsedResult) -&gt; None:\n        self.compiled_query = compiled_query\n\n    def search(self, context: ProviderContext):\n        raw_search = self.compiled_query.search(context.document)\n        if raw_search is None:\n            return\n        if isinstance(raw_search, list):\n            yield from raw_search\n        else:\n            yield raw_search\n\n    def single_value(self, context: ProviderContext) -&gt; Any:\n        return next(self.search(context), None)\n\n    def many_values(self, context: ProviderContext) -&gt; Iterable[Any]:\n        return self.search(context)\n</code></pre>"},{"location":"python_reference/pipeline/value_providers/jq_value_provder/","title":"Jq value provder","text":""},{"location":"python_reference/pipeline/value_providers/jq_value_provder/#nodestream.pipeline.value_providers.jq_value_provder.JqValueProvider","title":"<code>JqValueProvider</code>","text":"<p>             Bases: <code>ValueProvider</code></p> <p>A <code>ValueProvider</code> that uses Jq to extract values from a document.</p> Source code in <code>nodestream/pipeline/value_providers/jq_value_provder.py</code> Python<pre><code>class JqValueProvider(ValueProvider):\n\"\"\"A `ValueProvider` that uses Jq to extract values from a document.\"\"\"\n\n    __slots__ = (\"jq_program\",)\n\n    @classmethod\n    def install_yaml_tag(cls, loader: Type[SafeLoader]):\n        loader.add_constructor(\n            \"!jq\", lambda loader, node: cls(jq.compile(loader.construct_scalar(node)))\n        )\n\n    def __init__(self, jq_program) -&gt; None:\n        self.jq_program = jq_program\n\n    def search(self, context: ProviderContext):\n        raw_search = self.jq_program.input(context.document).all()\n        for hit in raw_search:\n            if hit is None:\n                return\n            if isinstance(hit, list):\n                yield from hit\n            else:\n                yield hit\n\n    def single_value(self, context: ProviderContext) -&gt; Any:\n        return next(self.search(context), None)\n\n    def many_values(self, context: ProviderContext) -&gt; Iterable[Any]:\n        return self.search(context)\n</code></pre>"},{"location":"python_reference/pipeline/value_providers/mapping_value_provider/","title":"Mapping value provider","text":""},{"location":"python_reference/pipeline/value_providers/mapping_value_provider/#nodestream.pipeline.value_providers.mapping_value_provider.MappingValueProvider","title":"<code>MappingValueProvider</code>","text":"<p>             Bases: <code>ValueProvider</code></p> <p>A <code>ValueProvider</code> that uses a mapping to extract values from a document.</p> Source code in <code>nodestream/pipeline/value_providers/mapping_value_provider.py</code> Python<pre><code>class MappingValueProvider(ValueProvider):\n\"\"\"A `ValueProvider` that uses a mapping to extract values from a document.\"\"\"\n\n    __slots__ = (\"mapping_name\", \"key\")\n\n    @classmethod\n    def install_yaml_tag(cls, loader: Type[SafeLoader]):\n        loader.add_constructor(\n            \"!mapping\",\n            lambda loader, node: MappingValueProvider(**loader.construct_mapping(node)),\n        )\n\n    def __init__(self, mapping_name: str, key: StaticValueOrValueProvider) -&gt; None:\n        self.mapping_name = mapping_name\n        self.key = ValueProvider.guarantee_value_provider(key)\n\n    def single_value(self, context: ProviderContext) -&gt; Any:\n        mapping = context.mappings.get(self.mapping_name)\n        if not mapping:\n            return\n\n        key = self.key.single_value(context)\n        if not key:\n            return\n\n        value = ValueProvider.guarantee_value_provider(mapping.get(key))\n        return value.single_value(context)\n\n    def many_values(self, context: ProviderContext) -&gt; Iterable[Any]:\n        value = self.single_value(context)\n        return [value] if value else []\n</code></pre>"},{"location":"python_reference/pipeline/value_providers/regex_value_provider/","title":"Regex value provider","text":""},{"location":"python_reference/pipeline/value_providers/static_value_provider/","title":"Static value provider","text":""},{"location":"python_reference/pipeline/value_providers/static_value_provider/#nodestream.pipeline.value_providers.static_value_provider.StaticValueProvider","title":"<code>StaticValueProvider</code>","text":"<p>             Bases: <code>ValueProvider</code></p> <p>A <code>ValueProvider</code> that always returns the same value.</p> Source code in <code>nodestream/pipeline/value_providers/static_value_provider.py</code> Python<pre><code>class StaticValueProvider(ValueProvider):\n\"\"\"A `ValueProvider` that always returns the same value.\"\"\"\n\n    __slots__ = (\"value\",)\n\n    def __init__(self, value) -&gt; None:\n        self.value = value\n\n    def single_value(self, _: ProviderContext) -&gt; Any:\n        return self.value\n\n    def many_values(self, _: ProviderContext) -&gt; Iterable[Any]:\n        return self.value if isinstance(self.value, Iterable) else [self.value]\n\n    @property\n    def is_static(self) -&gt; bool:\n        return True\n</code></pre>"},{"location":"python_reference/pipeline/value_providers/string_format_value_provider/","title":"String format value provider","text":""},{"location":"python_reference/pipeline/value_providers/string_format_value_provider/#nodestream.pipeline.value_providers.string_format_value_provider.StringFormattingValueProvider","title":"<code>StringFormattingValueProvider</code>","text":"<p>             Bases: <code>ValueProvider</code></p> <p>A <code>ValueProvider</code> that uses string formatting to produce a value.</p> Source code in <code>nodestream/pipeline/value_providers/string_format_value_provider.py</code> Python<pre><code>class StringFormattingValueProvider(ValueProvider):\n\"\"\"A `ValueProvider` that uses string formatting to produce a value.\"\"\"\n\n    __slots__ = (\"fmt\", \"subs\")\n\n    @classmethod\n    def install_yaml_tag(cls, loader: Type[SafeLoader]):\n        loader.add_constructor(\n            \"!format\", lambda loader, node: cls(**loader.construct_mapping(node))\n        )\n\n    def __init__(\n        self,\n        fmt: StaticValueOrValueProvider,\n        **subs: Dict[str, StaticValueOrValueProvider],\n    ) -&gt; None:\n        self.fmt = ValueProvider.guarantee_value_provider(fmt)\n        self.subs = ValueProvider.guarantee_provider_dictionary(subs)\n\n    def single_value(self, context: ProviderContext) -&gt; Any:\n        if (fmt := self.fmt.single_value(context)) is None:\n            return None\n\n        subs = {\n            field: provider.single_value(context)\n            for field, provider in self.subs.items()\n        }\n\n        return fmt.format(**subs)\n\n    def many_values(self, context: ProviderContext) -&gt; Iterable[Any]:\n        value = self.single_value(context)\n        return [value] if value else []\n</code></pre>"},{"location":"python_reference/pipeline/value_providers/value_provider/","title":"Value provider","text":""},{"location":"python_reference/pipeline/value_providers/value_provider/#nodestream.pipeline.value_providers.value_provider.ValueProvider","title":"<code>ValueProvider</code>","text":"<p>             Bases: <code>Pluggable</code>, <code>ABC</code></p> <p>A <code>ValueProvider</code> is a class that can extract values from a document.</p> Source code in <code>nodestream/pipeline/value_providers/value_provider.py</code> Python<pre><code>@VALUE_PROVIDER_REGISTRY.connect_baseclass\nclass ValueProvider(Pluggable, ABC):\n\"\"\"A `ValueProvider` is a class that can extract values from a document.\"\"\"\n\n    entrypoint_name = \"value_providers\"\n\n    @classmethod\n    def guarantee_value_provider(\n        cls, maybe_provider: StaticValueOrValueProvider\n    ) -&gt; \"ValueProvider\":\n        from .static_value_provider import StaticValueProvider\n\n        return (\n            maybe_provider\n            if isinstance(maybe_provider, ValueProvider)\n            else StaticValueProvider(maybe_provider)\n        )\n\n    @classmethod\n    def guarantee_provider_dictionary(\n        cls, maybe_providers: Dict[Any, StaticValueOrValueProvider]\n    ):\n        return {k: cls.guarantee_value_provider(v) for k, v in maybe_providers.items()}\n\n    @classmethod\n    def guarantee_provider_list(\n        cls, maybe_providers: Iterable[StaticValueOrValueProvider]\n    ):\n        return [cls.guarantee_value_provider(v) for v in maybe_providers]\n\n    @classmethod\n    def install_yaml_tag(cls, loader: Type[SafeLoader]):\n        pass\n\n    @abstractmethod\n    def single_value(self, context: ProviderContext) -&gt; Any:\n        raise NotImplementedError\n\n    @abstractmethod\n    def many_values(self, context: ProviderContext) -&gt; Iterable[Any]:\n        raise NotImplementedError\n\n    def normalize(self, value, **args):\n        return Normalizer.normalize_by_args(value, **args)\n\n    def normalize_single_value(\n        self, context: ProviderContext, **normalization_args\n    ) -&gt; Any:\n        return self.normalize(self.single_value(context), **normalization_args)\n\n    def normalize_many_values(\n        self, context: ProviderContext, **normalization_args\n    ) -&gt; Iterable[Any]:\n        for value in self.many_values(context):\n            yield self.normalize(value, **normalization_args)\n\n    @property\n    def is_static(self) -&gt; bool:\n        return False\n</code></pre>"},{"location":"python_reference/pipeline/value_providers/variable_value_provider/","title":"Variable value provider","text":""},{"location":"python_reference/pipeline/value_providers/variable_value_provider/#nodestream.pipeline.value_providers.variable_value_provider.VariableValueProvider","title":"<code>VariableValueProvider</code>","text":"<p>             Bases: <code>ValueProvider</code></p> <p>A <code>ValueProvider</code> that uses a variable to extract values from a document.</p> Source code in <code>nodestream/pipeline/value_providers/variable_value_provider.py</code> Python<pre><code>class VariableValueProvider(ValueProvider):\n\"\"\"A `ValueProvider` that uses a variable to extract values from a document.\"\"\"\n\n    __slots__ = (\"variable_name\",)\n\n    @classmethod\n    def install_yaml_tag(cls, loader: Type[SafeLoader]):\n        loader.add_constructor(\n            \"!variable\",\n            lambda loader, node: cls(loader.construct_scalar(node)),\n        )\n\n    def __init__(self, variable_name: str) -&gt; None:\n        self.variable_name = variable_name\n\n    def single_value(self, context: ProviderContext) -&gt; Any:\n        return context.variables.get(self.variable_name)\n\n    def many_values(self, context: ProviderContext) -&gt; Iterable[Any]:\n        value = self.single_value(context)\n        if value is None:\n            return []\n        return value if isinstance(value, list) else [value]\n</code></pre>"},{"location":"python_reference/project/pipeline_definition/","title":"Pipeline definition","text":""},{"location":"python_reference/project/pipeline_definition/#nodestream.project.pipeline_definition.PipelineDefinition","title":"<code>PipelineDefinition</code>  <code>dataclass</code>","text":"<p>             Bases: <code>IntrospectiveIngestionComponent</code>, <code>SavesToYaml</code>, <code>LoadsFromYaml</code></p> <p>A <code>PipelineDefinition</code> represents a pipeline that can be loaded from a file.</p> <p><code>PipelineDefinition</code> objects are used to load pipelines from files. They themselves are not pipelines, but rather represent the metadata needed to load a pipeline from a file.</p> <p><code>PipelineDefinition</code> objects are also <code>IntrospectiveIngestionComponent</code> objects, meaning that they can be introspected to determine their known schema definitions.</p> <p><code>PipelineDefinition</code> objects are also <code>SavesToYaml</code> and <code>LoadsFromYaml</code> objects, meaning that they can be serialized to and deserialized from YAML data.</p> Source code in <code>nodestream/project/pipeline_definition.py</code> Python<pre><code>@dataclass\nclass PipelineDefinition(IntrospectiveIngestionComponent, SavesToYaml, LoadsFromYaml):\n\"\"\"A `PipelineDefinition` represents a pipeline that can be loaded from a file.\n\n    `PipelineDefinition` objects are used to load pipelines from files. They themselves\n    are not pipelines, but rather represent the metadata needed to load a pipeline from\n    a file.\n\n    `PipelineDefinition` objects are also `IntrospectiveIngestionComponent` objects,\n    meaning that they can be introspected to determine their known schema definitions.\n\n    `PipelineDefinition` objects are also `SavesToYaml` and `LoadsFromYaml` objects,\n    meaning that they can be serialized to and deserialized from YAML data.\n    \"\"\"\n\n    name: str\n    file_path: Path\n    annotations: Dict[str, Any] = field(default_factory=dict)\n\n    @classmethod\n    def from_path(cls, file_path: Path):\n\"\"\"Create a `PipelineDefinition` from a file path.\n\n        The name of the pipeline will be the stem of the file path, and the annotations\n        will be empty. The pipeline will be loaded from the provided file path.\n\n        Args:\n            file_path (Path): The path to the file from which to load the pipeline.\n\n        Returns:\n            PipelineDefinition: The `PipelineDefinition` object.\n        \"\"\"\n        return cls(get_default_name(file_path), file_path)\n\n    @classmethod\n    def describe_yaml_schema(cls):\n        from schema import Optional, Or, Schema\n\n        return Schema(\n            Or(\n                str,\n                {\n                    Optional(\"name\"): str,\n                    \"path\": os.path.exists,\n                    Optional(\"annotations\"): {str: Or(str, int, float, bool)},\n                },\n            )\n        )\n\n    @classmethod\n    def from_file_data(cls, data, parent_annotations):\n        if isinstance(data, str):\n            data = {\"path\": data}\n\n        file_path = Path(data.pop(\"path\"))\n        name = data.pop(\"name\", get_default_name(file_path))\n        annotations = data.pop(\"annotations\", {})\n        return cls(name, file_path, {**parent_annotations, **annotations})\n\n    def to_file_data(self, verbose: bool = False):\n        using_default_name = self.name == self.file_path.stem\n        if using_default_name and not self.annotations and not verbose:\n            return str(self.file_path)\n\n        result = {\"path\": str(self.file_path)}\n        if not using_default_name or verbose:\n            result[\"name\"] = self.name\n        if self.annotations or verbose:\n            result[\"annotations\"] = self.annotations\n\n        return result\n\n    def initialize(self, init_args: PipelineInitializationArguments) -&gt; Pipeline:\n        return PipelineFileLoader(self.file_path).load_pipeline(init_args)\n\n    def remove_file(self, missing_ok: bool = True):\n\"\"\"Remove the file associated with this `PipelineDefinition`.\n\n        Args:\n            missing_ok (bool, optional): Whether to ignore missing files. Defaults to True.\n\n        Raises:\n            FileNotFoundError: If the file does not exist and `missing_ok` is False.\n        \"\"\"\n        self.file_path.unlink(missing_ok=missing_ok)\n\n    def initialize_for_introspection(self) -&gt; Pipeline:\n        return self.initialize(PipelineInitializationArguments.for_introspection())\n\n    def gather_object_shapes(self):\n        return self.initialize_for_introspection().gather_object_shapes()\n\n    def gather_present_relationships(self):\n        return self.initialize_for_introspection().gather_present_relationships()\n\n    def gather_used_indexes(self):\n        return self.initialize_for_introspection().gather_used_indexes()\n</code></pre>"},{"location":"python_reference/project/pipeline_definition/#nodestream.project.pipeline_definition.PipelineDefinition.from_path","title":"<code>from_path(file_path)</code>  <code>classmethod</code>","text":"<p>Create a <code>PipelineDefinition</code> from a file path.</p> <p>The name of the pipeline will be the stem of the file path, and the annotations will be empty. The pipeline will be loaded from the provided file path.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The path to the file from which to load the pipeline.</p> required <p>Returns:</p> Name Type Description <code>PipelineDefinition</code> <p>The <code>PipelineDefinition</code> object.</p> Source code in <code>nodestream/project/pipeline_definition.py</code> Python<pre><code>@classmethod\ndef from_path(cls, file_path: Path):\n\"\"\"Create a `PipelineDefinition` from a file path.\n\n    The name of the pipeline will be the stem of the file path, and the annotations\n    will be empty. The pipeline will be loaded from the provided file path.\n\n    Args:\n        file_path (Path): The path to the file from which to load the pipeline.\n\n    Returns:\n        PipelineDefinition: The `PipelineDefinition` object.\n    \"\"\"\n    return cls(get_default_name(file_path), file_path)\n</code></pre>"},{"location":"python_reference/project/pipeline_definition/#nodestream.project.pipeline_definition.PipelineDefinition.remove_file","title":"<code>remove_file(missing_ok=True)</code>","text":"<p>Remove the file associated with this <code>PipelineDefinition</code>.</p> <p>Parameters:</p> Name Type Description Default <code>missing_ok</code> <code>bool</code> <p>Whether to ignore missing files. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist and <code>missing_ok</code> is False.</p> Source code in <code>nodestream/project/pipeline_definition.py</code> Python<pre><code>def remove_file(self, missing_ok: bool = True):\n\"\"\"Remove the file associated with this `PipelineDefinition`.\n\n    Args:\n        missing_ok (bool, optional): Whether to ignore missing files. Defaults to True.\n\n    Raises:\n        FileNotFoundError: If the file does not exist and `missing_ok` is False.\n    \"\"\"\n    self.file_path.unlink(missing_ok=missing_ok)\n</code></pre>"},{"location":"python_reference/project/pipeline_scope/","title":"Pipeline scope","text":""},{"location":"python_reference/project/pipeline_scope/#nodestream.project.pipeline_scope.PipelineScope","title":"<code>PipelineScope</code>","text":"<p>             Bases: <code>AggregatedIntrospectiveIngestionComponent</code>, <code>LoadsFromYaml</code>, <code>SavesToYaml</code></p> <p>A <code>PipelineScope</code> represents a collection of pipelines subordinate to a project.</p> Source code in <code>nodestream/project/pipeline_scope.py</code> Python<pre><code>class PipelineScope(\n    AggregatedIntrospectiveIngestionComponent, LoadsFromYaml, SavesToYaml\n):\n\"\"\"A `PipelineScope` represents a collection of pipelines subordinate to a project.\"\"\"\n\n    def __init__(\n        self, name: str, pipelines: List[PipelineDefinition], persist: bool = True\n    ) -&gt; None:\n        self.persist = persist\n        self.name = name\n        self.pipelines_by_name: Dict[str, PipelineDefinition] = {}\n        for pipeline in pipelines:\n            self.add_pipeline_definition(pipeline)\n\n    @classmethod\n    def from_file_data(cls, scope_name, file_data):\n        pipelines_data = file_data.pop(\"pipelines\", [])\n        annotations = file_data.pop(\"annotations\", {})\n        pipelines = [\n            PipelineDefinition.from_file_data(pipeline_data, annotations)\n            for pipeline_data in pipelines_data\n        ]\n        return cls(scope_name, pipelines)\n\n    @classmethod\n    def describe_yaml_schema(cls):\n        from schema import Optional, Or, Schema\n\n        return Schema(\n            {\n                Optional(\"pipelines\"): [\n                    PipelineDefinition.describe_yaml_schema(),\n                ],\n                Optional(\"annotations\"): {\n                    str: Or(str, int, float, bool),\n                },\n            }\n        )\n\n    def to_file_data(self):\n        return {\n            \"pipelines\": [ppl.to_file_data() for ppl in self.pipelines_by_name.values()]\n        }\n\n    def all_subordinate_components(self) -&gt; Iterable[IntrospectiveIngestionComponent]:\n        return self.pipelines_by_name.values()\n\n    async def run_request(self, run_request: \"RunRequest\"):\n\"\"\"Execute a run request.\n\n        If the pipeline does not exist, this is a no-op. Otherwise, the run request\n        will be executed with the pipeline's definition. The run request will be\n        executed asynchronously via its `execute_with_definition` method.\n\n        Args:\n            run_request: The run request to execute.\n        \"\"\"\n        if (name := run_request.pipeline_name) not in self:\n            return\n\n        await run_request.execute_with_definition(self[name])\n\n    def __getitem__(self, pipeline_name):\n        return self.pipelines_by_name[pipeline_name]\n\n    def __contains__(self, pipeline_name):\n        return pipeline_name in self.pipelines_by_name\n\n    def add_pipeline_definition(self, definition: PipelineDefinition):\n\"\"\"Add a pipeline to the scope.\n\n        Args:\n            definition: The pipeline to add.\n        \"\"\"\n        self.pipelines_by_name[definition.name] = definition\n\n    def delete_pipeline(\n        self,\n        pipeline_name: str,\n        remove_pipeline_file: bool = True,\n        missing_ok: bool = True,\n    ) -&gt; bool:\n\"\"\"Delete a pipeline from the scope.\n\n        Args:\n            pipeline_name: The name of the pipeline to delete.\n            remove_pipeline_file: Whether to remove the pipeline's file from disk.\n            missing_ok: Whether to raise an error if the pipeline does not exist.\n\n        Returns:\n            True if the pipeline was deleted, False if it did not exist.\n        \"\"\"\n        definition = self.pipelines_by_name.get(pipeline_name)\n        if definition is None:\n            if not missing_ok:\n                raise MissingExpectedPipelineError(\n                    \"Attempted to delete pipeline that did not exist\"\n                )\n            else:\n                return False\n\n        del self.pipelines_by_name[pipeline_name]\n        if remove_pipeline_file:\n            definition.remove_file(missing_ok=missing_ok)\n\n        return True\n\n    @classmethod\n    def from_resources(\n        cls, name: str, package: resources.Package, persist: bool = False\n    ) -&gt; \"PipelineScope\":\n\"\"\"Load a `PipelineScope` from a package's resources.\n\n        Each `.yaml` file in the package's resources will be loaded as a pipeline.\n        Internally, this uses `importlib.resources` to load the files and calls\n        `PipelineDefinition.from_path` on each file. This means that the name of\n        the pipeline will be the name of the file without the .yaml extension.\n\n        Args:\n            name: The name of the scope.\n            package: The name of the package to load from.\n            persist: Whether or not to save the scope when the project is saved.\n\n        Returns:\n            A `PipelineScope` instance with the pipelines defined in the package.\n        \"\"\"\n        pipelines = [\n            PipelineDefinition.from_path(f)\n            for f in resources.files(package).iterdir()\n            if f.suffix == \".yaml\"\n        ]\n        return cls(name, pipelines, persist=persist)\n</code></pre>"},{"location":"python_reference/project/pipeline_scope/#nodestream.project.pipeline_scope.PipelineScope.add_pipeline_definition","title":"<code>add_pipeline_definition(definition)</code>","text":"<p>Add a pipeline to the scope.</p> <p>Parameters:</p> Name Type Description Default <code>definition</code> <code>PipelineDefinition</code> <p>The pipeline to add.</p> required Source code in <code>nodestream/project/pipeline_scope.py</code> Python<pre><code>def add_pipeline_definition(self, definition: PipelineDefinition):\n\"\"\"Add a pipeline to the scope.\n\n    Args:\n        definition: The pipeline to add.\n    \"\"\"\n    self.pipelines_by_name[definition.name] = definition\n</code></pre>"},{"location":"python_reference/project/pipeline_scope/#nodestream.project.pipeline_scope.PipelineScope.delete_pipeline","title":"<code>delete_pipeline(pipeline_name, remove_pipeline_file=True, missing_ok=True)</code>","text":"<p>Delete a pipeline from the scope.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>The name of the pipeline to delete.</p> required <code>remove_pipeline_file</code> <code>bool</code> <p>Whether to remove the pipeline's file from disk.</p> <code>True</code> <code>missing_ok</code> <code>bool</code> <p>Whether to raise an error if the pipeline does not exist.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the pipeline was deleted, False if it did not exist.</p> Source code in <code>nodestream/project/pipeline_scope.py</code> Python<pre><code>def delete_pipeline(\n    self,\n    pipeline_name: str,\n    remove_pipeline_file: bool = True,\n    missing_ok: bool = True,\n) -&gt; bool:\n\"\"\"Delete a pipeline from the scope.\n\n    Args:\n        pipeline_name: The name of the pipeline to delete.\n        remove_pipeline_file: Whether to remove the pipeline's file from disk.\n        missing_ok: Whether to raise an error if the pipeline does not exist.\n\n    Returns:\n        True if the pipeline was deleted, False if it did not exist.\n    \"\"\"\n    definition = self.pipelines_by_name.get(pipeline_name)\n    if definition is None:\n        if not missing_ok:\n            raise MissingExpectedPipelineError(\n                \"Attempted to delete pipeline that did not exist\"\n            )\n        else:\n            return False\n\n    del self.pipelines_by_name[pipeline_name]\n    if remove_pipeline_file:\n        definition.remove_file(missing_ok=missing_ok)\n\n    return True\n</code></pre>"},{"location":"python_reference/project/pipeline_scope/#nodestream.project.pipeline_scope.PipelineScope.from_resources","title":"<code>from_resources(name, package, persist=False)</code>  <code>classmethod</code>","text":"<p>Load a <code>PipelineScope</code> from a package's resources.</p> <p>Each <code>.yaml</code> file in the package's resources will be loaded as a pipeline. Internally, this uses <code>importlib.resources</code> to load the files and calls <code>PipelineDefinition.from_path</code> on each file. This means that the name of the pipeline will be the name of the file without the .yaml extension.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the scope.</p> required <code>package</code> <code>resources.Package</code> <p>The name of the package to load from.</p> required <code>persist</code> <code>bool</code> <p>Whether or not to save the scope when the project is saved.</p> <code>False</code> <p>Returns:</p> Type Description <code>PipelineScope</code> <p>A <code>PipelineScope</code> instance with the pipelines defined in the package.</p> Source code in <code>nodestream/project/pipeline_scope.py</code> Python<pre><code>@classmethod\ndef from_resources(\n    cls, name: str, package: resources.Package, persist: bool = False\n) -&gt; \"PipelineScope\":\n\"\"\"Load a `PipelineScope` from a package's resources.\n\n    Each `.yaml` file in the package's resources will be loaded as a pipeline.\n    Internally, this uses `importlib.resources` to load the files and calls\n    `PipelineDefinition.from_path` on each file. This means that the name of\n    the pipeline will be the name of the file without the .yaml extension.\n\n    Args:\n        name: The name of the scope.\n        package: The name of the package to load from.\n        persist: Whether or not to save the scope when the project is saved.\n\n    Returns:\n        A `PipelineScope` instance with the pipelines defined in the package.\n    \"\"\"\n    pipelines = [\n        PipelineDefinition.from_path(f)\n        for f in resources.files(package).iterdir()\n        if f.suffix == \".yaml\"\n    ]\n    return cls(name, pipelines, persist=persist)\n</code></pre>"},{"location":"python_reference/project/pipeline_scope/#nodestream.project.pipeline_scope.PipelineScope.run_request","title":"<code>run_request(run_request)</code>  <code>async</code>","text":"<p>Execute a run request.</p> <p>If the pipeline does not exist, this is a no-op. Otherwise, the run request will be executed with the pipeline's definition. The run request will be executed asynchronously via its <code>execute_with_definition</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>run_request</code> <code>RunRequest</code> <p>The run request to execute.</p> required Source code in <code>nodestream/project/pipeline_scope.py</code> Python<pre><code>async def run_request(self, run_request: \"RunRequest\"):\n\"\"\"Execute a run request.\n\n    If the pipeline does not exist, this is a no-op. Otherwise, the run request\n    will be executed with the pipeline's definition. The run request will be\n    executed asynchronously via its `execute_with_definition` method.\n\n    Args:\n        run_request: The run request to execute.\n    \"\"\"\n    if (name := run_request.pipeline_name) not in self:\n        return\n\n    await run_request.execute_with_definition(self[name])\n</code></pre>"},{"location":"python_reference/project/project/","title":"Project","text":""},{"location":"python_reference/project/project/#nodestream.project.project.Project","title":"<code>Project</code>","text":"<p>             Bases: <code>AggregatedIntrospectiveIngestionComponent</code>, <code>LoadsFromYamlFile</code>, <code>SavesToYamlFile</code></p> <p>A <code>Project</code> represents a collection of pipelines.</p> <p>A project is the top-level object in a nodestream project. It contains a collection of scopes, which in turn contain a collection of pipelines. When interacting with nodestream programmatically, you will typically interact with a project object. This is where pipeline execution begins and where all data about the project is stored.</p> Source code in <code>nodestream/project/project.py</code> Python<pre><code>class Project(\n    AggregatedIntrospectiveIngestionComponent, LoadsFromYamlFile, SavesToYamlFile\n):\n\"\"\"A `Project` represents a collection of pipelines.\n\n    A project is the top-level object in a nodestream project.\n    It contains a collection of scopes, which in turn contain a collection of pipelines.\n    When interacting with nodestream programmatically, you will typically interact with a project object.\n    This is where pipeline execution begins and where all data about the project is stored.\n    \"\"\"\n\n    @classmethod\n    def describe_yaml_schema(cls):\n        from schema import Optional, Schema\n\n        return Schema(\n            {\n                Optional(\"scopes\"): {\n                    str: PipelineScope.describe_yaml_schema(),\n                },\n            }\n        )\n\n    @classmethod\n    def from_file_data(cls, data) -&gt; \"Project\":\n\"\"\"Creates a project from file data.\n\n        The file data should be a dictionary containing a \"scopes\" key,\n        which should be a dictionary mapping scope names to scope file data and\n        should be validated by the schema returned by `describe_yaml_schema`.\n\n        This method should not be called directly. Instead, use `LoadsFromYamlFile.load_from_yaml_file`.\n\n        Args:\n            data (Dict): The file data to create the project from.\n\n        Returns:\n            Project: The project created from the file data.\n        \"\"\"\n        scopes_data = data.pop(\"scopes\", {})\n        scopes = [\n            PipelineScope.from_file_data(*scope_data)\n            for scope_data in scopes_data.items()\n        ]\n        project = cls(scopes)\n        for plugin in ProjectPlugin.all():\n            plugin().activate(project)\n        return project\n\n    def to_file_data(self) -&gt; dict:\n\"\"\"Returns the file data for the project.\n\n        This method should not be called directly. Instead, use `SavesToYamlFile.save_to_yaml_file`.\n\n        Returns:\n            Dict: The file data for the project.\n        \"\"\"\n        return {\n            \"scopes\": {\n                scope.name: scope.to_file_data()\n                for scope in self.scopes_by_name.values()\n                if scope.persist\n            },\n        }\n\n    def __init__(self, scopes: List[PipelineScope]):\n        self.scopes_by_name: Dict[str, PipelineScope] = {}\n        for scope in scopes:\n            self.add_scope(scope)\n\n    async def run(self, request: RunRequest):\n\"\"\"Takes a run request and runs the appropriate pipeline.\n\n        Args:\n            request (RunRequest): The run request to run.\n        \"\"\"\n        for scope in self.scopes_by_name.values():\n            await scope.run_request(request)\n\n    async def get_snapshot_for(self, pipeline_name: str) -&gt; list:\n\"\"\"Returns the output of the pipeline with the given name to be used as a snapshot.\n\n        This method is intended for testing purposes only. It will run the pipeline and return the results.\n        The pipeline is run with the `test` annotation, so components that you want to run must be annotated with `test` or not annotated at all.\n\n        Args:\n            pipeline_name (str): The name of the pipeline to get a snapshot for.\n\n        Returns:\n            str: The snapshot of the pipeline.\n        \"\"\"\n        run_request = RunRequest.for_testing(pipeline_name, results := [])\n        await self.run(run_request)\n        return results\n\n    def add_scope(self, scope: PipelineScope):\n\"\"\"Adds a scope to the project.\n\n        Args:\n            scope (PipelineScope): The scope to add.\n        \"\"\"\n        self.scopes_by_name[scope.name] = scope\n\n    def get_scopes_by_name(self, scope_name: Optional[str]) -&gt; Iterable[PipelineScope]:\n\"\"\"Returns the scopes with the given name.\n\n        If `scope_name` is None, all scopes will be returned. If no scopes are found, an empty list will be returned.\n        \"\"\"\n        if scope_name is None:\n            return self.scopes_by_name.values()\n\n        if scope_name not in self.scopes_by_name:\n            return []\n\n        return [self.scopes_by_name[scope_name]]\n\n    def delete_pipeline(\n        self,\n        scope_name: Optional[str],\n        pipeline_name: str,\n        remove_pipeline_file: bool = True,\n        missing_ok: bool = True,\n    ):\n\"\"\"Deletes a pipeline from the project.\n\n        Args:\n            scope_name (Optional[str]): The name of the scope containing the pipeline. If None, all scopes will be searched.\n            pipeline_name (str): The name of the pipeline to delete.\n            remove_pipeline_file (bool, optional): Whether to remove the pipeline file from disk. Defaults to True.\n            missing_ok (bool, optional): Whether to raise an error if the pipeline is not found. Defaults to True.\n\n        Raises:\n            ValueError: If the pipeline is not found and `missing_ok` is False.\n        \"\"\"\n        for scopes in self.get_scopes_by_name(scope_name):\n            scopes.delete_pipeline(\n                pipeline_name,\n                remove_pipeline_file=remove_pipeline_file,\n                missing_ok=missing_ok,\n            )\n\n    def get_schema(self, type_overrides_file: Optional[Path] = None) -&gt; GraphSchema:\n\"\"\"Returns a `GraphSchema` representing the project.\n\n        If a `type_overrides_file` is provided, the schema will be updated with the overrides.\n        Since nodestream does not know about the types of the data in the project, this is the\n        only way to provide type information to the schema where it is not available natively.\n\n        Every pipeline in the project will be loaded and introspected to build the schema.\n\n        Args:\n            type_overrides_file (Optional[Path], optional): A path to a YAML file containing type overrides. Defaults to None.\n\n        Returns:\n            GraphSchema: The schema representing the project.\n        \"\"\"\n        schema = self.generate_graph_schema()\n        if type_overrides_file is not None:\n            schema.apply_type_overrides_from_file(type_overrides_file)\n        return schema\n\n    def all_subordinate_components(self) -&gt; Iterable[IntrospectiveIngestionComponent]:\n        return self.scopes_by_name.values()\n\n    def dig_for_step_of_type(\n        self, step_type: Type[T]\n    ) -&gt; Iterable[Tuple[PipelineDefinition, int, T]]:\n\"\"\"Yields all steps of the given type in the project.\n\n        Yields tuples of (pipeline_definition, step_index, step).\n\n        Args:\n            step_type (Type[T]): The type of step to look for.\n\n        Yields:\n            Tuple[PipelineDefinition, int, T]: The steps of the given type and their locations.\n        \"\"\"\n        for scope in self.scopes_by_name.values():\n            for pipeline_definition in scope.pipelines_by_name.values():\n                pipeline_steps = (\n                    pipeline_definition.initialize_for_introspection().steps\n                )\n                for idx, step in enumerate(pipeline_steps):\n                    if isinstance(step, step_type):\n                        yield pipeline_definition, idx, step\n</code></pre>"},{"location":"python_reference/project/project/#nodestream.project.project.Project.add_scope","title":"<code>add_scope(scope)</code>","text":"<p>Adds a scope to the project.</p> <p>Parameters:</p> Name Type Description Default <code>scope</code> <code>PipelineScope</code> <p>The scope to add.</p> required Source code in <code>nodestream/project/project.py</code> Python<pre><code>def add_scope(self, scope: PipelineScope):\n\"\"\"Adds a scope to the project.\n\n    Args:\n        scope (PipelineScope): The scope to add.\n    \"\"\"\n    self.scopes_by_name[scope.name] = scope\n</code></pre>"},{"location":"python_reference/project/project/#nodestream.project.project.Project.delete_pipeline","title":"<code>delete_pipeline(scope_name, pipeline_name, remove_pipeline_file=True, missing_ok=True)</code>","text":"<p>Deletes a pipeline from the project.</p> <p>Parameters:</p> Name Type Description Default <code>scope_name</code> <code>Optional[str]</code> <p>The name of the scope containing the pipeline. If None, all scopes will be searched.</p> required <code>pipeline_name</code> <code>str</code> <p>The name of the pipeline to delete.</p> required <code>remove_pipeline_file</code> <code>bool</code> <p>Whether to remove the pipeline file from disk. Defaults to True.</p> <code>True</code> <code>missing_ok</code> <code>bool</code> <p>Whether to raise an error if the pipeline is not found. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pipeline is not found and <code>missing_ok</code> is False.</p> Source code in <code>nodestream/project/project.py</code> Python<pre><code>def delete_pipeline(\n    self,\n    scope_name: Optional[str],\n    pipeline_name: str,\n    remove_pipeline_file: bool = True,\n    missing_ok: bool = True,\n):\n\"\"\"Deletes a pipeline from the project.\n\n    Args:\n        scope_name (Optional[str]): The name of the scope containing the pipeline. If None, all scopes will be searched.\n        pipeline_name (str): The name of the pipeline to delete.\n        remove_pipeline_file (bool, optional): Whether to remove the pipeline file from disk. Defaults to True.\n        missing_ok (bool, optional): Whether to raise an error if the pipeline is not found. Defaults to True.\n\n    Raises:\n        ValueError: If the pipeline is not found and `missing_ok` is False.\n    \"\"\"\n    for scopes in self.get_scopes_by_name(scope_name):\n        scopes.delete_pipeline(\n            pipeline_name,\n            remove_pipeline_file=remove_pipeline_file,\n            missing_ok=missing_ok,\n        )\n</code></pre>"},{"location":"python_reference/project/project/#nodestream.project.project.Project.dig_for_step_of_type","title":"<code>dig_for_step_of_type(step_type)</code>","text":"<p>Yields all steps of the given type in the project.</p> <p>Yields tuples of (pipeline_definition, step_index, step).</p> <p>Parameters:</p> Name Type Description Default <code>step_type</code> <code>Type[T]</code> <p>The type of step to look for.</p> required <p>Yields:</p> Type Description <code>Iterable[Tuple[PipelineDefinition, int, T]]</code> <p>Tuple[PipelineDefinition, int, T]: The steps of the given type and their locations.</p> Source code in <code>nodestream/project/project.py</code> Python<pre><code>def dig_for_step_of_type(\n    self, step_type: Type[T]\n) -&gt; Iterable[Tuple[PipelineDefinition, int, T]]:\n\"\"\"Yields all steps of the given type in the project.\n\n    Yields tuples of (pipeline_definition, step_index, step).\n\n    Args:\n        step_type (Type[T]): The type of step to look for.\n\n    Yields:\n        Tuple[PipelineDefinition, int, T]: The steps of the given type and their locations.\n    \"\"\"\n    for scope in self.scopes_by_name.values():\n        for pipeline_definition in scope.pipelines_by_name.values():\n            pipeline_steps = (\n                pipeline_definition.initialize_for_introspection().steps\n            )\n            for idx, step in enumerate(pipeline_steps):\n                if isinstance(step, step_type):\n                    yield pipeline_definition, idx, step\n</code></pre>"},{"location":"python_reference/project/project/#nodestream.project.project.Project.from_file_data","title":"<code>from_file_data(data)</code>  <code>classmethod</code>","text":"<p>Creates a project from file data.</p> <p>The file data should be a dictionary containing a \"scopes\" key, which should be a dictionary mapping scope names to scope file data and should be validated by the schema returned by <code>describe_yaml_schema</code>.</p> <p>This method should not be called directly. Instead, use <code>LoadsFromYamlFile.load_from_yaml_file</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict</code> <p>The file data to create the project from.</p> required <p>Returns:</p> Name Type Description <code>Project</code> <code>Project</code> <p>The project created from the file data.</p> Source code in <code>nodestream/project/project.py</code> Python<pre><code>@classmethod\ndef from_file_data(cls, data) -&gt; \"Project\":\n\"\"\"Creates a project from file data.\n\n    The file data should be a dictionary containing a \"scopes\" key,\n    which should be a dictionary mapping scope names to scope file data and\n    should be validated by the schema returned by `describe_yaml_schema`.\n\n    This method should not be called directly. Instead, use `LoadsFromYamlFile.load_from_yaml_file`.\n\n    Args:\n        data (Dict): The file data to create the project from.\n\n    Returns:\n        Project: The project created from the file data.\n    \"\"\"\n    scopes_data = data.pop(\"scopes\", {})\n    scopes = [\n        PipelineScope.from_file_data(*scope_data)\n        for scope_data in scopes_data.items()\n    ]\n    project = cls(scopes)\n    for plugin in ProjectPlugin.all():\n        plugin().activate(project)\n    return project\n</code></pre>"},{"location":"python_reference/project/project/#nodestream.project.project.Project.get_schema","title":"<code>get_schema(type_overrides_file=None)</code>","text":"<p>Returns a <code>GraphSchema</code> representing the project.</p> <p>If a <code>type_overrides_file</code> is provided, the schema will be updated with the overrides. Since nodestream does not know about the types of the data in the project, this is the only way to provide type information to the schema where it is not available natively.</p> <p>Every pipeline in the project will be loaded and introspected to build the schema.</p> <p>Parameters:</p> Name Type Description Default <code>type_overrides_file</code> <code>Optional[Path]</code> <p>A path to a YAML file containing type overrides. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GraphSchema</code> <code>GraphSchema</code> <p>The schema representing the project.</p> Source code in <code>nodestream/project/project.py</code> Python<pre><code>def get_schema(self, type_overrides_file: Optional[Path] = None) -&gt; GraphSchema:\n\"\"\"Returns a `GraphSchema` representing the project.\n\n    If a `type_overrides_file` is provided, the schema will be updated with the overrides.\n    Since nodestream does not know about the types of the data in the project, this is the\n    only way to provide type information to the schema where it is not available natively.\n\n    Every pipeline in the project will be loaded and introspected to build the schema.\n\n    Args:\n        type_overrides_file (Optional[Path], optional): A path to a YAML file containing type overrides. Defaults to None.\n\n    Returns:\n        GraphSchema: The schema representing the project.\n    \"\"\"\n    schema = self.generate_graph_schema()\n    if type_overrides_file is not None:\n        schema.apply_type_overrides_from_file(type_overrides_file)\n    return schema\n</code></pre>"},{"location":"python_reference/project/project/#nodestream.project.project.Project.get_scopes_by_name","title":"<code>get_scopes_by_name(scope_name)</code>","text":"<p>Returns the scopes with the given name.</p> <p>If <code>scope_name</code> is None, all scopes will be returned. If no scopes are found, an empty list will be returned.</p> Source code in <code>nodestream/project/project.py</code> Python<pre><code>def get_scopes_by_name(self, scope_name: Optional[str]) -&gt; Iterable[PipelineScope]:\n\"\"\"Returns the scopes with the given name.\n\n    If `scope_name` is None, all scopes will be returned. If no scopes are found, an empty list will be returned.\n    \"\"\"\n    if scope_name is None:\n        return self.scopes_by_name.values()\n\n    if scope_name not in self.scopes_by_name:\n        return []\n\n    return [self.scopes_by_name[scope_name]]\n</code></pre>"},{"location":"python_reference/project/project/#nodestream.project.project.Project.get_snapshot_for","title":"<code>get_snapshot_for(pipeline_name)</code>  <code>async</code>","text":"<p>Returns the output of the pipeline with the given name to be used as a snapshot.</p> <p>This method is intended for testing purposes only. It will run the pipeline and return the results. The pipeline is run with the <code>test</code> annotation, so components that you want to run must be annotated with <code>test</code> or not annotated at all.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>The name of the pipeline to get a snapshot for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>list</code> <p>The snapshot of the pipeline.</p> Source code in <code>nodestream/project/project.py</code> Python<pre><code>async def get_snapshot_for(self, pipeline_name: str) -&gt; list:\n\"\"\"Returns the output of the pipeline with the given name to be used as a snapshot.\n\n    This method is intended for testing purposes only. It will run the pipeline and return the results.\n    The pipeline is run with the `test` annotation, so components that you want to run must be annotated with `test` or not annotated at all.\n\n    Args:\n        pipeline_name (str): The name of the pipeline to get a snapshot for.\n\n    Returns:\n        str: The snapshot of the pipeline.\n    \"\"\"\n    run_request = RunRequest.for_testing(pipeline_name, results := [])\n    await self.run(run_request)\n    return results\n</code></pre>"},{"location":"python_reference/project/project/#nodestream.project.project.Project.run","title":"<code>run(request)</code>  <code>async</code>","text":"<p>Takes a run request and runs the appropriate pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>RunRequest</code> <p>The run request to run.</p> required Source code in <code>nodestream/project/project.py</code> Python<pre><code>async def run(self, request: RunRequest):\n\"\"\"Takes a run request and runs the appropriate pipeline.\n\n    Args:\n        request (RunRequest): The run request to run.\n    \"\"\"\n    for scope in self.scopes_by_name.values():\n        await scope.run_request(request)\n</code></pre>"},{"location":"python_reference/project/project/#nodestream.project.project.Project.to_file_data","title":"<code>to_file_data()</code>","text":"<p>Returns the file data for the project.</p> <p>This method should not be called directly. Instead, use <code>SavesToYamlFile.save_to_yaml_file</code>.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>dict</code> <p>The file data for the project.</p> Source code in <code>nodestream/project/project.py</code> Python<pre><code>def to_file_data(self) -&gt; dict:\n\"\"\"Returns the file data for the project.\n\n    This method should not be called directly. Instead, use `SavesToYamlFile.save_to_yaml_file`.\n\n    Returns:\n        Dict: The file data for the project.\n    \"\"\"\n    return {\n        \"scopes\": {\n            scope.name: scope.to_file_data()\n            for scope in self.scopes_by_name.values()\n            if scope.persist\n        },\n    }\n</code></pre>"},{"location":"python_reference/project/project/#nodestream.project.project.ProjectPlugin","title":"<code>ProjectPlugin</code>","text":"<p>             Bases: <code>Pluggable</code></p> <p>A plugin that can be used to modify a project.</p> <p>Plugins are used to add functionality to projects. They are typically used to add new pipeline scopes or to modify existing scopes and pipelines. Plugins are activated when a project is loaded from disk.</p> <p>Plugins are registered by subclassing <code>ProjectPlugin</code> and implementing the <code>activate</code> method. Additionally, they must be use the <code>entry_points</code> mechanism to register themselves as a plugin.</p> Source code in <code>nodestream/project/project.py</code> Python<pre><code>class ProjectPlugin(Pluggable):\n\"\"\"A plugin that can be used to modify a project.\n\n    Plugins are used to add functionality to projects. They are typically used to add new pipeline scopes\n    or to modify existing scopes and pipelines. Plugins are activated when a project is loaded from disk.\n\n    Plugins are registered by subclassing `ProjectPlugin` and implementing the `activate` method.\n    Additionally, they must be use the `entry_points` mechanism to register themselves as a plugin.\n    \"\"\"\n\n    entrypoint_name = \"projects\"\n\n    def activate(self, project: Project):\n\"\"\"Called when a project is loaded from disk.\n\n        This is where the plugin should modify the project. For example, a plugin might add a new scope\n        or modify an existing scope. The plugin should not modify the project file on disk. Instead, it\n        should modify the project object in memory.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"python_reference/project/project/#nodestream.project.project.ProjectPlugin.activate","title":"<code>activate(project)</code>","text":"<p>Called when a project is loaded from disk.</p> <p>This is where the plugin should modify the project. For example, a plugin might add a new scope or modify an existing scope. The plugin should not modify the project file on disk. Instead, it should modify the project object in memory.</p> Source code in <code>nodestream/project/project.py</code> Python<pre><code>def activate(self, project: Project):\n\"\"\"Called when a project is loaded from disk.\n\n    This is where the plugin should modify the project. For example, a plugin might add a new scope\n    or modify an existing scope. The plugin should not modify the project file on disk. Instead, it\n    should modify the project object in memory.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"python_reference/project/run_request/","title":"Run request","text":""},{"location":"python_reference/project/run_request/#nodestream.project.run_request.RunRequest","title":"<code>RunRequest</code>  <code>dataclass</code>","text":"<p>A <code>RunRequest</code> represents a request to run a pipeline.</p> <p><code>RunRequest</code> objects should be submitted to a <code>Project</code> object to be executed via its <code>run_request</code> method. The <code>run_request</code> method will execute the run request on the appopriate pipeline if it exists. Otherwise, it will be a no-op.</p> Source code in <code>nodestream/project/run_request.py</code> Python<pre><code>@dataclass\nclass RunRequest:\n\"\"\"A `RunRequest` represents a request to run a pipeline.\n\n    `RunRequest` objects should be submitted to a `Project` object to be executed\n    via its `run_request` method. The `run_request` method will execute the run request\n    on the appopriate pipeline if it exists. Otherwise, it will be a no-op.\n    \"\"\"\n\n    pipeline_name: str\n    initialization_arguments: PipelineInitializationArguments\n    progress_reporter: PipelineProgressReporter\n\n    @classmethod\n    def for_testing(cls, pipeline_name: str, results_list: list) -&gt; \"RunRequest\":\n\"\"\"Create a `RunRequest` for testing.\n\n        This method is intended to be used for testing purposes only. It will create a\n        run request with the given pipeline name and `PipelineInitializationArguments`\n        for testing.\n\n        Args:\n            pipeline_name: The name of the pipeline to run.\n            results_list: The list to append results to.\n\n        Returns:\n            RunRequest: A `RunRequest` for testing.\n        \"\"\"\n        return cls(\n            pipeline_name,\n            PipelineInitializationArguments.for_testing(),\n            PipelineProgressReporter.for_testing(results_list),\n        )\n\n    async def execute_with_definition(self, definition: PipelineDefinition):\n\"\"\"Execute this run request with the given pipeline definition.\n\n        This method is intended to be called by `PipelineScope` and should not be called\n        directly without good reason. It will execute the run request asynchronously\n        with the given pipeline definition. The run request will be executed within a\n        context manager that sets the current pipeline name to the name of the pipeline\n        being executed.\n\n        Args:\n            definition: The pipeline definition to execute this run request with.\n        \"\"\"\n        with start_context(self.pipeline_name):\n            pipeline = definition.initialize(self.initialization_arguments)\n            await pipeline.run(self.progress_reporter)\n</code></pre>"},{"location":"python_reference/project/run_request/#nodestream.project.run_request.RunRequest.execute_with_definition","title":"<code>execute_with_definition(definition)</code>  <code>async</code>","text":"<p>Execute this run request with the given pipeline definition.</p> <p>This method is intended to be called by <code>PipelineScope</code> and should not be called directly without good reason. It will execute the run request asynchronously with the given pipeline definition. The run request will be executed within a context manager that sets the current pipeline name to the name of the pipeline being executed.</p> <p>Parameters:</p> Name Type Description Default <code>definition</code> <code>PipelineDefinition</code> <p>The pipeline definition to execute this run request with.</p> required Source code in <code>nodestream/project/run_request.py</code> Python<pre><code>async def execute_with_definition(self, definition: PipelineDefinition):\n\"\"\"Execute this run request with the given pipeline definition.\n\n    This method is intended to be called by `PipelineScope` and should not be called\n    directly without good reason. It will execute the run request asynchronously\n    with the given pipeline definition. The run request will be executed within a\n    context manager that sets the current pipeline name to the name of the pipeline\n    being executed.\n\n    Args:\n        definition: The pipeline definition to execute this run request with.\n    \"\"\"\n    with start_context(self.pipeline_name):\n        pipeline = definition.initialize(self.initialization_arguments)\n        await pipeline.run(self.progress_reporter)\n</code></pre>"},{"location":"python_reference/project/run_request/#nodestream.project.run_request.RunRequest.for_testing","title":"<code>for_testing(pipeline_name, results_list)</code>  <code>classmethod</code>","text":"<p>Create a <code>RunRequest</code> for testing.</p> <p>This method is intended to be used for testing purposes only. It will create a run request with the given pipeline name and <code>PipelineInitializationArguments</code> for testing.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>The name of the pipeline to run.</p> required <code>results_list</code> <code>list</code> <p>The list to append results to.</p> required <p>Returns:</p> Name Type Description <code>RunRequest</code> <code>RunRequest</code> <p>A <code>RunRequest</code> for testing.</p> Source code in <code>nodestream/project/run_request.py</code> Python<pre><code>@classmethod\ndef for_testing(cls, pipeline_name: str, results_list: list) -&gt; \"RunRequest\":\n\"\"\"Create a `RunRequest` for testing.\n\n    This method is intended to be used for testing purposes only. It will create a\n    run request with the given pipeline name and `PipelineInitializationArguments`\n    for testing.\n\n    Args:\n        pipeline_name: The name of the pipeline to run.\n        results_list: The list to append results to.\n\n    Returns:\n        RunRequest: A `RunRequest` for testing.\n    \"\"\"\n    return cls(\n        pipeline_name,\n        PipelineInitializationArguments.for_testing(),\n        PipelineProgressReporter.for_testing(results_list),\n    )\n</code></pre>"},{"location":"python_reference/project/audits/audit/","title":"Audit","text":""},{"location":"python_reference/project/audits/audit/#nodestream.project.audits.audit.Audit","title":"<code>Audit</code>","text":"<p>             Bases: <code>Pluggable</code></p> Source code in <code>nodestream/project/audits/audit.py</code> Python<pre><code>class Audit(Pluggable):\n    name = \"unnamed\"\n    description = \"Checks something about a project\"\n    entrypoint_name = \"audits\"\n\n    def __init__(self, printer: AuditPrinter) -&gt; None:\n        self.printer = printer\n        self.success_count = 0\n        self.warning_count = 0\n        self.failure_count = 0\n\n    async def run(self, project: Project) -&gt; None:\n        pass\n\n    def success(self, message: str) -&gt; None:\n\"\"\"Print a success message.\n\n        The total number of successes will be tracked and is accessible via the `success_count` attribute.\n\n        Args:\n            message: The message to print.\n        \"\"\"\n        self.success_count += 1\n        self.printer.print_success(message)\n\n    def failure(self, message: str) -&gt; None:\n\"\"\"Print a failure message.\n\n        The total number of failures will be tracked and is accessible via the `failure_count` attribute.\n        Non zero failure counts will cause the audit to fail and the command line to exit with a non-zero exit\n        code. Specifically, the exit code will be the number of failures.\n\n        Args:\n            message: The message to print.\n        \"\"\"\n        self.failure_count += 1\n        self.printer.print_failure(message)\n\n    def warning(self, message: str) -&gt; None:\n\"\"\"Print a warning message.\n\n        The total number of warnings will be tracked and is accessible via the `warnings_count`\n        attribute.\n        \"\"\"\n        self.warning_count += 1\n        self.printer.print_warning(message)\n</code></pre>"},{"location":"python_reference/project/audits/audit/#nodestream.project.audits.audit.Audit.failure","title":"<code>failure(message)</code>","text":"<p>Print a failure message.</p> <p>The total number of failures will be tracked and is accessible via the <code>failure_count</code> attribute. Non zero failure counts will cause the audit to fail and the command line to exit with a non-zero exit code. Specifically, the exit code will be the number of failures.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to print.</p> required Source code in <code>nodestream/project/audits/audit.py</code> Python<pre><code>def failure(self, message: str) -&gt; None:\n\"\"\"Print a failure message.\n\n    The total number of failures will be tracked and is accessible via the `failure_count` attribute.\n    Non zero failure counts will cause the audit to fail and the command line to exit with a non-zero exit\n    code. Specifically, the exit code will be the number of failures.\n\n    Args:\n        message: The message to print.\n    \"\"\"\n    self.failure_count += 1\n    self.printer.print_failure(message)\n</code></pre>"},{"location":"python_reference/project/audits/audit/#nodestream.project.audits.audit.Audit.success","title":"<code>success(message)</code>","text":"<p>Print a success message.</p> <p>The total number of successes will be tracked and is accessible via the <code>success_count</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to print.</p> required Source code in <code>nodestream/project/audits/audit.py</code> Python<pre><code>def success(self, message: str) -&gt; None:\n\"\"\"Print a success message.\n\n    The total number of successes will be tracked and is accessible via the `success_count` attribute.\n\n    Args:\n        message: The message to print.\n    \"\"\"\n    self.success_count += 1\n    self.printer.print_success(message)\n</code></pre>"},{"location":"python_reference/project/audits/audit/#nodestream.project.audits.audit.Audit.warning","title":"<code>warning(message)</code>","text":"<p>Print a warning message.</p> <p>The total number of warnings will be tracked and is accessible via the <code>warnings_count</code> attribute.</p> Source code in <code>nodestream/project/audits/audit.py</code> Python<pre><code>def warning(self, message: str) -&gt; None:\n\"\"\"Print a warning message.\n\n    The total number of warnings will be tracked and is accessible via the `warnings_count`\n    attribute.\n    \"\"\"\n    self.warning_count += 1\n    self.printer.print_warning(message)\n</code></pre>"},{"location":"python_reference/project/audits/audit_printer/","title":"Audit printer","text":""},{"location":"python_reference/project/audits/audit_printer/#nodestream.project.audits.audit_printer.AuditPrinter","title":"<code>AuditPrinter</code>","text":"<p>Prints audit messages to an output source. This is the default implementation that uses print to output to the console.</p> Source code in <code>nodestream/project/audits/audit_printer.py</code> Python<pre><code>class AuditPrinter:\n\"\"\"Prints audit messages to an output source. This is the default implementation that uses print to output to the console.\"\"\"\n\n    def print_success(self, message: str) -&gt; None:\n        print(message)\n\n    def print_failure(self, message: str) -&gt; None:\n        print(message)\n\n    def print_warning(self, message: str) -&gt; None:\n        print(message)\n</code></pre>"},{"location":"python_reference/project/audits/audit_referencial_integrity/","title":"Audit referencial integrity","text":""},{"location":"python_reference/project/audits/audit_ttls/","title":"Audit ttls","text":""},{"location":"python_reference/schema/indexes/","title":"Indexes","text":""},{"location":"python_reference/schema/indexes/#nodestream.schema.indexes.FieldIndex","title":"<code>FieldIndex</code>  <code>dataclass</code>","text":"<p>Defines an index that is used on a field for a Graph Object.</p> <p><code>FieldIndex</code>es are created on fields that do not imply a uniqueness constraint and \"just\" need to be indexes for query performance.</p> Source code in <code>nodestream/schema/indexes.py</code> Python<pre><code>@dataclass(frozen=True, slots=True)\nclass FieldIndex:\n\"\"\"Defines an index that is used on a field for a Graph Object.\n\n    `FieldIndex`es are created on fields that do not imply a uniqueness constraint and \"just\" need\n    to be indexes for query performance.\n    \"\"\"\n\n    type: str\n    field: str\n    object_type: GraphObjectType\n\n    async def ingest(self, strategy: \"IngestionStrategy\"):\n        await strategy.upsert_field_index(self)\n\n    @classmethod\n    def for_ttl_timestamp(\n        cls, type, object_type: GraphObjectType = GraphObjectType.NODE\n    ):\n        return cls(type, \"last_ingested_at\", object_type=object_type)\n</code></pre>"},{"location":"python_reference/schema/indexes/#nodestream.schema.indexes.KeyIndex","title":"<code>KeyIndex</code>  <code>dataclass</code>","text":"<p>Defines an Index that is used as a Key on a Node Type.</p> <p><code>KeyIndex</code>es are created by Interpretations that attempt to create and use a node reference as a Primary Key. Usually this implies the creation of a uniqueness constraint in the database.</p> <p>See information on your Underlying Graph Database Adapter for information on how this is implemented.</p> Source code in <code>nodestream/schema/indexes.py</code> Python<pre><code>@dataclass(frozen=True, slots=True)\nclass KeyIndex:\n\"\"\"Defines an Index that is used as a Key on a Node Type.\n\n    `KeyIndex`es are created by Interpretations that attempt to create and use\n    a node reference as a Primary Key. Usually this implies the creation of a\n    uniqueness constraint in the database.\n\n    See information on your Underlying Graph Database Adapter for information\n    on how this is implemented.\n    \"\"\"\n\n    type: str\n    identity_keys: frozenset\n\n    async def ingest(self, strategy: \"IngestionStrategy\"):\n        await strategy.upsert_key_index(self)\n</code></pre>"},{"location":"python_reference/schema/schema/","title":"Schema","text":""},{"location":"python_reference/schema/schema/#nodestream.schema.schema.AggregatedIntrospectiveIngestionComponent","title":"<code>AggregatedIntrospectiveIngestionComponent</code>","text":"<p>             Bases: <code>IntrospectiveIngestionComponent</code></p> <p>A mixin to provide utilities for <code>IntrospectableIngestionComponent</code>s that are aggregations of others.</p> <p>For many types in our ingestion hierarchy, they are aggregations of subordinate components that each on their own provide part of the make up for a graph schema. When we look at higher order components such as a Interpreter, they need to merge and resolve ambiguity amongst its child ingestion components (Interpretations).</p> <p>This mixin provides that higher order aggregation logic to merge and resolve ambiguity amongst such a hierarchy.</p> Source code in <code>nodestream/schema/schema.py</code> Python<pre><code>class AggregatedIntrospectiveIngestionComponent(IntrospectiveIngestionComponent):\n\"\"\"A mixin to provide utilities for `IntrospectableIngestionComponent`s that are aggregations of others.\n\n    For many types in our ingestion hierarchy, they are aggregations of subordinate components that each\n    on their own provide part of the make up for a graph schema. When we look at higher order components such\n    as a Interpreter, they need to merge and resolve ambiguity amongst its child ingestion components (Interpretations).\n\n    This mixin provides that higher order aggregation logic to merge and resolve ambiguity amongst such a hierarchy.\n    \"\"\"\n\n    @abstractmethod\n    def all_subordinate_components(self) -&gt; Iterable[IntrospectiveIngestionComponent]:\n        raise NotImplementedError\n\n    def gather_used_indexes(self):\n        return {\n            index\n            for component in self.all_subordinate_components()\n            for index in component.gather_used_indexes()\n        }\n\n    def gather_object_shapes(self):\n        shapes = [\n            shape\n            for component in self.all_subordinate_components()\n            for shape in component.gather_object_shapes()\n        ]\n        return _merge_overlapping_items(shapes, shapes)\n\n    def gather_present_relationships(self):\n        shapes = self.gather_object_shapes()\n        rels = [\n            rel\n            for component in self.all_subordinate_components()\n            for rel in component.gather_present_relationships()\n        ]\n        return _merge_overlapping_items(rels, shapes)\n</code></pre>"},{"location":"python_reference/schema/schema/#nodestream.schema.schema.GraphObjectShape","title":"<code>GraphObjectShape</code>  <code>dataclass</code>","text":"<p>Defines the shape of an object in the graph.</p> <p>The shape of an object consists of three parts:</p> <p>1.) Whether its a node or a relationship. 2.) What properties it has on it. 3.) What the type of the node/relationship is.</p> Source code in <code>nodestream/schema/schema.py</code> Python<pre><code>@dataclass(slots=True)\nclass GraphObjectShape:\n\"\"\"Defines the shape of an object in the graph.\n\n    The shape of an object consists of three parts:\n\n    1.) Whether its a node or a relationship.\n    2.) What properties it has on it.\n    3.) What the type of the node/relationship is.\n    \"\"\"\n\n    graph_object_type: GraphObjectType\n    object_type: TypeMarker\n    properties: PropertyMetadataSet\n\n    def overlaps_with(self, other: \"GraphObjectShape\") -&gt; bool:\n        return (\n            self.graph_object_type == other.graph_object_type\n            and self.object_type == other.object_type\n        )\n\n    def include(self, other: \"GraphObjectShape\"):\n        if not self.overlaps_with(other):\n            raise ValueError(\"Cannot include when the shape is not overlapping\")\n\n        self.properties.update(other.properties)\n\n    def property_names(self):\n        all_props = self.properties.properties\n        return [all_props[prop].name for prop in all_props.keys()]\n\n    def resolve_types(self, shapes: Iterable[\"GraphObjectShape\"]):\n        if object_type := self.object_type.resolve_type(shapes):\n            self.object_type = object_type\n\n    @property\n    def is_node(self) -&gt; bool:\n        return self.graph_object_type == GraphObjectType.NODE\n\n    @property\n    def is_relationship(self) -&gt; bool:\n        return self.graph_object_type == GraphObjectType.RELATIONSHIP\n\n    @property\n    def has_known_type(self) -&gt; bool:\n        return isinstance(self.object_type, KnownTypeMarker)\n\n    def __str__(self) -&gt; str:\n        return f\"[{self.graph_object_type}] {self.object_type}:\\n\\t{self.properties}\"\n</code></pre>"},{"location":"python_reference/schema/schema/#nodestream.schema.schema.GraphSchema","title":"<code>GraphSchema</code>  <code>dataclass</code>","text":"<p>Defines the Schema of a graph by specifiying the node and relationship types and how they interconnect.</p> <p>See <code>GraphObjectShape</code> and <code>PresentRelationship</code> for more details.</p> Source code in <code>nodestream/schema/schema.py</code> Python<pre><code>@dataclass(frozen=True, slots=True)\nclass GraphSchema:\n\"\"\"Defines the Schema of a graph by specifiying the node and relationship types and how they interconnect.\n\n    See `GraphObjectShape` and `PresentRelationship` for more details.\n    \"\"\"\n\n    object_shapes: List[GraphObjectShape]\n    relationships: List[PresentRelationship]\n\n    @classmethod\n    def empty(cls):\n        return cls(object_shapes=[], relationships=[])\n\n    def merge(self, other: \"GraphSchema\") -&gt; \"GraphSchema\":\n        all_shapes = self.object_shapes + other.object_shapes\n        all_rels = self.relationships + other.relationships\n        return GraphSchema(\n            object_shapes=_merge_overlapping_items(all_shapes, all_shapes),\n            relationships=_merge_overlapping_items(all_rels, all_shapes),\n        )\n\n    def known_node_types(self) -&gt; Iterable[GraphObjectShape]:\n        for shape in self.object_shapes:\n            if shape.is_node and shape.has_known_type:\n                yield shape\n\n    def known_relationship_types(self) -&gt; Iterable[GraphObjectShape]:\n        for shape in self.object_shapes:\n            if shape.is_relationship and shape.has_known_type:\n                yield shape\n\n    def apply_type_overrides_from_file(self, file_path: Path):\n        overrides = GraphSchemaOverrides.from_file(file_path)\n        self.apply_overrides(overrides)\n\n    def apply_overrides(self, overrides: \"GraphSchemaOverrides\"):\n        overrides.apply_to(self)\n</code></pre>"},{"location":"python_reference/schema/schema/#nodestream.schema.schema.IntrospectiveIngestionComponent","title":"<code>IntrospectiveIngestionComponent</code>","text":"<p>             Bases: <code>ABC</code></p> <p>An IntrospectableIngestionComponent is a componet that can be interrogated what it contributes the Graph Database Schema.</p> <p>Nearly all components in the transformation part of the ingestion stack (Interpreters, Interpretations, and Pipelines) are <code>IntrospectableIngestionComponent</code>s. Leaf components like Interpretations provide a relatively local scope as to what it knows about the schema. As is the hierarchy grows, more and more data is is combined and aggregated together until an entire schema is produced. For more information on aggregation, see <code>AggregatedIntrospectiveIngestionComponent</code>.</p> Source code in <code>nodestream/schema/schema.py</code> Python<pre><code>class IntrospectiveIngestionComponent(ABC):\n\"\"\"An IntrospectableIngestionComponent is a componet that can be interrogated what it contributes the Graph Database Schema.\n\n    Nearly all components in the transformation part of the ingestion stack (Interpreters, Interpretations, and Pipelines) are\n    `IntrospectableIngestionComponent`s. Leaf components like Interpretations provide a relatively local scope as to what it knows\n    about the schema. As is the hierarchy grows, more and more data is is combined and aggregated together until an entire schema\n    is produced. For more information on aggregation, see `AggregatedIntrospectiveIngestionComponent`.\n    \"\"\"\n\n    @abstractmethod\n    def gather_used_indexes(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def gather_object_shapes(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def gather_present_relationships(self):\n        raise NotImplementedError\n\n    def generate_graph_schema(self) -&gt; GraphSchema:\n        shapes = self.gather_object_shapes()\n        relationships = self.gather_present_relationships()\n        return GraphSchema(\n            object_shapes=_merge_overlapping_items(shapes, shapes),\n            relationships=_merge_overlapping_items(relationships, shapes),\n        )\n</code></pre>"},{"location":"python_reference/schema/schema/#nodestream.schema.schema.KnownTypeMarker","title":"<code>KnownTypeMarker</code>","text":"<p>             Bases: <code>TypeMarker</code></p> <p>Represents a type that is known within the context of its creation and may be referenced as something else in the future.</p> Source code in <code>nodestream/schema/schema.py</code> Python<pre><code>class KnownTypeMarker(TypeMarker):\n\"\"\"Represents a type that is known within the context of its creation and may be referenced as something else in the future.\"\"\"\n\n    def __init__(self, type: str, fulfills_alias: Optional[str] = None) -&gt; None:\n        self.type = type\n        self.alias = fulfills_alias\n\n    def resolve_type(\n        self, other_shapes: Iterable[\"GraphObjectShape\"]\n    ) -&gt; Optional[TypeMarker]:\n        return self\n\n    def fulfills_alias(self, desired_alias: str) -&gt; bool:\n        return self.alias == desired_alias\n\n    def __eq__(self, o: object) -&gt; bool:\n        return isinstance(o, KnownTypeMarker) and o.type == self.type\n\n    @classmethod\n    def fulfilling_source_node(cls, type: str) -&gt; \"KnownTypeMarker\":\n        return cls(type=type, fulfills_alias=SOURCE_ALIAS)\n\n    def __str__(self) -&gt; str:\n        return self.type\n</code></pre>"},{"location":"python_reference/schema/schema/#nodestream.schema.schema.PresentRelationship","title":"<code>PresentRelationship</code>  <code>dataclass</code>","text":"<p>Indicates that a relationship of a specifc type exissts between two nodes with a certain cardinality.</p> <p>The shape of the relationship itself as well as the shapes of the node types are stored as <code>GraphObjectShape</code>.</p> Source code in <code>nodestream/schema/schema.py</code> Python<pre><code>@dataclass(slots=True)\nclass PresentRelationship:\n\"\"\"Indicates that a relationship of a specifc type exissts between two nodes with a certain cardinality.\n\n    The shape of the relationship itself as well as the shapes of the node types are stored as `GraphObjectShape`.\n    \"\"\"\n\n    from_object_type: TypeMarker\n    to_object_type: TypeMarker\n    relationship_type: TypeMarker\n    from_side_cardinality: Cardinality\n    to_side_cardinality: Cardinality\n\n    def overlaps_with(self, other: \"PresentRelationship\") -&gt; bool:\n        return (\n            self.from_object_type == other.from_object_type\n            and self.to_object_type == other.to_object_type\n            and self.relationship_type == other.relationship_type\n        )\n\n    def include(self, other: \"PresentRelationship\"):\n        if not self.overlaps_with(other):\n            raise ValueError(\n                \"Cannot include when other relationship is not overlapping\"\n            )\n\n        # If multiple relationships come in, we do not have enough information to assume that\n        # the relationship is anything other than MANY-&gt;MANY. For any side that is \"single\", we\n        # do not know that the each one of the sources of the relationship will not all create a \"single\"\n        # sided relationship off of the same nodes.\n        self.from_side_cardinality = Cardinality.MANY\n        self.to_side_cardinality = Cardinality.MANY\n\n    def resolve_types(self, shapes: Iterable[\"GraphObjectShape\"]):\n        if object_type := self.from_object_type.resolve_type(shapes):\n            self.from_object_type = object_type\n        if object_type := self.to_object_type.resolve_type(shapes):\n            self.to_object_type = object_type\n\n        self.relationship_type = self.relationship_type.resolve_type(shapes)\n</code></pre>"},{"location":"python_reference/schema/schema/#nodestream.schema.schema.UnknownTypeMarker","title":"<code>UnknownTypeMarker</code>","text":"<p>             Bases: <code>TypeMarker</code></p> <p>Represents a type that is not known currently and requires context from others to resolve.</p> <p>In other words, we know what role this type is, but not what the exact type is.</p> Source code in <code>nodestream/schema/schema.py</code> Python<pre><code>class UnknownTypeMarker(TypeMarker):\n\"\"\"Represents a type that is not known currently and requires context from others to resolve.\n\n    In other words, we know what role this type is, but not what the exact type is.\n    \"\"\"\n\n    def __init__(self, alias: str) -&gt; None:\n        self.alias = alias\n\n    def resolve_type(\n        self, other_shapes: Iterable[\"GraphObjectShape\"]\n    ) -&gt; Optional[TypeMarker]:\n        for shape in other_shapes:\n            if shape.object_type.fulfills_alias(self.alias):\n                return shape.object_type\n\n    def fulfills_alias(self, desired_alias: str) -&gt; bool:\n        return False\n\n    def __eq__(self, o: object) -&gt; bool:\n        return isinstance(o, UnknownTypeMarker) and o.alias == self.alias\n\n    def __str__(self) -&gt; str:\n        return f\"[Uknown] Alias({self.alias}))\"\n\n    @classmethod\n    def source_node(cls):\n        return cls(alias=SOURCE_ALIAS)\n</code></pre>"},{"location":"python_reference/schema/printers/graph_schema_extraction/","title":"Graph schema extraction","text":""},{"location":"python_reference/schema/printers/graphql_schema_printer/","title":"Graphql schema printer","text":""},{"location":"python_reference/schema/printers/plain_text_schema_printer/","title":"Plain text schema printer","text":""},{"location":"python_reference/schema/printers/schema_printer/","title":"Schema printer","text":""}]}